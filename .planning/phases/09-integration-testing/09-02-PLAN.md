---
phase: 09-integration-testing
plan: 02
type: execute
wave: 1
depends_on: [09-01]
files_modified:
  - tests/stability/__init__.py
  - tests/stability/long_running_test.py
  - tests/stability/load_generator.py
  - tests/stability/emotion_stability.py
  - tests/stability/recovery_test.py
  - scripts/run_stability_test.sh
autonomous: true
must_haves:
  truths:
    - "7-day uptime requires automated long-running test infrastructure"
    - "Load generation simulates realistic user interaction patterns"
    - "Emotional state consistency must be verified over extended periods"
    - "Automatic restart testing validates recovery mechanisms"
  artifacts:
    - path: "tests/stability/long_running_test.py"
      provides: "Long-running stability test harness"
      exports: ["LongRunningStabilityTest", "StabilityMetrics"]
      min_lines: 200
    - path: "tests/stability/load_generator.py"
      provides: "Simulated user interaction load generator"
      exports: ["LoadGenerator", "InteractionPattern"]
      min_lines: 150
    - path: "tests/stability/emotion_stability.py"
      provides: "Emotional state consistency verification"
      exports: ["EmotionStabilityMonitor", "verify_emotion_consistency"]
      min_lines: 100
    - path: "tests/stability/recovery_test.py"
      provides: "Automatic restart and recovery testing"
      exports: ["RecoveryTest", "simulate_crash_recovery"]
      min_lines: 100
    - path: "scripts/run_stability_test.sh"
      provides: "Shell script to run 7-day stability test"
      exports: []
      min_lines: 50
  key_links:
    - from: "tests/stability/long_running_test.py"
      to: "src/conductor/orchestrator.py"
      via: "Monitors conductor uptime and health"
      pattern: "conductor.get_system_status"
    - from: "tests/stability/emotion_stability.py"
      to: "src/emotion/persistence.py"
      via: "Verifies emotional state persistence"
      pattern: "EmotionPersistence.load_latest_state"
    - from: "tests/stability/recovery_test.py"
      to: "src/emotion/persistence.py"
      via: "Validates state restoration after restart"
      pattern: "restore_and_age_state"
---

<objective>
Build Long-Running Stability Tests that validate Demi's ability to operate continuously for 7+ days without manual intervention. This includes simulating realistic user interactions, verifying emotional state consistency over time, and testing automatic restart/recovery mechanisms.

Purpose: Satisfy HEALTH-01 requirement (7-day uptime) and validate system resilience under sustained operation. Detect memory leaks, resource exhaustion, and degradation patterns that only appear over extended periods.

Output: Comprehensive stability test suite that can run for 7 days continuously, generating load, monitoring health metrics, and verifying emotional state consistency throughout.
</objective>

<execution_context>
@.planning/REQUIREMENTS.md
@src/conductor/health.py
@src/conductor/resource_monitor.py
@src/emotion/persistence.py
</execution_context>

<context>
@src/conductor/orchestrator.py
@src/conductor/resource_monitor.py
@src/emotion/decay.py
@src/emotion/persistence.py
@tests/integration/harness.py
</context>

<tasks>

<task type="auto">
  <name>Create long-running stability test harness</name>
  <files>tests/stability/long_running_test.py</files>
  <action>Create `tests/stability/long_running_test.py` with long-running test infrastructure:

**Imports:**
- `import asyncio`
- `import time`
- `import signal`
- `import sys`
- `from typing import Dict, List, Optional, Any`
- `from dataclasses import dataclass, field`
- `from datetime import datetime, timedelta`
- `from pathlib import Path`
- `import json`
- `from src.conductor.orchestrator import Conductor`
- `from src.conductor.health import get_health_monitor, HealthStatus`
- `from src.conductor.resource_monitor import ResourceMonitor`
- `from src.core.logger import get_logger`

**Dataclass StabilityMetrics:**
```python
@dataclass
class StabilityMetrics:
    """Metrics collected during stability test."""
    start_time: float = field(default_factory=time.time)
    total_interactions: int = 0
    successful_responses: int = 0
    failed_responses: int = 0
    restarts: int = 0
    emotion_checks: List[Dict] = field(default_factory=list)
    resource_snapshots: List[Dict] = field(default_factory=list)
    error_log: List[Dict] = field(default_factory=list)
    
    @property
    def uptime_hours(self) -> float:
        return (time.time() - self.start_time) / 3600
    
    @property
    def success_rate(self) -> float:
        total = self.successful_responses + self.failed_responses
        return self.successful_responses / total if total > 0 else 0.0
    
    def to_dict(self) -> Dict:
        return {
            "uptime_hours": self.uptime_hours,
            "total_interactions": self.total_interactions,
            "successful_responses": self.successful_responses,
            "failed_responses": self.failed_responses,
            "success_rate": self.success_rate,
            "restarts": self.restarts,
            "emotion_check_count": len(self.emotion_checks),
            "resource_snapshot_count": len(self.resource_snapshots),
            "error_count": len(self.error_log),
        }
```

**Class LongRunningStabilityTest:**
- `__init__(self, duration_hours: float = 168, checkpoint_interval_minutes: int = 60)`:
  - duration_hours: 168 = 7 days (default)
  - checkpoint_interval: How often to save progress
  - Initialize metrics and state
  - Setup signal handlers for graceful shutdown
  
- `async def setup(self, config_overrides: Optional[Dict] = None)`:
  - Initialize Conductor with test configuration
  - Setup ResourceMonitor with 30-second collection
  - Initialize health monitoring
  - Create checkpoint directory
  - Load previous checkpoint if exists
  
- `async def run(self)`:
  - Main test loop running for specified duration
  - Coordinate load generation, health checks, emotion verification
  - Save checkpoints at intervals
  - Handle graceful shutdown on signals
  
- `async def _monitoring_loop(self)`:
  - Continuous monitoring during test
  - Collect resource metrics every 30 seconds
  - Check health status every 5 seconds
  - Log any anomalies
  
- `async def _save_checkpoint(self)`:
  - Save current metrics to checkpoint file
  - Include timestamp and test progress
  - Allow resuming interrupted tests
  
- `def _load_checkpoint(self) -> Optional[Dict]`:
  - Load previous checkpoint if exists
  - Resume from saved state
  
- `async def graceful_shutdown(self)`:
  - Handle SIGINT/SIGTERM gracefully
  - Save final checkpoint
  - Cleanup resources
  - Generate final report
  
- `def generate_report(self) -> str`:
  - Generate human-readable test report
  - Include uptime, success rates, resource trends
  - Highlight any issues found

**Checkpoint format:**
```python
def _checkpoint_path(self) -> Path:
    checkpoint_dir = Path("~/.demi/stability_checkpoints").expanduser()
    checkpoint_dir.mkdir(parents=True, exist_ok=True)
    return checkpoint_dir / "stability_test_checkpoint.json"

def _save_checkpoint_impl(self):
    checkpoint = {
        "timestamp": time.time(),
        "start_time": self.metrics.start_time,
        "duration_hours": self.duration_hours,
        "metrics": self.metrics.to_dict(),
        "progress_percent": (self.metrics.uptime_hours / self.duration_hours) * 100,
    }
    with open(self._checkpoint_path(), 'w') as f:
        json.dump(checkpoint, f, indent=2)
```

**Signal handling:**
```python
def _setup_signal_handlers(self):
    """Setup handlers for graceful shutdown."""
    def signal_handler(signum, frame):
        logger.info(f"Received signal {signum}, initiating graceful shutdown...")
        self._shutdown_requested = True
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
```

**Main execution:**
```python
async def main():
    """Run stability test from command line."""
    import argparse
    parser = argparse.ArgumentParser(description="Run long-running stability test")
    parser.add_argument("--duration", type=float, default=168, help="Test duration in hours (default: 168 = 7 days)")
    parser.add_argument("--checkpoint-interval", type=int, default=60, help="Checkpoint interval in minutes")
    args = parser.parse_args()
    
    test = LongRunningStabilityTest(
        duration_hours=args.duration,
        checkpoint_interval_minutes=args.checkpoint_interval
    )
    
    await test.setup()
    try:
        await test.run()
    finally:
        report = test.generate_report()
        print(report)

if __name__ == "__main__":
    asyncio.run(main())
```
</action>
  <verify>
1. Import check: `python -c "from tests.stability.long_running_test import LongRunningStabilityTest, StabilityMetrics; print('Stability test import OK')"`
2. Metrics dataclass:
   ```python
   from tests.stability.long_running_test import StabilityMetrics
   m = StabilityMetrics()
   m.total_interactions = 100
   m.successful_responses = 95
   print(f"Success rate: {m.success_rate:.1%}")
   assert m.success_rate == 0.95
   ```
3. Checkpoint test:
   ```python
   import tempfile
   from pathlib import Path
   from tests.stability.long_running_test import LongRunningStabilityTest
   
   with tempfile.TemporaryDirectory() as tmpdir:
       test = LongRunningStabilityTest(duration_hours=1)
       test.metrics.total_interactions = 50
       test._save_checkpoint_impl()
       assert test._checkpoint_path().exists()
       print("Checkpoint save/load OK")
   ```
4. Report generation: Verify report contains all expected sections
  </verify>
  <done>LongRunningStabilityTest created with checkpointing, metrics collection, and graceful shutdown</done>
</task>

<task type="auto">
  <name>Create load generator for simulated user interactions</name>
  <files>tests/stability/load_generator.py</files>
  <action>Create `tests/stability/load_generator.py` with load generation:

**Imports:**
- `import asyncio`
- `import random`
- `from typing import Dict, List, Optional, Callable`
- `from dataclasses import dataclass`
- `from enum import Enum`
- `from datetime import datetime, timedelta`

**Class InteractionType:**
```python
class InteractionType(Enum):
    """Types of user interactions to simulate."""
    GREETING = "greeting"
    QUESTION = "question"
    AFFECTION = "affection"
    NEGATIVE = "negative"
    LONG_GAP = "long_gap"  # Simulates user away
    RANDOM = "random"
```

**Dataclass InteractionPattern:**
```python
@dataclass
class InteractionPattern:
    """Defines a pattern of interactions over time."""
    name: str
    interactions_per_hour: float  # Average interactions per hour
    burstiness: float  # 0=evenly spaced, 1=bursty
    types: List[InteractionType]
    duration_hours: float
```

**Pre-defined patterns:**
```python
# Realistic user patterns
PATTERNS = {
    "active_user": InteractionPattern(
        name="Active User",
        interactions_per_hour=12,  # Every 5 minutes
        burstiness=0.3,
        types=[InteractionType.GREETING, InteractionType.QUESTION, InteractionType.AFFECTION],
        duration_hours=8,  # Active during day
    ),
    "casual_user": InteractionPattern(
        name="Casual User",
        interactions_per_hour=3,  # Every 20 minutes
        burstiness=0.5,
        types=[InteractionType.GREETING, InteractionType.RANDOM],
        duration_hours=4,
    ),
    "sporadic_user": InteractionPattern(
        name="Sporadic User",
        interactions_per_hour=0.5,  ~# Every 2 hours
        burstiness=0.8,
        types=[InteractionType.QUESTION, InteractionType.LONG_GAP],
        duration_hours=24,
    ),
    "stress_test": InteractionPattern(
        name="Stress Test",
        interactions_per_hour=60,  # Every minute
        burstiness=0.2,
        types=list(InteractionType),
        duration_hours=1,
    ),
}
```

**Sample messages by type:**
```python
SAMPLE_MESSAGES = {
    InteractionType.GREETING: [
        "Hey Demi",
        "Hello",
        "Hi there",
        "What's up?",
        "Good morning",
    ],
    InteractionType.QUESTION: [
        "What do you think about this?",
        "Can you help me with something?",
        "What should I do?",
        "Tell me about yourself",
        "How are you feeling?",
    ],
    InteractionType.AFFECTION: [
        "I really like talking to you",
        "You're special to me",
        "I missed you",
        "You mean a lot to me",
        "I'm glad you're here",
    ],
    InteractionType.NEGATIVE: [
        "You're not helpful",
        "This is annoying",
        "I don't care what you think",
        "You're just a program",
        "Stop talking",
    ],
    InteractionType.RANDOM: [
        "The weather is nice today",
        "I had pizza for lunch",
        "My cat is sleeping",
        "Work was exhausting",
        "I'm bored",
    ],
}
```

**Class LoadGenerator:**
- `__init__(self, pattern: InteractionPattern, random_seed: Optional[int] = None)`:
  - Initialize with interaction pattern
  - Set random seed for reproducibility
  - Track statistics
  
- `async def generate_load(self, callback: Callable[[str], asyncio.Future], duration_hours: Optional[float] = None)`:
  - Generate interactions according to pattern
  - Call callback for each interaction
  - Handle burstiness (cluster interactions)
  - Run for specified duration or pattern duration
  
- `def _calculate_inter_arrival_time(self) -> float`:
  - Calculate time between interactions
  - Account for burstiness
  - Return seconds until next interaction
  
- `def _select_message(self) -> str`:
  - Choose random message from appropriate type
  - Weight by interaction type distribution
  
- `def get_stats(self) -> Dict`:
  - Return generation statistics
  - Include message counts by type
  - Average inter-arrival time

**Burstiness implementation:**
```python
def _calculate_inter_arrival_time(self) -> float:
    """Calculate time until next interaction with burstiness."""
    base_interval = 3600 / self.pattern.interactions_per_hour  # seconds
    
    if self.pattern.burstiness <= 0:
        return base_interval
    
    # Exponential distribution for bursty behavior
    # High burstiness = more variance
    lambda_param = 1 / (base_interval * (1 - self.pattern.burstiness * 0.5))
    interval = random.expovariate(lambda_param)
    
    # Cap at reasonable bounds
    return max(1.0, min(interval, base_interval * 10))
```

**Multi-pattern load:**
```python
class MultiPatternLoadGenerator:
    """Combine multiple interaction patterns for realistic load."""
    
    def __init__(self, patterns: List[Tuple[InteractionPattern, float]]):
        """
        Args:
            patterns: List of (pattern, weight) tuples
        """
        self.patterns = patterns
        self.generators = [LoadGenerator(p) for p, _ in patterns]
    
    async def generate_combined_load(self, callback: Callable):
        """Generate load from all patterns concurrently."""
        tasks = [
            gen.generate_load(callback)
            for gen in self.generators
        ]
        await asyncio.gather(*tasks, return_exceptions=True)
```
</action>
  <verify>
1. Import check: `python -c "from tests.stability.load_generator import LoadGenerator, InteractionPattern, PATTERNS; print('Load generator import OK')"`
2. Message selection:
   ```python
   from tests.stability.load_generator import LoadGenerator, PATTERNS
   gen = LoadGenerator(PATTERNS["active_user"])
   msg = gen._select_message()
   print(f"Selected message: {msg}")
   assert isinstance(msg, str)
   ```
3. Inter-arrival timing:
   ```python
   from tests.stability.load_generator import LoadGenerator, PATTERNS
   gen = LoadGenerator(PATTERNS["active_user"])
   times = [gen._calculate_inter_arrival_time() for _ in range(10)]
   print(f"Inter-arrival times: min={min(times):.1f}s, max={max(times):.1f}s, avg={sum(times)/len(times):.1f}s")
   ```
4. Pattern statistics:
   ```python
   for name, pattern in PATTERNS.items():
       print(f"{name}: {pattern.interactions_per_hour}/hr, burstiness={pattern.burstiness}")
   ```
  </verify>
  <done>LoadGenerator created with multiple interaction patterns, burstiness simulation, and realistic message sampling</done>
</task>

<task type="auto">
  <name>Create emotional state stability verification</name>
  <files>tests/stability/emotion_stability.py</files>
  <action>Create `tests/stability/emotion_stability.py`:

**Imports:**
- `from typing import Dict, List, Optional, Tuple`
- `from dataclasses import dataclass, field`
- `from datetime import datetime, timedelta`
- `from src.emotion.models import EmotionalState`
- `from src.emotion.persistence import EmotionPersistence`
- `from src.core.logger import get_logger`

**Dataclass EmotionSnapshot:**
```python
@dataclass
class EmotionSnapshot:
    """Snapshot of emotional state at a point in time."""
    timestamp: datetime
    state: EmotionalState
    trigger: str  # What caused this state (interaction, decay, etc.)
```

**Dataclass EmotionDriftReport:**
```python
@dataclass
class EmotionDriftReport:
    """Report on emotional state drift over time."""
    emotion_name: str
    start_value: float
    end_value: float
    min_value: float
    max_value: float
    drift_amount: float
    is_consistent: bool
    violations: List[Dict] = field(default_factory=list)
```

**Class EmotionStabilityMonitor:**
- `__init__(self, persistence: EmotionPersistence, check_interval_minutes: int = 30)`:
  - Initialize with emotion persistence layer
  - Set check interval
  - Initialize snapshot history
  
- `async def record_snapshot(self, trigger: str = "scheduled")`:
  - Record current emotional state
  - Store with timestamp and trigger
  - Maintain rolling window (last 7 days)
  
- `async def verify_consistency(self) -> Dict[str, EmotionDriftReport]`:
  - Analyze emotional state trends
  - Check for unexpected drift or jumps
  - Verify momentum is handled correctly
  - Return drift report for each emotion
  
- `def _check_emotion_bounds(self, snapshots: List[EmotionSnapshot]) -> List[Dict]`:
  - Verify all emotion values stay within [floor, 1.0]
  - Check for NaN or invalid values
  - Return list of violations
  
- `def _calculate_drift(self, snapshots: List[EmotionSnapshot], emotion: str) -> EmotionDriftReport`:
  - Calculate drift statistics for an emotion
  - Compare actual vs expected decay
  - Flag anomalies

**Consistency rules:**
```python
CONSISTENCY_RULES = {
    # Maximum allowed change between consecutive readings
    "max_single_change": 0.3,
    
    # Maximum allowed drift over 24 hours without interaction
    "max_daily_drift": 0.4,
    
    # Minimum floor enforcement (must be respected)
    "floor_violation_tolerance": 0.01,
    
    # Momentum decay check (should decay over time)
    "momentum_decay_expected": True,
}
```

**Verification functions:**
```python
async def verify_emotion_consistency(
    persistence: EmotionPersistence,
    hours_of_history: float = 24
) -> Dict[str, any]:
    """
    Verify emotional state has remained consistent over time.
    
    Args:
        persistence: EmotionPersistence instance
        hours_of_history: How far back to check
        
    Returns:
        Dict with consistency report
    """
    # Load recent states from database
    history = persistence.get_interaction_history(limit=1000)
    
    if len(history) < 2:
        return {"status": "insufficient_data", "message": "Need more history"}
    
    # Analyze state progression
    violations = []
    
    for i in range(1, len(history)):
        prev = EmotionalState.from_dict(history[i-1]["state_after"])
        curr = EmotionalState.from_dict(history[i]["state_after"])
        
        # Check each emotion
        for emotion_name in prev.get_all_emotions().keys():
            prev_val = getattr(prev, emotion_name)
            curr_val = getattr(curr, emotion_name)
            change = abs(curr_val - prev_val)
            
            if change > CONSISTENCY_RULES["max_single_change"]:
                violations.append({
                    "timestamp": history[i]["timestamp"],
                    "emotion": emotion_name,
                    "change": change,
                    "from_value": prev_val,
                    "to_value": curr_val,
                    "violation": "excessive_single_change",
                })
    
    return {
        "status": "consistent" if not violations else "violations_found",
        "violations": violations,
        "total_checks": len(history) - 1,
        "violation_count": len(violations),
    }


def verify_emotion_persistence_integrity(
    persistence: EmotionPersistence
) -> Dict[str, any]:
    """Verify database integrity for emotion storage."""
    try:
        # Try to load latest state
        state = persistence.load_latest_state()
        
        if state is None:
            return {"status": "no_state", "valid": False}
        
        # Verify all emotions in valid range
        issues = []
        for name, value in state.get_all_emotions().items():
            if not (0 <= value <= 1):
                issues.append(f"{name}={value} out of range [0,1]")
            if value != value:  # NaN check
                issues.append(f"{name} is NaN")
        
        return {
            "status": "valid" if not issues else "invalid",
            "valid": not issues,
            "issues": issues,
            "state_timestamp": state.last_updated.isoformat(),
        }
    except Exception as e:
        return {"status": "error", "valid": False, "error": str(e)}
```

**Decay verification:**
```python
async def verify_decay_behavior(
    initial_state: EmotionalState,
    current_state: EmotionalState,
    hours_elapsed: float
) -> Dict[str, any]:
    """Verify that emotion decay behaves correctly over time."""
    from src.emotion.decay import DecaySystem
    
    decay = DecaySystem()
    expected_state = decay.simulate_offline_decay(initial_state, hours_elapsed * 3600)
    
    discrepancies = []
    for emotion_name in initial_state.get_all_emotions().keys():
        expected = getattr(expected_state, emotion_name)
        actual = getattr(current_state, emotion_name)
        diff = abs(expected - actual)
        
        if diff > 0.05:  # 5% tolerance
            discrepancies.append({
                "emotion": emotion_name,
                "expected": expected,
                "actual": actual,
                "difference": diff,
            })
    
    return {
        "status": "correct" if not discrepancies else "discrepancy",
        "hours_elapsed": hours_elapsed,
        "discrepancies": discrepancies,
        "discrepancy_count": len(discrepancies),
    }
```
</action>
  <verify>
1. Import check: `python -c "from tests.stability.emotion_stability import EmotionStabilityMonitor, verify_emotion_consistency; print('Emotion stability import OK')"`
2. Snapshot recording:
   ```python
   from datetime import datetime
   from tests.stability.emotion_stability import EmotionSnapshot
   from src.emotion.models import EmotionalState
   
   state = EmotionalState(loneliness=0.7)
   snapshot = EmotionSnapshot(timestamp=datetime.now(), state=state, trigger="test")
   assert snapshot.state.loneliness == 0.7
   print("Snapshot creation OK")
   ```
3. Consistency rules:
   ```python
   from tests.stability.emotion_stability import CONSISTENCY_RULES
   print(f"Max single change: {CONSISTENCY_RULES['max_single_change']}")
   print(f"Max daily drift: {CONSISTENCY_RULES['max_daily_drift']}")
   ```
4. Drift calculation:
   ```python
   from tests.stability.emotion_stability import EmotionDriftReport
   report = EmotionDriftReport(
       emotion_name="loneliness",
       start_value=0.5,
       end_value=0.7,
       min_value=0.4,
       max_value=0.8,
       drift_amount=0.2,
       is_consistent=True
   )
   print(f"Drift report: {report.emotion_name} drifted {report.drift_amount}")
   ```
  </verify>
  <done>EmotionStabilityMonitor created with consistency verification, drift detection, and decay validation</done>
</task>

<task type="auto">
  <name>Create automatic restart and recovery testing</name>
  <files>tests/stability/recovery_test.py</files>
  <action>Create `tests/stability/recovery_test.py`:

**Imports:**
- `import asyncio`
- `import time`
- `import signal`
- `import os`
- `from typing import Dict, Optional, Any`
- `from dataclasses import dataclass`
- `from pathlib import Path`
- `import subprocess`
- `import json`
- `from src.emotion.models import EmotionalState`
- `from src.emotion.persistence import EmotionPersistence`

**Dataclass RecoveryResult:**
```python
@dataclass
class RecoveryResult:
    """Result of a recovery test."""
    test_name: str
    success: bool
    recovery_time_seconds: float
    emotion_preserved: bool
    state_difference: Optional[Dict[str, float]] = None
    error_message: Optional[str] = None
```

**Class RecoveryTest:**
- `__init__(self, test_db_path: str = "~/.demi/recovery_test.db")`:
  - Initialize with test database path
  - Setup persistence layer
  
- `async def test_clean_shutdown_recovery(self) -> RecoveryResult`:
  - Start Demi with known emotional state
  - Perform clean shutdown
  - Restart and verify state restored
  - Measure recovery time
  
- `async def test_crash_recovery(self) -> RecoveryResult`:
  - Start Demi with known state
  - Simulate crash (SIGKILL)
  - Restart and verify state recovered from last save
  - Verify no corruption
  
- `async def test_extended_offline_recovery(self, offline_hours: float = 24) -> RecoveryResult`:
  - Save emotional state
  - Simulate extended offline period
  - Restart with decay calculation
  - Verify emotions properly aged
  
- `async def test_corruption_recovery(self) -> RecoveryResult`:
  - Corrupt emotion database
  - Verify recovery from backup
  - Verify graceful fallback

**Test implementations:**
```python
async def test_clean_shutdown_recovery(self) -> RecoveryResult:
    """Test recovery after clean shutdown."""
    import tempfile
    
    start_time = time.time()
    
    try:
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = Path(tmpdir) / "emotions.db"
            persistence = EmotionPersistence(db_path=str(db_path))
            
            # Create known emotional state
            original_state = EmotionalState(
                loneliness=0.75,
                excitement=0.3,
                confidence=0.6,
            )
            persistence.save_state(original_state, notes="Pre-shutdown state")
            
            # Simulate shutdown by creating new persistence instance
            del persistence
            persistence = EmotionPersistence(db_path=str(db_path))
            
            # Recover state
            recovered_state = persistence.load_latest_state()
            
            # Compare
            differences = {}
            for name in original_state.get_all_emotions().keys():
                orig_val = getattr(original_state, name)
                recv_val = getattr(recovered_state, name)
                if abs(orig_val - recv_val) > 0.001:
                    differences[name] = {"original": orig_val, "recovered": recv_val}
            
            recovery_time = time.time() - start_time
            
            return RecoveryResult(
                test_name="clean_shutdown_recovery",
                success=len(differences) == 0,
                recovery_time_seconds=recovery_time,
                emotion_preserved=len(differences) == 0,
                state_difference=differences if differences else None,
            )
            
    except Exception as e:
        return RecoveryResult(
            test_name="clean_shutdown_recovery",
            success=False,
            recovery_time_seconds=time.time() - start_time,
            emotion_preserved=False,
            error_message=str(e),
        )


async def simulate_crash_recovery(
    persistence: EmotionPersistence,
    state_before: EmotionalState
) -> RecoveryResult:
    """Simulate crash and verify recovery."""
    start_time = time.time()
    
    try:
        # Save state
        persistence.save_state(state_before, notes="Pre-crash state")
        
        # Simulate crash - just reload without proper shutdown
        # (In real test, would use subprocess and SIGKILL)
        
        # Recover
        recovered = persistence.load_latest_state()
        
        recovery_time = time.time() - start_time
        
        # Should be identical since we just saved
        preserved = all(
            getattr(state_before, name) == getattr(recovered, name)
            for name in state_before.get_all_emotions().keys()
        )
        
        return RecoveryResult(
            test_name="crash_recovery",
            success=preserved,
            recovery_time_seconds=recovery_time,
            emotion_preserved=preserved,
        )
        
    except Exception as e:
        return RecoveryResult(
            test_name="crash_recovery",
            success=False,
            recovery_time_seconds=time.time() - start_time,
            emotion_preserved=False,
            error_message=str(e),
        )


async def test_backup_restore(
    persistence: EmotionPersistence
) -> RecoveryResult:
    """Test backup and restore functionality."""
    start_time = time.time()
    
    try:
        # Create state
        original = EmotionalState(confidence=0.8, affection=0.6)
        persistence.save_state(original)
        persistence.create_backup_snapshot(original, snapshot_type="test")
        
        # Corrupt main state (simulate by saving invalid)
        corrupted = EmotionalState(confidence=float('nan'))
        persistence.save_state(corrupted)
        
        # Restore from backup
        restored = persistence.restore_from_backup(backup_age_hours=1)
        
        recovery_time = time.time() - start_time
        
        success = restored is not None and restored.confidence == 0.8
        
        return RecoveryResult(
            test_name="backup_restore",
            success=success,
            recovery_time_seconds=recovery_time,
            emotion_preserved=success,
        )
        
    except Exception as e:
        return RecoveryResult(
            test_name="backup_restore",
            success=False,
            recovery_time_seconds=time.time() - start_time,
            emotion_preserved=False,
            error_message=str(e),
        )
```

**Automated recovery test runner:**
```python
async def run_all_recovery_tests() -> Dict[str, RecoveryResult]:
    """Run all recovery tests and return results."""
    import tempfile
    
    results = {}
    
    with tempfile.TemporaryDirectory() as tmpdir:
        db_path = Path(tmpdir) / "test.db"
        persistence = EmotionPersistence(db_path=str(db_path))
        
        # Test 1: Clean shutdown
        test = RecoveryTest()
        results["clean_shutdown"] = await test.test_clean_shutdown_recovery()
        
        # Test 2: Crash recovery
        state = EmotionalState(loneliness=0.6, excitement=0.7)
        results["crash_recovery"] = await simulate_crash_recovery(persistence, state)
        
        # Test 3: Backup restore
        results["backup_restore"] = await test_backup_restore(persistence)
    
    return results
```
</action>
  <verify>
1. Import check: `python -c "from tests.stability.recovery_test import RecoveryTest, RecoveryResult; print('Recovery test import OK')"`
2. Recovery result:
   ```python
   from tests.stability.recovery_test import RecoveryResult
   result = RecoveryResult(
       test_name="test_recovery",
       success=True,
       recovery_time_seconds=0.5,
       emotion_preserved=True
   )
   assert result.success and result.emotion_preserved
   print("Recovery result creation OK")
   ```
3. Clean shutdown test (mock):
   ```python
   import asyncio
   from tests.stability.recovery_test import RecoveryTest
   
   async def test():
       rt = RecoveryTest()
       result = await rt.test_clean_shutdown_recovery()
       print(f"Clean shutdown test: {result.success}, time={result.recovery_time_seconds:.3f}s")
   
   asyncio.run(test())
   ```
4. All recovery tests:
   ```python
   import asyncio
   from tests.stability.recovery_test import run_all_recovery_tests
   
   async def test():
       results = await run_all_recovery_tests()
       for name, result in results.items():
           print(f"{name}: {'PASS' if result.success else 'FAIL'}")
   
   asyncio.run(test())
   ```
  </verify>
  <done>RecoveryTest created with clean shutdown, crash recovery, and backup restore validation</done>
</task>

<task type="auto">
  <name>Create stability test runner shell script</name>
  <files>scripts/run_stability_test.sh</files>
  <action>Create `scripts/run_stability_test.sh`:

```bash
#!/bin/bash
#
# Long-running stability test runner for Demi
# Runs 7-day (168 hour) stability test with monitoring
#

set -e

# Configuration
DURATION_HOURS=${1:-168}  # Default: 7 days
CHECKPOINT_INTERVAL=${2:-60}  # Minutes
LOG_DIR="${HOME}/.demi/stability_logs"
CHECKPOINT_DIR="${HOME}/.demi/stability_checkpoints"
PIDFILE="${LOG_DIR}/stability_test.pid"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Setup directories
setup() {
    echo -e "${GREEN}Setting up stability test environment...${NC}"
    mkdir -p "$LOG_DIR"
    mkdir -p "$CHECKPOINT_DIR"
    
    # Check Python environment
    if ! command -v python3 &> /dev/null; then
        echo -e "${RED}Error: python3 not found${NC}"
        exit 1
    fi
    
    # Verify test files exist
    if [ ! -f "tests/stability/long_running_test.py" ]; then
        echo -e "${RED}Error: Stability test files not found${NC}"
        exit 1
    fi
    
    echo -e "${GREEN}Environment ready${NC}"
}

# Start the stability test
start_test() {
    echo -e "${GREEN}Starting ${DURATION_HOURS} hour stability test...${NC}"
    echo "Log directory: $LOG_DIR"
    echo "Checkpoint directory: $CHECKPOINT_DIR"
    echo "Checkpoint interval: ${CHECKPOINT_INTERVAL} minutes"
    
    # Run test with output logging
    nohup python3 -m tests.stability.long_running_test \
        --duration "$DURATION_HOURS" \
        --checkpoint-interval "$CHECKPOINT_INTERVAL" \
        > "${LOG_DIR}/stability_test.log" 2>&1 &
    
    PID=$!
    echo $PID > "$PIDFILE"
    
    echo -e "${GREEN}Stability test started with PID: $PID${NC}"
    echo "To monitor: tail -f ${LOG_DIR}/stability_test.log"
    echo "To stop: kill $(cat $PIDFILE)"
}

# Check test status
status() {
    if [ -f "$PIDFILE" ]; then
        PID=$(cat "$PIDFILE")
        if ps -p "$PID" > /dev/null 2>&1; then
            echo -e "${GREEN}Stability test is running (PID: $PID)${NC}"
            
            # Show recent log
            if [ -f "${LOG_DIR}/stability_test.log" ]; then
                echo "Recent activity:"
                tail -n 20 "${LOG_DIR}/stability_test.log"
            fi
            
            # Show checkpoint if exists
            if [ -f "${CHECKPOINT_DIR}/stability_test_checkpoint.json" ]; then
                echo ""
                echo "Latest checkpoint:"
                cat "${CHECKPOINT_DIR}/stability_test_checkpoint.json"
            fi
        else
            echo -e "${YELLOW}Stability test is not running (stale PID file)${NC}"
            rm -f "$PIDFILE"
        fi
    else
        echo -e "${YELLOW}Stability test is not running${NC}"
    fi
}

# Stop the test
stop_test() {
    if [ -f "$PIDFILE" ]; then
        PID=$(cat "$PIDFILE")
        echo -e "${YELLOW}Stopping stability test (PID: $PID)...${NC}"
        kill "$PID" 2>/dev/null || true
        rm -f "$PIDFILE"
        
        # Wait for graceful shutdown
        for i in {1..10}; do
            if ! ps -p "$PID" > /dev/null 2>&1; then
                echo -e "${GREEN}Stability test stopped gracefully${NC}"
                return 0
            fi
            sleep 1
        done
        
        # Force kill if still running
        echo -e "${RED}Force killing stability test...${NC}"
        kill -9 "$PID" 2>/dev/null || true
    else
        echo -e "${YELLOW}No stability test running${NC}"
    fi
}

# Generate final report
report() {
    echo -e "${GREEN}Generating stability test report...${NC}"
    
    if [ -f "${LOG_DIR}/stability_test.log" ]; then
        echo "=== Test Log Summary ==="
        tail -n 100 "${LOG_DIR}/stability_test.log"
    fi
    
    if [ -f "${CHECKPOINT_DIR}/stability_test_checkpoint.json" ]; then
        echo ""
        echo "=== Final Checkpoint ==="
        cat "${CHECKPOINT_DIR}/stability_test_checkpoint.json"
    fi
    
    # Check for errors in log
    if [ -f "${LOG_DIR}/stability_test.log" ]; then
        ERROR_COUNT=$(grep -c "ERROR" "${LOG_DIR}/stability_test.log" 2>/dev/null || echo 0)
        echo ""
        echo "=== Error Summary ==="
        echo "Total errors: $ERROR_COUNT"
        if [ "$ERROR_COUNT" -gt 0 ]; then
            grep "ERROR" "${LOG_DIR}/stability_test.log" | tail -n 10
        fi
    fi
}

# Main command dispatcher
case "${3:-start}" in
    start)
        setup
        start_test
        ;;
    status)
        status
        ;;
    stop)
        stop_test
        ;;
    restart)
        stop_test
        sleep 2
        setup
        start_test
        ;;
    report)
        report
        ;;
    *)
        echo "Usage: $0 [DURATION_HOURS] [CHECKPOINT_MINUTES] [start|stop|restart|status|report]"
        echo ""
        echo "Examples:"
        echo "  $0                    # Run 7-day test (default)"
        echo "  $0 24                 # Run 24-hour test"
        echo "  $0 168 30 start       # Run 7-day test with 30-min checkpoints"
        echo "  $0 status             # Check test status"
        echo "  $0 stop               # Stop running test"
        echo "  $0 report             # Generate final report"
        exit 1
        ;;
esac
```

Make script executable:
```bash
chmod +x scripts/run_stability_test.sh
```
</action>
  <verify>
1. Script syntax check: `bash -n /home/mystiatech/projects/Demi/scripts/run_stability_test.sh && echo "Script syntax OK"`
2. Help output: `bash scripts/run_stability_test.sh 2>&1 | head -20`
3. Directory creation: Verify LOG_DIR and CHECKPOINT_DIR paths
4. Command parsing: Test each command (start, stop, status, report)
  </verify>
  <done>Shell script created for managing 7-day stability test execution</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Import verification:** All stability modules import without errors
2. **Harness test:** Run short (1-minute) stability test to verify infrastructure
3. **Load generator:** Verify generates expected interaction patterns
4. **Emotion stability:** Test consistency checking with sample data
5. **Recovery tests:** Run all recovery scenarios
6. **Script test:** Verify shell script commands work

**Stability test targets:**
- Test setup completes in <30 seconds
- Load generator produces realistic interaction patterns
- Emotion consistency checking detects anomalies
- Recovery tests all pass
- 7-day test can run unattended with checkpointing
</verification>

<success_criteria>
- LongRunningStabilityTest runs for 7 days with checkpointing
- LoadGenerator produces realistic user interaction patterns
- EmotionStabilityMonitor detects state inconsistencies
- RecoveryTest validates all restart scenarios
- Shell script manages test lifecycle (start/stop/status/report)
- Test suite verifies 7-day uptime capability (HEALTH-01)
- Emotional state consistency maintained over 7 days (HEALTH-03)
- No memory leaks or resource exhaustion over test period
</success_criteria>

<output>
After completion, create `.planning/phases/09-integration-testing/09-02-SUMMARY.md`
</output>
