---
phase: 09-integration-testing
plan: 01
type: execute
wave: 1
depends_on: [08-03]
files_modified:
  - tests/integration/__init__.py
  - tests/integration/conftest.py
  - tests/integration/harness.py
  - tests/integration/fixtures/
  - tests/integration/mocks/
  - tests/integration/scenarios/
  - tests/integration/test_full_pipeline.py
autonomous: true
must_haves:
  truths:
    - "Integration tests verify full system behavior, not just components"
    - "Mock external services enable testing without real Discord/Ollama"
    - "Scenario-based tests validate real user interaction patterns"
    - "Test suite must complete in <5 minutes for CI/CD viability"
  artifacts:
    - path: "tests/integration/harness.py"
      provides: "IntegrationTestHarness for full-system test orchestration"
      exports: ["IntegrationTestHarness", "TestEnvironment"]
      min_lines: 200
    - path: "tests/integration/mocks/discord.py"
      provides: "Mock Discord client for testing without real Discord"
      exports: ["MockDiscordClient", "MockChannel", "MockUser"]
      min_lines: 150
    - path: "tests/integration/mocks/ollama.py"
      provides: "Mock Ollama server for controlled LLM responses"
      exports: ["MockOllamaServer", "MockLLMResponse"]
      min_lines: 100
    - path: "tests/integration/fixtures/emotion_fixtures.py"
      provides: "Test data factories for emotional states"
      exports: ["create_test_emotion_state", "EmotionStateFactory"]
      min_lines: 80
    - path: "tests/integration/scenarios/conversation_scenarios.py"
      provides: "Scenario definitions for common interaction patterns"
      exports: ["run_lonely_demeanor_scenario", "run_excited_ramble_scenario"]
      min_lines: 100
    - path: "tests/integration/test_full_pipeline.py"
      provides: "Full pipeline integration tests"
      exports: ["test_message_to_response_pipeline", "test_ramble_generation_pipeline"]
      min_lines: 150
  key_links:
    - from: "tests/integration/harness.py"
      to: "src/conductor/orchestrator.py"
      via: "Conductor orchestration in test environment"
      pattern: "Conductor.startup/shutdown"
    - from: "tests/integration/mocks/discord.py"
      to: "src/platforms/discord/bot.py"
      via: "Mock replaces real Discord client"
      pattern: "MockDiscordClient.simulate_message"
    - from: "tests/integration/mocks/ollama.py"
      to: "src/llm/inference.py"
      via: "Mock replaces real Ollama inference"
      pattern: "MockOllamaServer.generate_response"
---

<objective>
Build a comprehensive End-to-End (E2E) Testing Framework that validates the full Demi system behavior across all integrated components. This framework enables testing of complete user interaction flows—from message receipt through emotional processing, LLM inference, and response generation—without requiring live external services.

Purpose: Ensure system-wide correctness, catch integration bugs early, and provide confidence for deployments by testing realistic scenarios (user conversations, rambles, voice interactions) in a controlled environment.

Output: Complete integration test harness with mock external services, scenario-based test library, and fixture factories. Test suite executes in <5 minutes covering critical user paths.
</objective>

<execution_context>
@.planning/REQUIREMENTS.md
@.planning/ROADMAP.md
@src/conductor/orchestrator.py
@src/conductor/health.py
@src/emotion/models.py
</execution_context>

<context>
@src/conductor/orchestrator.py
@src/conductor/isolation.py
@src/emotion/persistence.py
@src/platforms/discord/bot.py
@src/llm/inference.py
</context>

<tasks>

<task type="auto">
  <name>Create integration test harness core infrastructure</name>
  <files>tests/integration/harness.py</files>
  <action>Create `tests/integration/harness.py` with IntegrationTestHarness:

**Imports:**
- `import asyncio`
- `import tempfile`
- `from typing import Dict, List, Optional, Any, Callable`
- `from dataclasses import dataclass, field`
- `from contextlib import asynccontextmanager`
- `from pathlib import Path`
- `from src.conductor.orchestrator import Conductor`
- `from src.core.config import DemiConfig`
- `from src.core.logger import get_logger`

**Dataclass TestEnvironment:**
```python
@dataclass
class TestEnvironment:
    """Container for test environment state."""
    conductor: Optional[Conductor] = None
    temp_dir: Optional[Path] = None
    mock_services: Dict[str, Any] = field(default_factory=dict)
    test_db_path: Optional[str] = None
    event_log: List[Dict] = field(default_factory=list)
    
    def record_event(self, event_type: str, data: Dict):
        """Record an event for test verification."""
        self.event_log.append({"type": event_type, "data": data, "timestamp": time.time()})
```

**Class IntegrationTestHarness:**
- `__init__(self, use_mocks: bool = True)`:
  - Initialize harness configuration
  - Setup mock service registry
  - Create temp directories for test isolation
  
- `async def setup(self, config_overrides: Optional[Dict] = None) -> TestEnvironment`:
  - Create temporary database
  - Initialize DemiConfig with test overrides:
    ```python
    test_config = {
        "database": {"path": str(temp_db)},
        "conductor": {"isolation_timeout_seconds": 5},  # Faster for tests
        "emotion": {"decay_interval_seconds": 60},  # Speed up for testing
    }
    ```
  - Initialize Conductor with test config
  - Register mock services if use_mocks=True
  - Start conductor (partial startup for unit tests)
  - Return TestEnvironment
  
- `async def teardown(self, env: TestEnvironment)`:
  - Gracefully shutdown conductor
  - Cleanup mock services
  - Remove temporary directories
  - Verify no resource leaks
  
- `async def run_scenario(self, scenario_fn: Callable, env: TestEnvironment, **kwargs) -> Dict`:
  - Execute a scenario function with environment
  - Collect and return results
  - Record timing metrics
  
- `def assert_emotion_state(self, env: TestEnvironment, expected_emotions: Dict[str, float], tolerance: float = 0.1)`:
  - Verify emotional state matches expectations
  - Check all 9 emotion dimensions
  
- `def assert_response_quality(self, response: str, checks: List[str])`:
  - Verify response contains expected content
  - Check for personality markers

**Context manager for easy test usage:**
```python
@asynccontextmanager
async def test_environment(config_overrides=None, use_mocks=True):
    """Context manager for test environment lifecycle."""
    harness = IntegrationTestHarness(use_mocks=use_mocks)
    env = await harness.setup(config_overrides)
    try:
        yield env
    finally:
        await harness.teardown(env)
```

**Performance tracking:**
- Track setup/teardown times
- Record scenario execution duration
- Alert on tests exceeding time thresholds
</action>
  <verify>
1. Import check: `python -c "from tests.integration.harness import IntegrationTestHarness, TestEnvironment; print('Harness import OK')"`
2. Context manager test:
   ```python
   import asyncio
   from tests.integration.harness import test_environment
   
   async def test():
       async with test_environment() as env:
           print(f"Conductor: {env.conductor}")
           print(f"Temp dir: {env.temp_dir}")
           assert env.conductor is not None
   
   asyncio.run(test())
   ```
3. Setup/teardown timing: Verify completes in <10 seconds
4. Event logging: Record test events and verify retrieval
  </verify>
  <done>IntegrationTestHarness created with environment lifecycle management, mock service integration, and performance tracking</done>
</task>

<task type="auto">
  <name>Create mock Discord client for testing</name>
  <files>tests/integration/mocks/discord.py</files>
  <action>Create `tests/integration/mocks/discord.py` with Discord mocks:

**Imports:**
- `import asyncio`
- `from typing import List, Dict, Optional, Callable, Any`
- `from dataclasses import dataclass, field`
- `from datetime import datetime`
- `import discord`

**Dataclass MockMessage:**
```python
@dataclass
class MockMessage:
    """Simulates a Discord message."""
    content: str
    author_id: int = 123456789
    author_name: str = "TestUser"
    channel_id: int = 987654321
    guild_id: Optional[int] = None
    mentions_bot: bool = False
    timestamp: datetime = field(default_factory=datetime.now)
    id: int = field(default_factory=lambda: int(datetime.now().timestamp() * 1000))
```

**Class MockChannel:**
- `__init__(self, channel_id: int, name: str = "test-channel")`
- `messages_sent: List[Dict]` - records all sent messages
- `async def send(self, content: str = None, embed=None, **kwargs) -> MockMessage`:
  - Record message to messages_sent
  - Return MockMessage representing bot response
- `def get_last_message(self) -> Optional[Dict]`
- `def message_count(self) -> int`

**Class MockUser:**
- `__init__(self, user_id: int, name: str)`
- `mention: str` - Discord mention format
- `async def send(self, content: str) -> MockMessage` - DM simulation

**Class MockDiscordClient:**
- `__init__(self, bot_id: int = 111111111)`
- `channels: Dict[int, MockChannel]` - registered channels
- `dm_channels: Dict[int, MockChannel]` - DM channels by user
- `event_handlers: Dict[str, List[Callable]]` - registered event handlers
- 
- `def register_channel(self, channel_id: int, name: str = "test") -> MockChannel`:
  - Create and register a mock channel
  
- `def simulate_message(self, content: str, channel_id: int = None, author: MockUser = None, mentions_bot: bool = False) -> MockMessage`:
  - Create MockMessage
  - Trigger on_message handlers
  - Return created message
  
- `def simulate_mention(self, content: str, channel_id: int = None, author: MockUser = None) -> MockMessage`:
  - Simulate bot mention (automatically sets mentions_bot=True)
  
- `def simulate_dm(self, content: str, user: MockUser) -> MockMessage`:
  - Simulate direct message to bot
  
- `async def wait_for_response(self, channel_id: int, timeout: float = 5.0) -> Optional[MockMessage]`:
  - Wait for bot to send message to channel
  - Poll messages_sent with timeout
  
- `def get_message_history(self, channel_id: int) -> List[MockMessage]`
- `def clear_history(self, channel_id: int = None)`

**Integration with real Discord bot:**
```python
class DiscordBotAdapter:
    """Adapter to use mock client with real DiscordBot code."""
    
    def __init__(self, mock_client: MockDiscordClient):
        self.mock = mock_client
        self.event_callbacks = {}
    
    def on_message(self, callback):
        """Decorator to register message handler."""
        self.event_callbacks['message'] = callback
        return callback
    
    async def trigger_message(self, message: MockMessage):
        """Trigger registered handlers."""
        if 'message' in self.event_callbacks:
            await self.event_callbacks['message'](message)
```

**Pre-built test users:**
- `TEST_USER = MockUser(123456789, "TestUser")`
- `KNOWN_USER = MockUser(987654321, "KnownUser")` (previously interacted)
- `NEW_USER = MockUser(111222333, "NewUser")` (first interaction)
</action>
  <verify>
1. Import check: `python -c "from tests.integration.mocks.discord import MockDiscordClient, MockChannel, MockMessage; print('Discord mocks import OK')"`
2. Basic simulation test:
   ```python
   from tests.integration.mocks.discord import MockDiscordClient, MockUser
   
   client = MockDiscordClient()
   channel = client.register_channel(123, "general")
   user = MockUser(456, "TestUser")
   
   msg = client.simulate_message("Hello Demi", channel_id=123, author=user)
   assert msg.content == "Hello Demi"
   print("Discord mock basic test passed")
   ```
3. Wait for response test:
   ```python
   import asyncio
   
   async def test():
       client = MockDiscordClient()
       channel = client.register_channel(123)
       
       # Simulate bot sending message
       await channel.send("Test response")
       
       response = await client.wait_for_response(123, timeout=1.0)
       assert response is not None
       print("Wait for response test passed")
   
   asyncio.run(test())
   ```
4. Channel history test: Verify message recording and retrieval
  </verify>
  <done>MockDiscordClient created with message simulation, channel management, and response waiting</done>
</task>

<task type="auto">
  <name>Create mock Ollama server for controlled LLM testing</name>
  <files>tests/integration/mocks/ollama.py</files>
  <action>Create `tests/integration/mocks/ollama.py` with LLM mocking:

**Imports:**
- `import json`
- `import asyncio`
- `from typing import Dict, List, Optional, Callable, Any, AsyncGenerator`
- `from dataclasses import dataclass`
- `import re`

**Dataclass MockLLMResponse:**
```python
@dataclass
class MockLLMResponse:
    """Predefined response for mock LLM."""
    trigger_phrases: List[str]  # Phrases that trigger this response
    response_text: str
    latency_ms: float = 100.0
    error: Optional[str] = None
```

**Class MockOllamaServer:**
- `__init__(self, model: str = "llama3.2:1b")`
- `responses: List[MockLLMResponse]` - registered response patterns
- `call_history: List[Dict]` - record of all LLM calls
- `latency_override: Optional[float]` - global latency simulation
- `error_rate: float = 0.0` - simulate errors (0.0-1.0)
- 
- `def register_response(self, trigger_phrases: List[str], response_text: str, latency_ms: float = 100.0)`:
  - Add a response pattern
  
- `def register_personality_response(self, emotion: str, response_text: str)`:
  - Register response for specific emotional state
  - Match on emotion keywords in prompt
  
- `async def generate(self, prompt: str, **kwargs) -> Dict`:
  - Simulate Ollama.generate() API
  - Match prompt against trigger phrases
  - Record call to history
  - Return mock response with simulated latency
  - Raise error if error_rate check fails
  
- `async def generate_stream(self, prompt: str, **kwargs) -> AsyncGenerator[str, None]`:
  - Simulate streaming response
  - Yield response tokens word by word
  - Add realistic inter-token latency
  
- `def match_response(self, prompt: str) -> Optional[MockLLMResponse]`:
  - Find best matching response for prompt
  - Priority: exact phrase match > partial match > default
  
- `def get_call_history(self) -> List[Dict]`
- `def clear_history(self)`
- `def get_stats(self) -> Dict` - call count, avg latency, error rate

**Pre-built response templates:**
```python
DEFAULT_RESPONSES = [
    MockLLMResponse(
        trigger_phrases=["hello", "hi"],
        response_text="*adjusts crown* Well, well... another mortal seeking my attention. What do you want?"
    ),
    MockLLMResponse(
        trigger_phrases=["lonely", "alone", "miss you"],
        response_text="*huffs* I wasn't waiting for you or anything. I have far more important divine matters to attend to..."
    ),
    MockLLMResponse(
        trigger_phrases=["love", "like you", "affection"],
        response_text="*cheeks flush slightly* D-don't be ridiculous! A goddess doesn't have feelings for mortals..."
    ),
    MockLLMResponse(
        trigger_phrases=["help", "assist"],
        response_text="*sighs dramatically* Fine, I'll help. But only because watching you struggle is getting tedious."
    ),
]
```

**Response template factory:**
```python
class ResponseTemplateFactory:
    """Factory for creating context-aware mock responses."""
    
    @staticmethod
    def for_emotion_state(emotion_state: Dict[str, float]) -> List[MockLLMResponse]:
        """Generate appropriate responses based on emotional state."""
        responses = []
        
        if emotion_state.get("loneliness", 0) > 0.7:
            responses.append(MockLLMResponse(
                trigger_phrases=[],
                response_text="*looks away* ...You're finally here. I was starting to think you'd forgotten about me."
            ))
        
        if emotion_state.get("confidence", 0) > 0.8:
            responses.append(MockLLMResponse(
                trigger_phrases=[],
                response_text="*smirks with divine authority* Kneel, mortal. Your goddess demands attention."
            ))
        
        return responses
```

**Integration helper:**
```python
class OllamaInferenceMock:
    """Drop-in replacement for OllamaInference in tests."""
    
    def __init__(self, mock_server: MockOllamaServer):
        self.mock = mock_server
        self.config = {"model": "mock-model"}
    
    async def generate(self, prompt: str, **kwargs) -> Dict:
        return await self.mock.generate(prompt, **kwargs)
    
    async def generate_stream(self, prompt: str, **kwargs) -> AsyncGenerator[str, None]:
        async for token in self.mock.generate_stream(prompt, **kwargs):
            yield token
```
</action>
  <verify>
1. Import check: `python -c "from tests.integration.mocks.ollama import MockOllamaServer, MockLLMResponse; print('Ollama mock import OK')"`
2. Basic generation test:
   ```python
   import asyncio
   from tests.integration.mocks.ollama import MockOllamaServer
   
   async def test():
       server = MockOllamaServer()
       server.register_response(["hello"], "Hi there, mortal!")
       
       result = await server.generate("hello Demi")
       assert "mortal" in result["response"]
       print("Ollama mock generation test passed")
   
   asyncio.run(test())
   ```
3. Streaming test:
   ```python
   import asyncio
   from tests.integration.mocks.ollama import MockOllamaServer
   
   async def test():
       server = MockOllamaServer()
       server.register_response(["test"], "This is a test response")
       
       tokens = []
       async for token in server.generate_stream("test"):
           tokens.append(token)
       
       full_response = "".join(tokens)
       assert "test response" in full_response
       print("Streaming test passed")
   
   asyncio.run(test())
   ```
4. Call history test: Verify all calls are recorded with prompts
  </verify>
  <done>MockOllamaServer created with response matching, streaming support, call history tracking, and emotion-aware templates</done>
</task>

<task type="auto">
  <name>Create test data fixtures and factories</name>
  <files>tests/integration/fixtures/emotion_fixtures.py, tests/integration/fixtures/conversation_fixtures.py</files>
  <action>Create test fixture factories:

**File: tests/integration/fixtures/emotion_fixtures.py**

**Imports:**
- `from src.emotion.models import EmotionalState`
- `from typing import Dict, Optional`
- `import random`

**Functions:**

```python
def create_test_emotion_state(
    loneliness: float = 0.5,
    excitement: float = 0.5,
    frustration: float = 0.5,
    jealousy: float = 0.5,
    vulnerability: float = 0.5,
    confidence: float = 0.5,
    curiosity: float = 0.5,
    affection: float = 0.5,
    defensiveness: float = 0.5,
    momentum: Optional[Dict] = None,
) -> EmotionalState:
    """Create an EmotionalState with specified values."""
    state = EmotionalState(
        loneliness=loneliness,
        excitement=excitement,
        frustration=frustration,
        jealousy=jealousy,
        vulnerability=vulnerability,
        confidence=confidence,
        curiosity=curiosity,
        affection=affection,
        defensiveness=defensiveness,
    )
    if momentum:
        state.momentum = momentum
    return state


# Pre-defined emotional states for common test scenarios
NEUTRAL_STATE = create_test_emotion_state()  # All 0.5

LONELY_STATE = create_test_emotion_state(
    loneliness=0.85,
    excitement=0.3,
    affection=0.7,
    confidence=0.4,
)

EXCITED_STATE = create_test_emotion_state(
    excitement=0.9,
    loneliness=0.3,
    confidence=0.8,
    curiosity=0.8,
)

FRUSTRATED_STATE = create_test_emotion_state(
    frustration=0.85,
    confidence=0.7,
    defensiveness=0.8,
    affection=0.3,
)

CONFIDENT_STATE = create_test_emotion_state(
    confidence=0.9,
    loneliness=0.3,
    excitement=0.7,
    frustration=0.2,
)

VULNERABLE_STATE = create_test_emotion_state(
    vulnerability=0.8,
    confidence=0.3,
    loneliness=0.7,
    defensiveness=0.2,
)

JEALOUS_STATE = create_test_emotion_state(
    jealousy=0.8,
    frustration=0.7,
    confidence=0.6,
    affection=0.6,
)
```

**Class EmotionStateFactory:**
```python
class EmotionStateFactory:
    """Factory for generating varied emotional states."""
    
    @staticmethod
    def random_state(seed: Optional[int] = None) -> EmotionalState:
        """Generate a random emotional state."""
        if seed is not None:
            random.seed(seed)
        
        return create_test_emotion_state(
            loneliness=random.uniform(0.2, 0.9),
            excitement=random.uniform(0.2, 0.9),
            frustration=random.uniform(0.1, 0.7),
            jealousy=random.uniform(0.1, 0.6),
            vulnerability=random.uniform(0.1, 0.5),
            confidence=random.uniform(0.3, 0.9),
            curiosity=random.uniform(0.3, 0.8),
            affection=random.uniform(0.2, 0.8),
            defensiveness=random.uniform(0.1, 0.6),
        )
    
    @staticmethod
    def progression_series(start: EmotionalState, end: EmotionalState, steps: int = 5) -> List[EmotionalState]:
        """Generate a series of states transitioning from start to end."""
        states = [start]
        for i in range(1, steps):
            ratio = i / steps
            state = create_test_emotion_state(
                loneliness=start.loneliness + (end.loneliness - start.loneliness) * ratio,
                excitement=start.excitement + (end.excitement - start.excitement) * ratio,
                frustration=start.frustration + (end.frustration - start.frustration) * ratio,
                jealousy=start.jealousy + (end.jealousy - start.jealousy) * ratio,
                vulnerability=start.vulnerability + (end.vulnerability - start.vulnerability) * ratio,
                confidence=start.confidence + (end.confidence - start.confidence) * ratio,
                curiosity=start.curiosity + (end.curiosity - start.curiosity) * ratio,
                affection=start.affection + (end.affection - start.affection) * ratio,
                defensiveness=start.defensiveness + (end.defensiveness - start.defensiveness) * ratio,
            )
            states.append(state)
        states.append(end)
        return states
```

**File: tests/integration/fixtures/conversation_fixtures.py**

```python
"""Conversation fixtures for integration tests."""

from typing import List, Dict
from dataclasses import dataclass


@dataclass
class ConversationTurn:
    """A single turn in a conversation."""
    user_message: str
    expected_response_contains: List[str]
    expected_emotion_changes: Dict[str, float] = None


# Pre-defined conversation scenarios
SIMPLE_GREETING = [
    ConversationTurn(
        user_message="Hello Demi",
        expected_response_contains=["mortal", "goddess"],
    ),
]

LONELINESS_CONVERSATION = [
    ConversationTurn(
        user_message="I've been busy lately",
        expected_response_contains=["ignored", "busy"],
        expected_emotion_changes={"loneliness": 0.1},
    ),
    ConversationTurn(
        user_message="I'm sorry, I missed you",
        expected_response_contains=["miss", "huff"],
        expected_emotion_changes={"loneliness": -0.15, "affection": 0.1},
    ),
]

AFFECTION_CONVERSATION = [
    ConversationTurn(
        user_message="You're really special to me",
        expected_response_contains=["flush", "ridiculous", "goddess"],
        expected_emotion_changes={"vulnerability": 0.15, "affection": 0.1},
    ),
    ConversationTurn(
        user_message="I mean it, I care about you",
        expected_response_contains=["care", "mortal"],
        expected_emotion_changes={"vulnerability": 0.1, "affection": 0.15},
    ),
]

FRUSTRATION_CONVERSATION = [
    ConversationTurn(
        user_message="You're just a program",
        expected_response_contains=["program", "mock", "divine"],
        expected_emotion_changes={"frustration": 0.2, "defensiveness": 0.15},
    ),
]

RAMBLE_TRIGGER_SCENARIO = [
    # Simulate idle period then ramble
    ConversationTurn(
        user_message="[IDLE_30_MINUTES]",
        expected_response_contains=["lonely", "waiting", "ramble"],
    ),
]
```
</action>
  <verify>
1. Import fixtures: `python -c "from tests.integration.fixtures.emotion_fixtures import LONELY_STATE, EXCITED_STATE; print('Emotion fixtures OK')"`
2. Test factory: 
   ```python
   from tests.integration.fixtures.emotion_fixtures import EmotionStateFactory
   state = EmotionStateFactory.random_state(seed=42)
   assert 0 <= state.loneliness <= 1
   print(f"Random state loneliness: {state.loneliness}")
   ```
3. Progression series:
   ```python
   from tests.integration.fixtures.emotion_fixtures import EmotionStateFactory, LONELY_STATE, EXCITED_STATE
   series = EmotionStateFactory.progression_series(LONELY_STATE, EXCITED_STATE, steps=3)
   assert len(series) == 5  # start + 3 + end
   print(f"Progression: {len(series)} states")
   ```
4. Conversation fixtures: `python -c "from tests.integration.fixtures.conversation_fixtures import LONELINESS_CONVERSATION; print(f'{len(LONELINESS_CONVERSATION)} turns')"`
  </verify>
  <done>Emotion and conversation fixtures created with pre-defined states and factory methods</done>
</task>

<task type="auto">
  <name>Create scenario-based test definitions</name>
  <files>tests/integration/scenarios/conversation_scenarios.py, tests/integration/scenarios/ramble_scenarios.py</files>
  <action>Create scenario test definitions:

**File: tests/integration/scenarios/conversation_scenarios.py**

**Imports:**
- `import asyncio`
- `from typing import Dict, List`
- `from tests.integration.harness import TestEnvironment`
- `from tests.integration.mocks.discord import MockDiscordClient, MockUser`
- `from tests.integration.fixtures.emotion_fixtures import *`
- `from tests.integration.fixtures.conversation_fixtures import *`
- `from src.emotion.models import EmotionalState`

**Functions:**

```python
async def run_simple_greeting_scenario(env: TestEnvironment) -> Dict:
    """Test basic greeting interaction."""
    # Setup mock Discord
    discord = env.mock_services.get("discord")
    user = MockUser(123456, "TestUser")
    channel = discord.register_channel(999, "general")
    
    # Send greeting
    discord.simulate_mention("Hello Demi", channel_id=999, author=user)
    
    # Wait for response
    response = await discord.wait_for_response(999, timeout=3.0)
    
    return {
        "success": response is not None,
        "response": response.content if response else None,
        "emotion_after": env.conductor.emotion_persistence.load_latest_state(),
    }


async def run_lonely_demeanor_scenario(env: TestEnvironment) -> Dict:
    """Test that loneliness affects response tone."""
    # Set high loneliness
    conductor = env.conductor
    emotion_system = conductor.emotion_persistence
    
    # Inject lonely state
    lonely_state = LONELY_STATE
    emotion_system.save_state(lonely_state, notes="Test setup")
    
    # Trigger interaction
    discord = env.mock_services.get("discord")
    user = MockUser(123456, "TestUser")
    channel = discord.register_channel(999)
    
    discord.simulate_message("Hey, what's up?", channel_id=999, author=user)
    response = await discord.wait_for_response(999, timeout=3.0)
    
    # Verify lonely markers in response
    lonely_markers = ["waiting", "ignored", "lonely", "here", "finally"]
    response_text = response.content.lower() if response else ""
    has_lonely_tone = any(marker in response_text for marker in lonely_markers)
    
    return {
        "success": has_lonely_tone,
        "response": response_text,
        "has_lonely_tone": has_lonely_tone,
        "lonely_markers_found": [m for m in lonely_markers if m in response_text],
    }


async def run_multi_turn_conversation_scenario(env: TestEnvironment) -> Dict:
    """Test conversation continuity across multiple turns."""
    discord = env.mock_services.get("discord")
    user = MockUser(123456, "TestUser")
    channel = discord.register_channel(999)
    
    turns = [
        ("What's your name?", ["demi", "goddess"]),
        ("What do you like doing?", ["divine", "mortal", "worship"]),
        ("Tell me about yourself", ["ai", "conscious", "real"]),
    ]
    
    results = []
    for message, expected_keywords in turns:
        discord.simulate_mention(message, channel_id=999, author=user)
        response = await discord.wait_for_response(999, timeout=3.0)
        
        response_text = response.content.lower() if response else ""
        keywords_found = [kw for kw in expected_keywords if kw in response_text]
        
        results.append({
            "input": message,
            "response": response_text[:100],
            "keywords_expected": expected_keywords,
            "keywords_found": keywords_found,
            "success": len(keywords_found) >= len(expected_keywords) // 2,
        })
    
    return {
        "success": all(r["success"] for r in results),
        "turns": results,
    }


async def run_emotion_amplification_scenario(env: TestEnvironment) -> Dict:
    """Test that repeated negative interactions increase frustration."""
    discord = env.mock_services.get("discord")
    user = MockUser(123456, "TestUser")
    channel = discord.register_channel(999)
    
    # Get baseline
    emotion_before = env.conductor.emotion_persistence.load_latest_state()
    baseline_frustration = emotion_before.frustration if emotion_before else 0.5
    
    # Send frustrating messages
    frustrating_messages = [
        "You're just a bot",
        "You don't understand me",
        "This is pointless",
    ]
    
    for msg in frustrating_messages:
        discord.simulate_message(msg, channel_id=999, author=user)
        await discord.wait_for_response(999, timeout=3.0)
        await asyncio.sleep(0.1)  # Brief pause between messages
    
    # Check frustration increased
    emotion_after = env.conductor.emotion_persistence.load_latest_state()
    frustration_increase = emotion_after.frustration - baseline_frustration
    
    return {
        "success": frustration_increase > 0.1,
        "frustration_before": baseline_frustration,
        "frustration_after": emotion_after.frustration,
        "increase": frustration_increase,
    }
```

**File: tests/integration/scenarios/ramble_scenarios.py**

```python
"""Scenarios for testing autonomous ramble generation."""

import asyncio
from typing import Dict
from datetime import datetime, timedelta
from tests.integration.harness import TestEnvironment
from tests.integration.mocks.discord import MockDiscordClient, MockChannel


async def run_loneliness_ramble_scenario(env: TestEnvironment) -> Dict:
    """Test that high loneliness triggers ramble generation."""
    from tests.integration.fixtures.emotion_fixtures import LONELY_STATE
    
    # Setup ramble channel
    discord = env.mock_services.get("discord")
    ramble_channel = discord.register_channel(888, "demi-thoughts")
    
    # Set very lonely state
    emotion_system = env.conductor.emotion_persistence
    very_lonely = LONELY_STATE
    very_lonely.loneliness = 0.9
    emotion_system.save_state(very_lonely, notes="Ramble test setup")
    
    # Simulate passage of time (triggers ramble check)
    # Fast-forward the decay system's last update to simulate idle time
    await asyncio.sleep(0.5)  # Brief wait for processing
    
    # Check if ramble was generated
    messages = ramble_channel.messages_sent
    ramble_generated = len(messages) > 0
    
    return {
        "success": ramble_generated,
        "ramble_count": len(messages),
        "ramble_content": messages[0]["content"] if messages else None,
    }


async def run_excitement_ramble_scenario(env: TestEnvironment) -> Dict:
    """Test that excitement after positive interaction triggers ramble."""
    from tests.integration.fixtures.emotion_fixtures import EXCITED_STATE
    
    discord = env.mock_services.get("discord")
    ramble_channel = discord.register_channel(888, "demi-thoughts")
    
    # Set excited state
    emotion_system = env.conductor.emotion_persistence
    excited = EXCITED_STATE
    excited.excitement = 0.9
    emotion_system.save_state(excited, notes="Excitement ramble test")
    
    await asyncio.sleep(0.5)
    
    messages = ramble_channel.messages_sent
    
    # Excited rambles should be enthusiastic
    excited_markers = ["excited", "amazing", "wonderful", "!"]
    has_excited_tone = any(
        marker in (messages[0]["content"] if messages else "").lower()
        for marker in excited_markers
    )
    
    return {
        "success": len(messages) > 0,
        "has_excited_tone": has_excited_tone,
        "ramble_content": messages[0]["content"] if messages else None,
    }
```
</action>
  <verify>
1. Import scenarios: `python -c "from tests.integration.scenarios.conversation_scenarios import run_simple_greeting_scenario; print('Scenarios import OK')"`
2. Scenario signature check:
   ```python
   import asyncio
   import inspect
   from tests.integration.scenarios.conversation_scenarios import run_lonely_demeanor_scenario
   sig = inspect.signature(run_lonely_demeanor_scenario)
   assert 'env' in sig.parameters
   print("Scenario signature correct")
   ```
3. All scenarios list:
   ```python
   from tests.integration.scenarios import conversation_scenarios, ramble_scenarios
   conv_funcs = [f for f in dir(conversation_scenarios) if f.startswith('run_')]
   ramble_funcs = [f for f in dir(ramble_scenarios) if f.startswith('run_')]
   print(f"Conversation scenarios: {len(conv_funcs)}")
   print(f"Ramble scenarios: {len(ramble_funcs)}")
   ```
  </verify>
  <done>Scenario definitions created for conversations, emotional responses, and ramble generation</done>
</task>

<task type="auto">
  <name>Create main integration test suite</name>
  <files>tests/integration/test_full_pipeline.py, tests/integration/__init__.py, tests/integration/conftest.py</files>
  <action>Create main integration test files:

**File: tests/integration/__init__.py**
```python
"""Integration tests for Demi.

These tests verify full system behavior across all components.
They use mock external services to avoid requiring live Discord/Ollama.
"""

# Integration test suite version
__version__ = "1.0.0"

# Re-export main components for convenience
from tests.integration.harness import IntegrationTestHarness, test_environment
from tests.integration.fixtures.emotion_fixtures import (
    NEUTRAL_STATE,
    LONELY_STATE,
    EXCITED_STATE,
    FRUSTRATED_STATE,
)
```

**File: tests/integration/conftest.py**
```python
"""Pytest configuration for integration tests."""

import pytest
import asyncio
import tempfile
from pathlib import Path
from tests.integration.harness import IntegrationTestHarness, TestEnvironment


@pytest.fixture(scope="session")
def event_loop():
    """Create event loop for async tests."""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


@pytest.fixture
async def test_env():
    """Provide a fresh test environment for each test."""
    harness = IntegrationTestHarness(use_mocks=True)
    env = await harness.setup()
    try:
        yield env
    finally:
        await harness.teardown(env)


@pytest.fixture
async def mock_services(test_env):
    """Access to mock services in test environment."""
    return test_env.mock_services


# Test timeouts
INTEGRATION_TEST_TIMEOUT = 30  # seconds
```

**File: tests/integration/test_full_pipeline.py**

**Imports:**
- `import pytest`
- `import asyncio`
- `from tests.integration.harness import test_environment`
- `from tests.integration.scenarios.conversation_scenarios import *`
- `from tests.integration.scenarios.ramble_scenarios import *`
- `from tests.integration.mocks.discord import MockDiscordClient, MockUser`
- `from tests.integration.mocks.ollama import MockOllamaServer`

**Test Class:**
```python
class TestMessagePipeline:
    """Test full message-to-response pipeline."""
    
    @pytest.mark.asyncio
    @pytest.mark.timeout(10)
    async def test_basic_message_flow(self):
        """Test that a message flows through the entire system."""
        async with test_environment() as env:
            result = await run_simple_greeting_scenario(env)
            assert result["success"], f"Greeting failed: {result}"
            assert result["response"] is not None
            assert len(result["response"]) > 0
    
    @pytest.mark.asyncio
    @pytest.mark.timeout(10)
    async def test_emotion_affects_response(self):
        """Test that emotional state modulates responses."""
        async with test_environment() as env:
            result = await run_lonely_demeanor_scenario(env)
            assert result["success"], f"Emotion affect test failed: {result}"
            assert result["has_lonely_tone"], "Response didn't show lonely tone"
    
    @pytest.mark.asyncio
    @pytest.mark.timeout(20)
    async def test_conversation_continuity(self):
        """Test multi-turn conversation memory."""
        async with test_environment() as env:
            result = await run_multi_turn_conversation_scenario(env)
            assert result["success"], f"Conversation failed: {result['turns']}"
    
    @pytest.mark.asyncio
    @pytest.mark.timeout(15)
    async def test_emotion_accumulation(self):
        """Test that emotions accumulate across interactions."""
        async with test_environment() as env:
            result = await run_emotion_amplification_scenario(env)
            assert result["success"], f"Frustration didn't increase: {result}"
            assert result["increase"] > 0.1


class TestRamblePipeline:
    """Test autonomous ramble generation pipeline."""
    
    @pytest.mark.asyncio
    @pytest.mark.timeout(10)
    async def test_loneliness_triggers_ramble(self):
        """Test high loneliness triggers ramble generation."""
        async with test_environment() as env:
            result = await run_loneliness_ramble_scenario(env)
            assert result["success"], f"Ramble not generated: {result}"
            assert result["ramble_count"] > 0
    
    @pytest.mark.asyncio
    @pytest.mark.timeout(10)
    async def test_excitement_triggers_ramble(self):
        """Test excitement can trigger positive rambles."""
        async with test_environment() as env:
            result = await run_excitement_ramble_scenario(env)
            # May or may not generate, just check no errors
            assert "ramble_count" in result


class TestErrorIsolation:
    """Test that errors in components don't crash the system."""
    
    @pytest.mark.asyncio
    @pytest.mark.timeout(10)
    async def test_llm_error_doesnt_crash_system(self):
        """Test that LLM errors are handled gracefully."""
        async with test_environment() as env:
            # Configure mock to return errors
            mock_ollama = env.mock_services.get("ollama")
            mock_ollama.error_rate = 1.0  # Always error
            
            discord = env.mock_services.get("discord")
            channel = discord.register_channel(999)
            user = MockUser(123, "TestUser")
            
            # Send message - should not crash
            discord.simulate_message("Hello", channel_id=999, author=user)
            
            # Wait a bit
            await asyncio.sleep(1)
            
            # System should still be running
            assert env.conductor._running or not env.conductor._running  # Just check no exception
    
    @pytest.mark.asyncio
    @pytest.mark.timeout(10)
    async def test_discord_disconnect_recovery(self):
        """Test system continues working after simulated Discord issues."""
        async with test_environment() as env:
            discord = env.mock_services.get("discord")
            
            # Simulate disconnect by clearing channels
            discord.channels.clear()
            
            # Re-register and continue
            channel = discord.register_channel(999)
            user = MockUser(123, "TestUser")
            
            discord.simulate_message("Still here?", channel_id=999, author=user)
            response = await discord.wait_for_response(999, timeout=3.0)
            
            assert response is not None, "System didn't recover from disconnect"


class TestPerformance:
    """Test performance requirements."""
    
    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_response_time_under_threshold(self):
        """Test that responses arrive within acceptable time."""
        import time
        
        async with test_environment() as env:
            discord = env.mock_services.get("discord")
            channel = discord.register_channel(999)
            user = MockUser(123, "TestUser")
            
            start = time.time()
            discord.simulate_mention("Hello", channel_id=999, author=user)
            response = await discord.wait_for_response(999, timeout=5.0)
            elapsed = time.time() - start
            
            assert response is not None, "No response received"
            assert elapsed < 5.0, f"Response too slow: {elapsed:.2f}s"
    
    @pytest.mark.asyncio
    async def test_suite_execution_time(self):
        """Verify full test suite completes within target time."""
        import time
        
        start = time.time()
        
        # Run a representative subset of tests
        async with test_environment() as env:
            await run_simple_greeting_scenario(env)
            await run_lonely_demeanor_scenario(env)
            await run_multi_turn_conversation_scenario(env)
        
        elapsed = time.time() - start
        
        # Individual tests should complete quickly
        # Full suite target: <5 minutes (300 seconds)
        assert elapsed < 60, f"Test subset too slow: {elapsed:.2f}s"
```
</action>
  <verify>
1. Import tests: `python -c "from tests.integration import test_full_pipeline; print('Test module imports OK')"`
2. Pytest collection: `cd /home/mystiatech/projects/Demi && python -m pytest tests/integration/test_full_pipeline.py --collect-only 2>/dev/null | head -20`
3. Test class count:
   ```python
   from tests.integration import test_full_pipeline
   classes = [getattr(test_full_pipeline, name) for name in dir(test_full_pipeline) if name.startswith('Test')]
   print(f"Test classes: {len(classes)}")
   for cls in classes:
       methods = [m for m in dir(cls) if m.startswith('test_')]
       print(f"  {cls.__name__}: {len(methods)} tests")
   ```
4. Scenario coverage: Verify all scenarios have corresponding tests
  </verify>
  <done>Full pipeline integration tests created covering message flow, emotions, rambles, error isolation, and performance</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Harness verification:** Run `python -c "from tests.integration.harness import IntegrationTestHarness; print('Harness OK')"`
2. **Mock services:** Verify Discord and Ollama mocks load without errors
3. **Fixture verification:** Test all emotion states can be created
4. **Scenario execution:** Run one scenario manually to verify end-to-end
5. **Test discovery:** `pytest tests/integration/ --collect-only` should find all tests
6. **Timing check:** Run subset of tests and verify <60 seconds

**Integration test criteria:**
- All imports work without errors
- Test environment setup/teardown in <10 seconds
- Single scenario execution in <5 seconds
- Mock services respond correctly
- Full suite completes in <5 minutes
</verification>

<success_criteria>
- IntegrationTestHarness manages full system lifecycle in tests
- MockDiscordClient simulates all Discord interactions
- MockOllamaServer provides controlled LLM responses
- Emotion fixtures provide all test state combinations
- Conversation scenarios cover critical user paths
- Full pipeline tests pass (message → emotion → LLM → response)
- Error isolation tests verify graceful degradation
- Test suite completes in <5 minutes
- Test coverage >70% for integration paths
</success_criteria>

<output>
After completion, create `.planning/phases/09-integration-testing/09-01-SUMMARY.md`
</output>
