---
phase: 09-integration-testing
plan: 03
type: execute
wave: 1
depends_on: [09-01]
files_modified:
  - src/monitoring/memory_profiler.py
  - src/monitoring/__init__.py
  - tests/stability/memory_leak_test.py
  - tests/stability/resource_cleanup_test.py
  - scripts/profile_memory.sh
  - .demi/memory_profile_config.yaml
autonomous: true
must_haves:
  truths:
    - "Memory profiling must track both process-level and object-level usage"
    - "Leak detection requires baseline comparison over time"
    - "Resource cleanup verification ensures no dangling references"
    - "10GB limit requires aggressive monitoring and alerting"
  artifacts:
    - path: "src/monitoring/memory_profiler.py"
      provides: "Memory profiling and leak detection system"
      exports: ["MemoryProfiler", "MemorySnapshot", "LeakDetector"]
      min_lines: 250
    - path: "src/monitoring/__init__.py"
      provides: "Monitoring module exports"
      exports: ["MemoryProfiler", "get_memory_profiler"]
      min_lines: 30
    - path: "tests/stability/memory_leak_test.py"
      provides: "Memory leak detection tests"
      exports: ["test_memory_leak_conversations", "test_memory_leak_emotions"]
      min_lines: 150
    - path: "tests/stability/resource_cleanup_test.py"
      provides: "Resource cleanup verification tests"
      exports: ["test_discord_cleanup", "test_llm_cleanup"]
      min_lines: 100
    - path: "scripts/profile_memory.sh"
      provides: "Memory profiling script for long-running tests"
      exports: []
      min_lines: 50
  key_links:
    - from: "src/monitoring/memory_profiler.py"
      to: "src/conductor/resource_monitor.py"
      via: "Integrates with ResourceMonitor for system-level metrics"
      pattern: "ResourceMonitor.get_current_metrics"
    - from: "src/monitoring/memory_profiler.py"
      to: "src/emotion/persistence.py"
      via: "Tracks EmotionPersistence memory usage"
      pattern: "track_object_memory"
---

<objective>
Build Memory Profiling & Leak Detection system that ensures Demi stays below 10GB sustained memory usage (with 12GB available). This includes process-level memory tracking, object-level profiling, leak detection algorithms, and resource cleanup verification integrated with the existing ResourceMonitor.

Purpose: Satisfy HEALTH-02 requirement (<10GB sustained memory) by detecting and preventing memory leaks, tracking allocation patterns, and ensuring proper resource cleanup across all components.

Output: Comprehensive memory profiling system with automated leak detection, integration with ResourceMonitor, and test suite validating <5% memory growth over 7 days.
</objective>

<execution_context>
@.planning/REQUIREMENTS.md
@src/conductor/resource_monitor.py
@src/emotion/persistence.py
</execution_context>

<context>
@src/conductor/resource_monitor.py
@src/conductor/orchestrator.py
@src/llm/inference.py
@src/emotion/persistence.py
@src/platforms/discord/bot.py
</context>

<tasks>

<task type="auto">
  <name>Create memory profiler with object tracking</name>
  <files>src/monitoring/memory_profiler.py</files>
  <action>Create `src/monitoring/memory_profiler.py` with comprehensive memory profiling:

**Imports:**
- `import gc`
- `import sys`
- `import tracemalloc`
- `import psutil`
- `import asyncio`
- `from typing import Dict, List, Optional, Any, Set`
- `from dataclasses import dataclass, field`
- `from datetime import datetime`
- `from collections import defaultdict`
- `import weakref`
- `import objgraph`  # Optional, for object graph analysis
- `from src.core.logger import get_logger`
- `from src.conductor.resource_monitor import ResourceMonitor`

**Dataclass MemorySnapshot:**
```python
@dataclass
class MemorySnapshot:
    """Snapshot of memory usage at a point in time."""
    timestamp: datetime
    process_rss_mb: float
    process_vms_mb: float
    python_objects_mb: float
    top_allocations: List[Dict[str, Any]] = field(default_factory=list)
    gc_objects_count: int = 0
    gc_generations: Tuple[int, int, int] = field(default_factory=lambda: (0, 0, 0))
    
    def to_dict(self) -> Dict:
        return {
            "timestamp": self.timestamp.isoformat(),
            "process_rss_mb": self.process_rss_mb,
            "process_vms_mb": self.process_vms_mb,
            "python_objects_mb": self.python_objects_mb,
            "gc_objects_count": self.gc_objects_count,
        }
```

**Dataclass TrackedObject:**
```python
@dataclass
class TrackedObject:
    """A tracked object for memory monitoring."""
    name: str
    obj_ref: weakref.ref
    initial_size_mb: float
    creation_time: datetime
    snapshots: List[Dict] = field(default_factory=list)
    
    @property
    def is_alive(self) -> bool:
        return self.obj_ref() is not None
    
    def current_size_estimate(self) -> float:
        """Estimate current size using heuristics if object is alive."""
        obj = self.obj_ref()
        if obj is None:
            return 0.0
        # Use sys.getsizeof for rough estimate
        try:
            return sys.getsizeof(obj) / (1024 * 1024)
        except:
            return self.initial_size_mb
```

**Class MemoryProfiler:**
- `__init__(self, warning_threshold_mb: float = 8192, critical_threshold_mb: float = 10240)`:
  - warning: 8GB (alert when approaching)
  - critical: 10GB (must not exceed)
  - Initialize tracemalloc for detailed tracking
  - Setup tracked objects registry
  - Connect to ResourceMonitor
  
- `async def start_profiling(self, interval_seconds: int = 60)`:
  - Start tracemalloc tracking
  - Begin background snapshot collection
  - Register with ResourceMonitor
  
- `async def stop_profiling(self)`:
  - Stop background collection
  - Final snapshot
  - Generate report
  
- `async def take_snapshot(self) -> MemorySnapshot`:
  - Collect process memory via psutil
  - Get Python object statistics via tracemalloc
  - Count GC objects
  - Get top memory allocations
  - Return snapshot
  
- `def track_object(self, obj: Any, name: str) -> str`:
  - Register an object for individual tracking
  - Use weakref to avoid preventing GC
  - Return tracking ID
  
- `def untrack_object(self, tracking_id: str)`:
  - Remove object from tracking
  
- `def get_tracked_objects_report(self) -> Dict[str, Any]`:
  - Report on all tracked objects
  - Show growth over time
  - Identify potential leaks
  
- `async def _profiling_loop(self, interval_seconds: int)`:
  - Background loop taking periodic snapshots
  - Check thresholds and alert
  - Store history for trend analysis
  
- `def check_thresholds(self, snapshot: MemorySnapshot) -> List[str]`:
  - Check against warning/critical thresholds
  - Return list of alerts
  
- `def get_growth_rate(self, hours: float = 24) -> float`:
  - Calculate memory growth rate (% per hour)
  - Use linear regression on historical data
  - Return percentage growth

**Detailed allocation tracking:**
```python
def get_top_allocations(self, limit: int = 10) -> List[Dict]:
    """Get top memory allocations by size."""
    if not tracemalloc.is_tracing():
        return []
    
    snapshot = tracemalloc.take_snapshot()
    top_stats = snapshot.statistics('lineno')[:limit]
    
    allocations = []
    for stat in top_stats:
        allocations.append({
            "file": stat.traceback.format()[-1] if stat.traceback else "unknown",
            "size_mb": stat.size / (1024 * 1024),
            "count": stat.count,
        })
    
    return allocations


def get_object_type_counts(self) -> Dict[str, int]:
    """Get counts of objects by type (useful for leak detection)."""
    gc.collect()  # Clean up first
    counts = defaultdict(int)
    
    for obj in gc.get_objects():
        obj_type = type(obj).__name__
        counts[obj_type] += 1
    
    return dict(sorted(counts.items(), key=lambda x: x[1], reverse=True)[:50])
```

**Growth analysis:**
```python
def analyze_growth(
    self,
    snapshot1: MemorySnapshot,
    snapshot2: MemorySnapshot
) -> Dict[str, Any]:
    """Analyze memory growth between two snapshots."""
    rss_growth_mb = snapshot2.process_rss_mb - snapshot1.process_rss_mb
    hours_elapsed = (snapshot2.timestamp - snapshot1.timestamp).total_seconds() / 3600
    
    growth_rate_per_hour = rss_growth_mb / hours_elapsed if hours_elapsed > 0 else 0
    growth_percent = (rss_growth_mb / snapshot1.process_rss_mb) * 100 if snapshot1.process_rss_mb > 0 else 0
    
    return {
        "rss_growth_mb": rss_growth_mb,
        "hours_elapsed": hours_elapsed,
        "growth_rate_mb_per_hour": growth_rate_per_hour,
        "growth_percent": growth_percent,
        "is_concerning": growth_percent > 5,  # >5% growth is concerning
    }
```

**Integration with ResourceMonitor:**
```python
def integrate_with_resource_monitor(self, resource_monitor: ResourceMonitor):
    """Connect memory profiler to existing resource monitoring."""
    self._resource_monitor = resource_monitor
    
    # Override the resource monitor's collection to include our data
    original_collect = resource_monitor.collect_metrics
    
    async def enhanced_collect():
        # Call original
        result = await original_collect()
        
        # Add memory profiling data
        snapshot = await self.take_snapshot()
        result["python_memory_mb"] = snapshot.python_objects_mb
        result["gc_objects"] = snapshot.gc_objects_count
        
        return result
    
    resource_monitor.collect_metrics = enhanced_collect
```
</action>
  <verify>
1. Import check: `python -c "from src.monitoring.memory_profiler import MemoryProfiler, MemorySnapshot; print('Memory profiler import OK')"`
2. Snapshot creation:
   ```python
   from datetime import datetime
   from src.monitoring.memory_profiler import MemorySnapshot
   snap = MemorySnapshot(
       timestamp=datetime.now(),
       process_rss_mb=512.0,
       process_vms_mb=1024.0,
       python_objects_mb=256.0
   )
   print(f"Snapshot: {snap.process_rss_mb}MB RSS")
   ```
3. Growth analysis:
   ```python
   from datetime import datetime, timedelta
   from src.monitoring.memory_profiler import MemoryProfiler, MemorySnapshot
   
   profiler = MemoryProfiler()
   snap1 = MemorySnapshot(datetime.now(), 1000.0, 2000.0, 500.0)
   snap2 = MemorySnapshot(datetime.now() + timedelta(hours=1), 1100.0, 2100.0, 550.0)
   
   analysis = profiler.analyze_growth(snap1, snap2)
   print(f"Growth: {analysis['growth_percent']:.1f}%")
   assert analysis['rss_growth_mb'] == 100.0
   ```
4. Object tracking:
   ```python
   import gc
   from src.monitoring.memory_profiler import TrackedObject
   
   obj = {"data": "test" * 1000}
   import weakref
   tracked = TrackedObject("test_obj", weakref.ref(obj), 0.01, datetime.now())
   print(f"Tracked object alive: {tracked.is_alive}")
   assert tracked.is_alive
   ```
  </verify>
  <done>MemoryProfiler created with snapshot tracking, object monitoring, and growth analysis</done>
</task>

<task type="auto">
  <name>Create leak detector with automated detection algorithms</name>
  <files>src/monitoring/memory_profiler.py (LeakDetector class)</files>
  <action>Add LeakDetector class to `src/monitoring/memory_profiler.py`:

```python
class LeakDetector:
    """
    Automated memory leak detection using statistical analysis.
    
    Detects leaks by:
    1. Trend analysis (consistent growth over time)
    2. Object accumulation (growing counts of specific types)
    3. Reference cycles (objects not being GC'd)
    """
    
    def __init__(
        self,
        min_growth_threshold_mb: float = 50.0,
        min_growth_percent: float = 5.0,
        observation_periods: int = 6,  # Number of periods to observe
        period_minutes: float = 60.0,  # Duration of each period
    ):
        self.min_growth_threshold_mb = min_growth_threshold_mb
        self.min_growth_percent = min_growth_percent
        self.observation_periods = observation_periods
        self.period_minutes = period_minutes
        
        self._observations: List[MemorySnapshot] = []
        self._suspected_leaks: List[Dict] = []
        self._baseline_established = False
        
        logger.info(
            "LeakDetector initialized",
            min_growth_mb=min_growth_threshold_mb,
            min_growth_percent=min_growth_percent,
            observation_periods=observation_periods,
        )
    
    def add_observation(self, snapshot: MemorySnapshot):
        """Add a memory observation for analysis."""
        self._observations.append(snapshot)
        
        # Keep only necessary history
        max_observations = self.observation_periods + 2
        if len(self._observations) > max_observations:
            self._observations = self._observations[-max_observations:]
        
        # Check for leaks if we have enough data
        if len(self._observations) >= self.observation_periods:
            self._analyze_for_leaks()
    
    def _analyze_for_leaks(self):
        """Analyze observations for leak patterns."""
        if len(self._observations) < self.observation_periods:
            return
        
        # Get recent observations
        recent = self._observations[-self.observation_periods:]
        
        # Calculate trend using linear regression
        x = list(range(len(recent)))
        y = [obs.process_rss_mb for obs in recent]
        
        slope = self._calculate_slope(x, y)
        
        # Calculate total growth
        total_growth_mb = recent[-1].process_rss_mb - recent[0].process_rss_mb
        growth_percent = (total_growth_mb / recent[0].process_rss_mb) * 100
        
        # Check for leak conditions
        is_leak = (
            slope > 0 and  # Consistent growth
            total_growth_mb > self.min_growth_threshold_mb and
            growth_percent > self.min_growth_percent
        )
        
        if is_leak:
            leak_report = {
                "detected_at": datetime.now().isoformat(),
                "growth_mb": total_growth_mb,
                "growth_percent": growth_percent,
                "slope_mb_per_period": slope,
                "observation_count": len(recent),
                "start_memory_mb": recent[0].process_rss_mb,
                "current_memory_mb": recent[-1].process_rss_mb,
            }
            
            # Avoid duplicate reports
            if not self._suspected_leaks or \
               self._suspected_leaks[-1]["growth_mb"] != total_growth_mb:
                self._suspected_leaks.append(leak_report)
                logger.warning("Potential memory leak detected", **leak_report)
    
    def _calculate_slope(self, x: List[float], y: List[float]) -> float:
        """Calculate slope using simple linear regression."""
        n = len(x)
        if n < 2:
            return 0.0
        
        sum_x = sum(x)
        sum_y = sum(y)
        sum_xy = sum(xi * yi for xi, yi in zip(x, y))
        sum_x2 = sum(xi ** 2 for xi in x)
        
        denominator = n * sum_x2 - sum_x ** 2
        if denominator == 0:
            return 0.0
        
        slope = (n * sum_xy - sum_x * sum_y) / denominator
        return slope
    
    def get_suspected_leaks(self) -> List[Dict]:
        """Get list of suspected memory leaks."""
        return self._suspected_leaks.copy()
    
    def is_leak_detected(self) -> bool:
        """Check if any leak has been detected."""
        return len(self._suspected_leaks) > 0
    
    def clear_suspected_leaks(self):
        """Clear suspected leaks list."""
        self._suspected_leaks.clear()
    
    def analyze_object_growth(
        self,
        baseline_counts: Dict[str, int],
        current_counts: Dict[str, int]
    ) -> List[Dict]:
        """Analyze growth in object counts by type."""
        growth_items = []
        
        for obj_type, current_count in current_counts.items():
            baseline_count = baseline_counts.get(obj_type, 0)
            growth = current_count - baseline_count
            growth_percent = (growth / baseline_count * 100) if baseline_count > 0 else float('inf')
            
            # Flag significant growth
            if growth > 100 or growth_percent > 50:
                growth_items.append({
                    "type": obj_type,
                    "baseline": baseline_count,
                    "current": current_count,
                    "growth": growth,
                    "growth_percent": growth_percent if growth_percent != float('inf') else None,
                })
        
        # Sort by absolute growth
        growth_items.sort(key=lambda x: x["growth"], reverse=True)
        return growth_items[:20]  # Top 20 growing types
    
    def detect_reference_cycles(self) -> List[Dict]:
        """Detect objects involved in reference cycles."""
        gc.collect()  # Clean up first
        
        # Get objects that can't be collected
        unreachable = gc.collect(2)  # Full collection
        
        # Get garbage (objects with cycles)
        cycles = []
        for item in gc.garbage if hasattr(gc, 'garbage') else []:
            cycles.append({
                "type": type(item).__name__,
                "repr": repr(item)[:100],  # Truncated
            })
        
        return cycles
    
    def generate_leak_report(self) -> Dict[str, Any]:
        """Generate comprehensive leak analysis report."""
        if not self._observations:
            return {"status": "insufficient_data", "message": "No observations yet"}
        
        recent = self._observations[-10:] if len(self._observations) >= 10 else self._observations
        
        # Calculate statistics
        memory_values = [obs.process_rss_mb for obs in recent]
        avg_memory = sum(memory_values) / len(memory_values)
        max_memory = max(memory_values)
        min_memory = min(memory_values)
        
        return {
            "status": "leak_detected" if self.is_leak_detected() else "no_leak",
            "observation_count": len(self._observations),
            "suspected_leaks_count": len(self._suspected_leaks),
            "recent_memory_stats": {
                "average_mb": avg_memory,
                "min_mb": min_memory,
                "max_mb": max_memory,
                "range_mb": max_memory - min_memory,
            },
            "suspected_leaks": self._suspected_leaks[-5:],  # Last 5
        }
```

**Global profiler instance:**
```python
# Global profiler instance
_memory_profiler_instance: Optional[MemoryProfiler] = None
_leak_detector_instance: Optional[LeakDetector] = None


def get_memory_profiler() -> MemoryProfiler:
    """Get global memory profiler instance."""
    global _memory_profiler_instance
    if _memory_profiler_instance is None:
        _memory_profiler_instance = MemoryProfiler()
    return _memory_profiler_instance


def get_leak_detector() -> LeakDetector:
    """Get global leak detector instance."""
    global _leak_detector_instance
    if _leak_detector_instance is None:
        _leak_detector_instance = LeakDetector()
    return _leak_detector_instance
```
</action>
  <verify>
1. Leak detector import: `python -c "from src.monitoring.memory_profiler import LeakDetector; print('LeakDetector import OK')"`
2. Slope calculation:
   ```python
   from src.monitoring.memory_profiler import LeakDetector
   ld = LeakDetector()
   slope = ld._calculate_slope([0, 1, 2], [100, 110, 120])
   print(f"Slope: {slope}")  # Should be 10
   assert abs(slope - 10) < 0.01
   ```
3. Object growth analysis:
   ```python
   from src.monitoring.memory_profiler import LeakDetector
   ld = LeakDetector()
   baseline = {"str": 1000, "dict": 500}
   current = {"str": 1200, "dict": 800}
   growth = ld.analyze_object_growth(baseline, current)
   print(f"Growing types: {len(growth)}")
   assert len(growth) == 2
   ```
4. Leak report:
   ```python
   from src.monitoring.memory_profiler import get_leak_detector
   detector = get_leak_detector()
   report = detector.generate_leak_report()
   print(f"Report status: {report['status']}")
   ```
  </verify>
  <done>LeakDetector added with trend analysis, object growth tracking, and cycle detection</done>
</task>

<task type="auto">
  <name>Create monitoring module initialization</name>
  <files>src/monitoring/__init__.py</files>
  <action>Create `src/monitoring/__init__.py`:

```python
"""Monitoring module for Demi.

Provides memory profiling, leak detection, and resource monitoring
integrations for long-running stability testing.
"""

from src.monitoring.memory_profiler import (
    MemoryProfiler,
    MemorySnapshot,
    LeakDetector,
    TrackedObject,
    get_memory_profiler,
    get_leak_detector,
)

__all__ = [
    # Main classes
    "MemoryProfiler",
    "MemorySnapshot",
    "LeakDetector",
    "TrackedObject",
    # Global instances
    "get_memory_profiler",
    "get_leak_detector",
]

__version__ = "1.0.0"
```
</action>
  <verify>
1. Module import: `python -c "from src.monitoring import MemoryProfiler, LeakDetector, get_memory_profiler; print('Monitoring module import OK')"`
2. Exports check: `python -c "from src.monitoring import __all__; print(f'Exports: {len(__all__)} items')"`
3. Global instances: `python -c "from src.monitoring import get_memory_profiler, get_leak_detector; mp = get_memory_profiler(); ld = get_leak_detector(); print(f'Profiler: {type(mp).__name__}, Detector: {type(ld).__name__}')"`
  </verify>
  <done>Monitoring module initialization with all exports</done>
</task>

<task type="auto">
  <name>Create memory leak detection tests</name>
  <files>tests/stability/memory_leak_test.py</files>
  <action>Create `tests/stability/memory_leak_test.py`:

**Imports:**
- `import pytest`
- `import asyncio`
- `import gc`
- `import time`
- `from typing import List, Dict`
- `from src.monitoring import get_memory_profiler, get_leak_detector`
- `from src.monitoring.memory_profiler import MemoryProfiler, LeakDetector`
- `from tests.integration.harness import test_environment`
- `from tests.stability.load_generator import LoadGenerator, PATTERNS`

**Tests:**
```python
class TestMemoryLeakConversations:
    """Test for memory leaks during conversation handling."""
    
    @pytest.mark.asyncio
    @pytest.mark.timeout(300)  # 5 minutes
    async def test_conversation_memory_stability(self):
        """
        Test that memory doesn't grow significantly during extended conversation.
        Run ~100 conversations and verify <5% memory growth.
        """
        profiler = get_memory_profiler()
        detector = get_leak_detector()
        
        # Force GC and take baseline
        gc.collect()
        baseline = await profiler.take_snapshot()
        
        async with test_environment() as env:
            # Simulate many conversations
            discord = env.mock_services.get("discord")
            channel = discord.register_channel(999)
            
            messages = [
                "Hello Demi",
                "How are you?",
                "Tell me a story",
                "What do you think?",
                "Thanks for chatting",
            ]
            
            for i in range(100):
                from tests.integration.mocks.discord import MockUser
                user = MockUser(1000 + i, f"User{i}")
                
                for msg in messages:
                    discord.simulate_mention(msg, channel_id=999, author=user)
                    await discord.wait_for_response(999, timeout=2.0)
                    await asyncio.sleep(0.1)  # Brief pause
                
                # Take observation every 10 users
                if i % 10 == 0:
                    snapshot = await profiler.take_snapshot()
                    detector.add_observation(snapshot)
        
        # Final measurement
        gc.collect()
        final = await profiler.take_snapshot()
        
        # Calculate growth
        growth_mb = final.process_rss_mb - baseline.process_rss_mb
        growth_percent = (growth_mb / baseline.process_rss_mb) * 100
        
        print(f"Memory growth: {growth_mb:.1f}MB ({growth_percent:.1f}%)")
        print(f"Baseline: {baseline.process_rss_mb:.1f}MB, Final: {final.process_rss_mb:.1f}MB")
        
        # Assert <10% growth (allowing some variance)
        assert growth_percent < 10, f"Memory grew by {growth_percent:.1f}% - possible leak"
        assert not detector.is_leak_detected(), "Leak detector flagged potential leak"
    
    @pytest.mark.asyncio
    @pytest.mark.timeout(180)
    async def test_emotion_system_memory(self):
        """Test that emotion system doesn't leak memory over time."""
        profiler = get_memory_profiler()
        
        gc.collect()
        baseline = await profiler.take_snapshot()
        
        async with test_environment() as env:
            persistence = env.conductor.emotion_persistence
            
            # Save many emotion states
            from src.emotion.models import EmotionalState
            for i in range(500):
                state = EmotionalState(
                    loneliness=0.3 + (i % 10) / 20,
                    excitement=0.4 + (i % 8) / 20,
                )
                persistence.save_state(state, notes=f"Test state {i}")
            
            # Query and load states
            for _ in range(100):
                persistence.load_latest_state()
        
        gc.collect()
        final = await profiler.take_snapshot()
        
        growth_percent = ((final.process_rss_mb - baseline.process_rss_mb) / baseline.process_rss_mb) * 100
        
        print(f"Emotion system memory growth: {growth_percent:.1f}%")
        assert growth_percent < 15, f"Emotion system memory grew by {growth_percent:.1f}%"


class TestMemoryLeakRambles:
    """Test for memory leaks during ramble generation."""
    
    @pytest.mark.asyncio
    @pytest.mark.timeout(120)
    async def test_ramble_generation_memory(self):
        """Test ramble generation doesn't accumulate memory."""
        profiler = get_memory_profiler()
        
        gc.collect()
        baseline = await profiler.take_snapshot()
        
        async with test_environment() as env:
            # Trigger many rambles
            from tests.integration.fixtures.emotion_fixtures import LONELY_STATE
            
            persistence = env.conductor.emotion_persistence
            
            for i in range(50):
                state = LONELY_STATE
                state.loneliness = 0.9
                persistence.save_state(state)
                
                # Wait for potential ramble processing
                await asyncio.sleep(0.5)
        
        gc.collect()
        final = await profiler.take_snapshot()
        
        growth_percent = ((final.process_rss_mb - baseline.process_rss_mb) / baseline.process_rss_mb) * 100
        
        print(f"Ramble memory growth: {growth_percent:.1f}%")
        assert growth_percent < 10


class TestLoadGeneratorMemory:
    """Test memory behavior under load generation."""
    
    @pytest.mark.asyncio
    @pytest.mark.timeout(300)
    async def test_sustained_load_memory(self):
        """
        Test memory under sustained load for 3 minutes.
        Simulates the load patterns used in 7-day tests.
        """
        profiler = get_memory_profiler()
        detector = get_leak_detector()
        
        gc.collect()
        baseline = await profiler.take_snapshot()
        detector.add_observation(baseline)
        
        async with test_environment() as env:
            discord = env.mock_services.get("discord")
            channel = discord.register_channel(999)
            
            # Create load generator with stress pattern
            from tests.stability.load_generator import LoadGenerator, PATTERNS
            generator = LoadGenerator(PATTERNS["stress_test"])
            
            messages_processed = 0
            
            async def handle_message(msg: str):
                nonlocal messages_processed
                from tests.integration.mocks.discord import MockUser
                user = MockUser(999, "LoadTestUser")
                discord.simulate_message(msg, channel_id=999, author=user)
                messages_processed += 1
            
            # Run for 3 minutes
            start_time = time.time()
            task = asyncio.create_task(generator.generate_load(handle_message, duration_hours=0.05))
            
            # Take periodic snapshots
            while time.time() - start_time < 180:
                await asyncio.sleep(30)
                snapshot = await profiler.take_snapshot()
                detector.add_observation(snapshot)
                print(f"[{int(time.time() - start_time)}s] Memory: {snapshot.process_rss_mb:.0f}MB, "
                      f"Messages: {messages_processed}")
            
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass
        
        gc.collect()
        final = await profiler.take_snapshot()
        
        # Analyze growth
        growth_percent = ((final.process_rss_mb - baseline.process_rss_mb) / baseline.process_rss_mb) * 100
        
        print(f"\nFinal memory growth: {growth_percent:.1f}%")
        print(f"Leak detected: {detector.is_leak_detected()}")
        
        # Under sustained load, some growth is acceptable, but not a leak pattern
        assert growth_percent < 20, f"Excessive memory growth: {growth_percent:.1f}%"


class TestLeakDetectorAccuracy:
    """Test leak detector's ability to detect actual leaks."""
    
    def test_leak_detector_identifies_growth_pattern(self):
        """Test that leak detector flags consistent growth."""
        from datetime import datetime, timedelta
        from src.monitoring.memory_profiler import LeakDetector, MemorySnapshot
        
        detector = LeakDetector(min_growth_threshold_mb=10, min_growth_percent=2)
        
        # Create snapshots with clear growth pattern
        base_time = datetime.now()
        base_memory = 1000.0
        
        for i in range(10):
            snapshot = MemorySnapshot(
                timestamp=base_time + timedelta(minutes=i*10),
                process_rss_mb=base_memory + (i * 20),  # Growing 20MB per period
                process_vms_mb=base_memory * 2,
                python_objects_mb=base_memory / 2,
            )
            detector.add_observation(snapshot)
        
        assert detector.is_leak_detected(), "Leak detector should flag this growth pattern"
        
        leaks = detector.get_suspected_leaks()
        assert len(leaks) > 0
        assert leaks[0]["growth_mb"] > 100  # Should detect ~180MB growth
    
    def test_leak_detector_ignores_stable_memory(self):
        """Test that leak detector doesn't flag stable memory."""
        from datetime import datetime, timedelta
        from src.monitoring.memory_profiler import LeakDetector, MemorySnapshot
        
        detector = LeakDetector(min_growth_threshold_mb=50, min_growth_percent=5)
        
        base_time = datetime.now()
        
        # Create snapshots with stable memory (small fluctuations)
        for i in range(10):
            snapshot = MemorySnapshot(
                timestamp=base_time + timedelta(minutes=i*10),
                process_rss_mb=1000.0 + (i % 3) * 5,  # Small fluctuations
                process_vms_mb=2000.0,
                python_objects_mb=500.0,
            )
            detector.add_observation(snapshot)
        
        assert not detector.is_leak_detected(), "Leak detector should not flag stable memory"
```
</action>
  <verify>
1. Import check: `python -c "from tests.stability.memory_leak_test import TestMemoryLeakConversations; print('Memory leak tests import OK')"`
2. Test collection: `python -m pytest tests/stability/memory_leak_test.py --collect-only 2>/dev/null | head -20`
3. Leak detector test:
   ```python
   from tests.stability.memory_leak_test import TestLeakDetectorAccuracy
   t = TestLeakDetectorAccuracy()
   t.test_leak_detector_identifies_growth_pattern()
   print("Leak detector accuracy test passed")
   ```
4. Test count verification
  </verify>
  <done>Memory leak tests created for conversations, emotions, rambles, and load generation</done>
</task>

<task type="auto">
  <name>Create resource cleanup verification tests</name>
  <files>tests/stability/resource_cleanup_test.py</files>
  <action>Create `tests/stability/resource_cleanup_test.py`:

**Imports:**
- `import pytest`
- `import asyncio`
- `import gc`
- `import weakref`
- `from typing import Optional`
- `from src.monitoring import get_memory_profiler`

**Tests:**
```python
class TestDiscordResourceCleanup:
    """Test Discord client resources are properly cleaned up."""
    
    @pytest.mark.asyncio
    async def test_discord_client_cleanup(self):
        """Verify Discord client doesn't leave dangling references."""
        from tests.integration.mocks.discord import MockDiscordClient, MockUser
        
        profiler = get_memory_profiler()
        
        # Track client with weak reference
        client: Optional[MockDiscordClient] = MockDiscordClient()
        weak = weakref.ref(client)
        
        # Use client
        channel = client.register_channel(999)
        user = MockUser(123, "Test")
        client.simulate_message("test", channel_id=999, author=user)
        
        # Clear reference
        client_id = id(client)
        del client
        del channel
        del user
        
        # Force GC
        gc.collect()
        
        # Verify cleanup
        assert weak() is None, "MockDiscordClient was not garbage collected"
    
    @pytest.mark.asyncio
    async def test_discord_message_cleanup(self):
        """Verify messages are cleaned up after processing."""
        from tests.integration.mocks.discord import MockDiscordClient, MockMessage
        
        gc.collect()
        initial_objects = len(gc.get_objects())
        
        client = MockDiscordClient()
        
        # Generate many messages
        for i in range(100):
            msg = MockMessage(f"Message {i}")
            # Process and discard
            del msg
        
        del client
        gc.collect()
        
        final_objects = len(gc.get_objects())
        growth = final_objects - initial_objects
        
        # Allow some growth for test infrastructure
        print(f"Object growth: {growth}")
        assert growth < 500, f"Too many objects retained: {growth}"


class TestLLMResourceCleanup:
    """Test LLM inference resources are properly cleaned up."""
    
    @pytest.mark.asyncio
    async def test_ollama_mock_cleanup(self):
        """Verify mock Ollama server cleans up properly."""
        from tests.integration.mocks.ollama import MockOllamaServer
        
        gc.collect()
        
        server = MockOllamaServer()
        
        # Register responses
        for i in range(50):
            server.register_response([f"trigger{i}"], f"response{i}")
        
        # Generate some responses
        for i in range(20):
            await server.generate(f"trigger{i % 10}")
        
        weak = weakref.ref(server)
        del server
        gc.collect()
        
        assert weak() is None, "MockOllamaServer was not garbage collected"
    
    @pytest.mark.asyncio
    async def test_conversation_history_cleanup(self):
        """Verify conversation history doesn't grow unbounded."""
        async with test_environment() as env:
            history = env.conductor.conversation_history
            
            # Add many messages
            for i in range(200):
                history.add_message("user", f"Message {i}")
                history.add_message("assistant", f"Response {i}")
            
            # Get conversation
            messages = history.get_recent_messages(limit=1000)
            
            # Should have reasonable limit
            assert len(messages) <= 1000, "Conversation history exceeded limit"


class TestEmotionResourceCleanup:
    """Test emotion system resource cleanup."""
    
    @pytest.mark.asyncio
    async def test_emotion_state_cleanup(self):
        """Verify emotion states are properly cleaned up."""
        from src.emotion.models import EmotionalState
        
        gc.collect()
        
        states = []
        weak_refs = []
        
        # Create many states
        for i in range(100):
            state = EmotionalState(loneliness=i/100)
            states.append(state)
            weak_refs.append(weakref.ref(state))
        
        # Clear all references
        del states
        gc.collect()
        
        # Check cleanup
        remaining = sum(1 for ref in weak_refs if ref() is not None)
        print(f"Remaining emotion states: {remaining}")
        
        # Most should be cleaned up
        assert remaining < 10, f"Too many emotion states retained: {remaining}"
    
    @pytest.mark.asyncio
    async def test_persistence_connection_cleanup(self):
        """Verify database connections are properly closed."""
        import tempfile
        from pathlib import Path
        from src.emotion.persistence import EmotionPersistence
        
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = Path(tmpdir) / "test.db"
            
            # Create and use persistence
            persistence = EmotionPersistence(db_path=str(db_path))
            
            from src.emotion.models import EmotionalState
            for i in range(50):
                state = EmotionalState()
                persistence.save_state(state)
            
            # Delete persistence
            del persistence
            gc.collect()
            
            # File should be accessible (connection closed)
            assert db_path.exists(), "Database file should exist"
            
            # Should be able to reopen
            persistence2 = EmotionPersistence(db_path=str(db_path))
            state = persistence2.load_latest_state()
            assert state is not None, "Should be able to read after reopen"


class TestConductorResourceCleanup:
    """Test Conductor resource cleanup."""
    
    @pytest.mark.asyncio
    async def test_conductor_cleanup_after_shutdown(self):
        """Verify Conductor cleans up resources on shutdown."""
        from tests.integration.harness import IntegrationTestHarness
        
        gc.collect()
        initial_objects = len(gc.get_objects())
        
        harness = IntegrationTestHarness()
        env = await harness.setup()
        
        # Verify conductor is running
        assert env.conductor is not None
        
        # Shutdown
        await harness.teardown(env)
        
        del harness
        del env
        gc.collect()
        
        final_objects = len(gc.get_objects())
        growth = final_objects - initial_objects
        
        print(f"Object growth after conductor cycle: {growth}")
        # Allow some growth for module-level instances
        assert growth < 1000, f"Too many objects retained after conductor shutdown: {growth}"


class TestTrackedObjectLifecycle:
    """Test memory profiler's tracked object lifecycle."""
    
    def test_tracked_object_weak_reference(self):
        """Verify tracked objects use weak references."""
        from src.monitoring.memory_profiler import TrackedObject
        from datetime import datetime
        
        obj = {"data": "test" * 1000}
        ref = weakref.ref(obj)
        
        tracked = TrackedObject("test", ref, 0.01, datetime.now())
        
        assert tracked.is_alive
        
        # Delete original
        del obj
        gc.collect()
        
        assert not tracked.is_alive, "Tracked object should detect deletion"
    
    def test_object_tracking_doesnt_prevent_gc(self):
        """Verify tracking doesn't prevent garbage collection."""
        from src.monitoring import get_memory_profiler
        
        profiler = get_memory_profiler()
        
        gc.collect()
        
        # Create and track object
        obj = {"large_data": "x" * 1000000}
        profiler.track_object(obj, "test_large")
        
        # Verify we can still GC it
        del obj
        gc.collect()
        
        # Should not affect future operations
        snapshot = asyncio.run(profiler.take_snapshot())
        assert snapshot is not None
```
</action>
  <verify>
1. Import check: `python -c "from tests.stability.resource_cleanup_test import TestDiscordResourceCleanup; print('Cleanup tests import OK')"`
2. Weak reference test:
   ```python
   import gc
   import weakref
   from tests.stability.resource_cleanup_test import TestTrackedObjectLifecycle
   
   t = TestTrackedObjectLifecycle()
   t.test_tracked_object_weak_reference()
   print("Weak reference test passed")
   ```
3. Test collection: `python -m pytest tests/stability/resource_cleanup_test.py --collect-only 2>/dev/null | head -20`
4. Verify test coverage of all major components
  </verify>
  <done>Resource cleanup tests created for Discord, LLM, emotion, and conductor components</done>
</task>

<task type="auto">
  <name>Create memory profiling shell script</name>
  <files>scripts/profile_memory.sh</files>
  <action>Create `scripts/profile_memory.sh`:

```bash
#!/bin/bash
#
# Memory profiling script for Demi
# Runs memory profiling and generates reports
#

set -e

# Configuration
DURATION_MINUTES=${1:-60}
SAMPLE_INTERVAL=${2:-30}
OUTPUT_DIR="${HOME}/.demi/memory_profiles"
PIDFILE="${OUTPUT_DIR}/memory_profile.pid"

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

setup() {
    echo -e "${GREEN}Setting up memory profiling...${NC}"
    mkdir -p "$OUTPUT_DIR"
    
    # Check for required tools
    if ! command -v python3 &> /dev/null; then
        echo -e "${RED}Error: python3 not found${NC}"
        exit 1
    fi
    
    echo "Output directory: $OUTPUT_DIR"
    echo "Duration: $DURATION_MINUTES minutes"
    echo "Sample interval: $SAMPLE_INTERVAL seconds"
}

run_profiler() {
    echo -e "${GREEN}Starting memory profiler...${NC}"
    
    python3 << EOF
import asyncio
import sys
import time
import json
from datetime import datetime
from pathlib import Path

sys.path.insert(0, '/home/mystiatech/projects/Demi')

from src.monitoring import get_memory_profiler, get_leak_detector
from src.monitoring.memory_profiler import MemoryProfiler

async def profile():
    profiler = get_memory_profiler()
    detector = get_leak_detector()
    
    await profiler.start_profiling(interval_seconds=$SAMPLE_INTERVAL)
    
    output_dir = Path("$OUTPUT_DIR")
    duration = $DURATION_MINUTES * 60
    start_time = time.time()
    
    print(f"Profiling started at {datetime.now()}")
    print(f"Duration: $DURATION_MINUTES minutes")
    print(f"Press Ctrl+C to stop early")
    
    snapshots = []
    
    try:
        while time.time() - start_time < duration:
            await asyncio.sleep($SAMPLE_INTERVAL)
            
            snapshot = await profiler.take_snapshot()
            detector.add_observation(snapshot)
            snapshots.append(snapshot.to_dict())
            
            # Log progress
            elapsed = time.time() - start_time
            progress = (elapsed / duration) * 100
            print(f"[{progress:.1f}%] Memory: {snapshot.process_rss_mb:.0f}MB, "
                  f"Objects: {snapshot.gc_objects_count}")
            
            # Check for leaks
            if detector.is_leak_detected():
                leaks = detector.get_suspected_leaks()
                print(f"WARNING: Potential leak detected! {len(leaks)} suspected leaks")
    
    except KeyboardInterrupt:
        print("\nProfiling interrupted by user")
    
    finally:
        await profiler.stop_profiling()
        
        # Save report
        report = {
            "start_time": datetime.fromtimestamp(start_time).isoformat(),
            "end_time": datetime.now().isoformat(),
            "duration_minutes": (time.time() - start_time) / 60,
            "sample_count": len(snapshots),
            "snapshots": snapshots,
            "leak_detected": detector.is_leak_detected(),
            "suspected_leaks": detector.get_suspected_leaks(),
        }
        
        report_file = output_dir / f"memory_profile_{int(start_time)}.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"\nReport saved to: {report_file}")
        
        # Generate summary
        if snapshots:
            initial = snapshots[0]["process_rss_mb"]
            final = snapshots[-1]["process_rss_mb"]
            growth = final - initial
            growth_pct = (growth / initial) * 100 if initial > 0 else 0
            
            print(f"\n=== Memory Profile Summary ===")
            print(f"Initial memory: {initial:.1f}MB")
            print(f"Final memory: {final:.1f}MB")
            print(f"Growth: {growth:+.1f}MB ({growth_pct:+.1f}%)")
            print(f"Samples collected: {len(snapshots)}")
            
            if detector.is_leak_detected():
                print(f"\n⚠️  LEAK DETECTED")
                for leak in detector.get_suspected_leaks():
                    print(f"  - Growth: {leak['growth_mb']:.1f}MB ({leak['growth_percent']:.1f}%)")

asyncio.run(profile())
EOF
}

generate_report() {
    echo -e "${GREEN}Generating memory report...${NC}"
    
    latest_report=$(ls -t "${OUTPUT_DIR}"/memory_profile_*.json 2>/dev/null | head -1)
    
    if [ -z "$latest_report" ]; then
        echo -e "${YELLOW}No memory profile reports found${NC}"
        return
    fi
    
    echo "Latest report: $latest_report"
    
    # Display summary
    python3 << EOF
import json
from pathlib import Path

report_file = Path("$latest_report")
with open(report_file) as f:
    report = json.load(f)

print("\n=== Memory Profile Report ===")
print(f"Duration: {report['duration_minutes']:.1f} minutes")
print(f"Samples: {report['sample_count']}")
print(f"Leak detected: {report['leak_detected']}")

if report['snapshots']:
    snapshots = report['snapshots']
    initial = snapshots[0]['process_rss_mb']
    final = snapshots[-1]['process_rss_mb']
    max_mem = max(s['process_rss_mb'] for s in snapshots)
    
    print(f"\nMemory Usage:")
    print(f"  Initial: {initial:.1f}MB")
    print(f"  Final: {final:.1f}MB")
    print(f"  Peak: {max_mem:.1f}MB")
    print(f"  Growth: {final - initial:+.1f}MB ({((final - initial) / initial) * 100:+.1f}%)")

if report['suspected_leaks']:
    print(f"\nSuspected Leaks:")
    for leak in report['suspected_leaks']:
        print(f"  - {leak['growth_mb']:.1f}MB ({leak['growth_percent']:.1f}%) at {leak['detected_at']}")
EOF
}

# Command dispatcher
case "${3:-run}" in
    run)
        setup
        run_profiler
        ;;
    report)
        generate_report
        ;;
    *)
        echo "Usage: $0 [DURATION_MINUTES] [SAMPLE_INTERVAL_SECONDS] [run|report]"
        echo ""
        echo "Examples:"
        echo "  $0                    # Profile for 60 minutes, 30s intervals"
        echo "  $0 120 60             # Profile for 2 hours, 60s intervals"
        echo "  $0 report             # Generate report from latest profile"
        exit 1
        ;;
esac
```

Make executable:
```bash
chmod +x scripts/profile_memory.sh
```
</action>
  <verify>
1. Script syntax: `bash -n /home/mystiatech/projects/Demi/scripts/profile_memory.sh && echo "Script syntax OK"`
2. Help output: `bash scripts/profile_memory.sh 2>&1 | head -15`
3. Report generation: Verify Python heredoc syntax
4. Directory setup: Check OUTPUT_DIR path
  </verify>
  <done>Memory profiling shell script created for running profiles and generating reports</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Module imports:** All monitoring modules import without errors
2. **Profiler test:** Run short profile (1 minute) to verify functionality
3. **Leak detection:** Verify leak detector flags growth patterns correctly
4. **Cleanup tests:** Run resource cleanup tests
5. **Integration:** Verify integration with ResourceMonitor

**Memory profiling targets:**
- Memory tracking accurate to within 5%
- Leak detector identifies >50MB growth over 6 hours
- Object tracking doesn't prevent GC
- Resource cleanup tests pass
- <10% memory growth in 3-minute stress test
</verification>

<success_criteria>
- MemoryProfiler tracks process and Python memory accurately
- LeakDetector identifies memory growth patterns with statistical confidence
- Object tracking uses weak references (doesn't prevent GC)
- Integration with ResourceMonitor provides unified metrics
- Memory leak tests verify <5% growth over extended periods
- Resource cleanup tests verify no dangling references
- Memory stays below 10GB sustained during 7-day test (HEALTH-02)
- <5% memory growth over 7-day period
</success_criteria>

<output>
After completion, create `.planning/phases/09-integration-testing/09-03-SUMMARY.md`
</output>
