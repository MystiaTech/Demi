---
phase: 08-voice-io
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: ["requirements.txt", "src/core/defaults.yaml", "src/voice/__init__.py", "src/voice/stt.py", "src/voice/tts.py", "tests/test_voice_stt.py", "tests/test_voice_tts.py"]
autonomous: true

must_haves:
  truths:
    - "Voice STT system transcribes audio input to text"
    - "Voice TTS system converts text responses to speech"
    - "Voice configuration integrates with existing Demi config system"
    - "Voice components handle errors gracefully without crashing system"
    - "Voice system can be enabled/disabled via configuration"
  artifacts:
    - path: "src/voice/stt.py"
      provides: "Whisper-based speech-to-text conversion"
      min_lines: 50
      contains: "class WhisperSTT"
    - path: "src/voice/tts.py"
      provides: "pyttsx3-based text-to-speech synthesis"
      min_lines: 40
      contains: "class pyttsx3TTS"
    - path: "src/voice/__init__.py"
      provides: "Voice module exports and integration"
      min_lines: 20
    - path: "requirements.txt"
      provides: "Voice library dependencies"
      contains: "openai-whisper"
      contains: "pyttsx3"
    - path: "tests/test_voice_stt.py"
      provides: "STT component test coverage"
      min_lines: 30
    - path: "tests/test_voice_tts.py"
      provides: "TTS component test coverage"
      min_lines: 30
  key_links:
    - from: "src/voice/stt.py"
      to: "src/core/config.py"
      via: "Configuration loading for Whisper model settings"
      pattern: "DemiConfig.*voice.*stt"
    - from: "src/voice/tts.py"
      to: "src/core/config.py"
      via: "Configuration loading for TTS engine settings"
      pattern: "DemiConfig.*voice.*tts"
    - from: "src/voice/__init__.py"
      to: "src/llm/inference.py"
      via: "Voice input text routing to LLM pipeline"
      pattern: "transcribe.*text.*inference"
---

<objective>
Create foundational voice I/O system with Whisper STT and pyttsx3 TTS integration.

Purpose: Establish the core voice processing capabilities that enable Demi to understand spoken input and generate spoken responses. This provides the foundation for voice channel communication and future voice features.

Output: Working voice module with STT/TTS capabilities, configuration integration, and comprehensive test coverage.
</objective>

<execution_context>
@~/.opencode/get-shit-done/workflows/execute-plan.md
@~/.opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/core/defaults.yaml
@requirements.txt
@src/llm/inference.py
@src/core/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add voice dependencies and create voice module structure</name>
  <files>requirements.txt, src/voice/__init__.py</files>
  <action>
1. Update requirements.txt to add voice libraries:
   - openai-whisper>=20231017
   - pyttsx3>=2.90
   - sounddevice>=0.4.6 (for audio input)
   - numpy>=1.24.0 (for audio processing)

2. Create src/voice/__init__.py with:
   - Module exports (WhisperSTT, pyttsx3TTS)
   - VoiceConfig dataclass for configuration validation
   - Factory functions for STT/TTS initialization
   - Error handling and graceful fallbacks when voice disabled

3. Ensure voice module follows existing patterns:
   - Use DemiLogger for structured logging
   - Integrate with existing configuration system
   - Handle missing dependencies gracefully
   - Provide clear error messages for troubleshooting
  </action>
  <verify>python -c "import src.voice; print('Voice module imports successfully')" && pip check</verify>
  <done>Voice module structure created with all dependencies properly imported and configured</done>
</task>

<task type="auto">
  <name>Task 2: Implement Whisper-based Speech-to-Text (STT) system</name>
  <files>src/voice/stt.py, tests/test_voice_stt.py</files>
  <action>
1. Create WhisperSTT class in src/voice/stt.py with:
   - Async transcribe_audio() method accepting audio file path or audio data
   - Model loading with configurable model size (tiny, base, small, medium)
   - Language support (English default, configurable)
   - Graceful handling of missing Whisper (fallback to text input)
   - Resource usage monitoring (CPU/memory during transcription)
   - Audio format validation and conversion support

2. Implement STT configuration integration:
   - Load Whisper model size from config (voice.stt.model_size)
   - Support for different audio formats (wav, mp3, flac)
   - Configurable timeout and retry logic
   - Error handling for audio corruption and model loading failures

3. Create comprehensive tests in tests/test_voice_stt.py:
   - Mock Whisper model for unit tests
   - Test transcribe_audio() with various audio formats
   - Test error handling for invalid audio
   - Test configuration loading and validation
   - Test graceful fallback when Whisper unavailable
  </action>
  <verify>pytest tests/test_voice_stt.py -v && python -c "from src.voice.stt import WhisperSTT; print('STT class imports successfully')"</verify>
  <done>Whisper STT implementation working with all tests passing and proper error handling</done>
</task>

<task type="auto">
  <name>Task 3: Implement pyttsx3-based Text-to-Speech (TTS) system</name>
  <files>src/voice/tts.py, tests/test_voice_tts.py</files>
  <action>
1. Create pyttsx3TTS class in src/voice/tts.py with:
   - Async speak_text() method for immediate audio output
   - save_to_file() method for audio file generation
   - Voice selection and configuration (rate, volume)
   - Emotional tone modulation based on emotional state
   - Non-blocking speech with stop() and pause() methods
   - Engine lifecycle management (init/cleanup)

2. Implement emotional tone modulation:
   - Map emotional state to speech parameters:
     * Lonely: slower rate (120), lower volume (0.7)
     * Excited: faster rate (180), higher volume (1.0)
     * Frustrated: very fast rate (200), harsher tone
     * Confident: normal rate (150), clear volume (0.9)
   - Integrate with existing EmotionalState dataclass
   - Provide smooth transitions between emotional states

3. Create comprehensive tests in tests/test_voice_tts.py:
   - Mock pyttsx3 engine for unit tests
   - Test speak_text() with various texts and emotional states
   - Test save_to_file() functionality
   - Test emotional tone modulation mapping
   - Test engine lifecycle and error handling
   - Test graceful fallback when pyttsx3 unavailable
  </action>
  <verify>pytest tests/test_voice_tts.py -v && python -c "from src.voice.tts import pyttsx3TTS; print('TTS class imports successfully')"</verify>
  <done>pyttsx3 TTS implementation working with emotional modulation and all tests passing</done>
</task>

</tasks>

<verification>
1. Voice module imports successfully without dependency errors
2. Whisper STT can transcribe audio files (test with sample audio)
3. pyttsx3 TTS can convert text to speech output
4. Configuration system properly loads voice settings
5. Error handling works when voice libraries are missing
6. All unit tests pass (>80% coverage for voice module)
7. Voice components follow existing code patterns and logging
</verification>

<success_criteria>
- Voice system foundation established with STT/TTS capabilities
- Configuration integrated with existing Demi config system
- Comprehensive test coverage ensuring reliability
- Graceful error handling for missing dependencies
- Emotional tone modulation framework in place for TTS
- Ready for integration with Discord voice channels and Android WebSocket
</success_criteria>

<output>
After completion, create `.planning/phases/08-voice-io/08-01-SUMMARY.md`
</output>