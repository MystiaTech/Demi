---
phase: 08-voice-io
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/voice/stt.py
  - src/voice/audio_capture.py
  - src/voice/vad.py
  - src/voice/__init__.py
autonomous: true
must_haves:
  truths:
    - "Whisper STT converts voice to text with <5s latency"
    - "Voice Activity Detection (VAD) prevents processing silence"
    - "Audio capture streams in chunks for real-time processing"
    - "STT pipeline integrates with Conductor message routing"
  artifacts:
    - path: "src/voice/stt.py"
      provides: "SpeechToText engine with Whisper backend"
      exports: ["SpeechToText", "TranscriptionResult"]
      min_lines: 150
    - path: "src/voice/audio_capture.py"
      provides: "Audio stream capture for microphone/Discord sources"
      exports: ["AudioCapture", "AudioStream"]
      min_lines: 100
    - path: "src/voice/vad.py"
      provides: "Voice Activity Detection using webrtcvad"
      exports: ["VoiceActivityDetector"]
      min_lines: 80
    - path: "src/voice/__init__.py"
      provides: "Voice module exports"
      exports: ["SpeechToText", "AudioCapture", "VoiceActivityDetector"]
  key_links:
    - from: "src/voice/stt.py"
      to: "src/conductor/orchestrator.py"
      via: "transcription text passed as user message"
      pattern: "conductor\.request_inference"
    - from: "src/voice/stt.py"
      to: "src/core/logger.py"
      via: "logging transcription events and latency"
      pattern: "logger\.(info|debug|error)"
---

<objective>
Build the Speech-to-Text (STT) pipeline that converts voice input to text using OpenAI Whisper or faster-whisper. This enables Demi to receive voice commands and conversations, transcribing them for processing by the LLM pipeline.

Purpose: Enable voice input capability for Discord voice channels and future Android voice integration, satisfying VOICE-01 requirement.

Output: SpeechToText engine with VAD (Voice Activity Detection), audio capture, and real-time transcription with <5s end-to-end latency.
</objective>

<execution_context>
@~/.opencode/get-shit-done/workflows/execute-plan.md
@.planning/REQUIREMENTS.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/PROJECT.md
@DEMI_PERSONA.md
@src/conductor/orchestrator.py
@src/core/logger.py
</context>

<tasks>

<task type="auto">
  <name>Create Voice Activity Detection (VAD) module</name>
  <files>src/voice/vad.py</files>
  <action>Create `src/voice/vad.py` with VoiceActivityDetector class:

**Imports:**
- `import webrtcvad`
- `import collections`
- `from typing import Optional, Callable`
- `import numpy as np`

**Class VoiceActivityDetector:**
- `__init__(self, aggressiveness: int = 3, frame_duration_ms: int = 30)`:
  - aggressiveness: 0-3 (0=least aggressive, 3=most aggressive filtering)
  - frame_duration_ms: must be 10, 20, or 30ms per webrtcvad requirements
  - Initialize VAD: `self.vad = webrtcvad.Vad(aggressiveness)`
  
- `is_speech(self, audio_frame: bytes, sample_rate: int) -> bool`:
  - Returns True if frame contains speech
  - Validates sample_rate (8000, 16000, 32000, 48000 Hz supported)
  - Validates frame length matches frame_duration_ms
  
- `create_streaming_buffer(self, padding_duration_ms: int = 300) -> SpeechBuffer`:
  - Returns SpeechBuffer class that manages ring buffer for streaming VAD
  - Padding adds context before/after speech detection
  
**Class SpeechBuffer:**
- Ring buffer using `collections.deque`
- `add_frame(frame: bytes, is_speech: bool)` - add audio frame with VAD result
- `get_speech_segments() -> list[bytes]` - returns completed speech segments
- `clear()` - reset buffer

**Helper functions:**
- `validate_audio_format(audio_data: bytes, sample_rate: int) -> bool`
- `normalize_audio(audio_data: bytes) -> bytes` - convert to 16-bit PCM if needed

**Implementation notes:**
- webrtcvad requires specific frame sizes based on sample_rate and frame_duration
- 16-bit PCM audio required (mono)
- Handle edge cases: silence, noise, very short utterances (<200ms)
- Log VAD decisions at DEBUG level for tuning
</action>
  <verify>
1. Import check: `python -c "from src.voice.vad import VoiceActivityDetector; print('VAD import OK')"`
2. VAD initialization: Create detector with aggressiveness=3, verify vad object created
3. Speech detection test:
   ```python
   vad = VoiceActivityDetector()
   # Create dummy 30ms of 16-bit PCM silence (should return False)
   silence = b'\x00' * 480  # 30ms at 16kHz = 480 samples * 2 bytes
   result = vad.is_speech(silence, 16000)
   print(f"Silence detected: {result}")  # Should be False
   ```
4. Buffer test: Add frames to SpeechBuffer, verify segments extracted correctly
  </verify>
  <done>VoiceActivityDetector created with webrtcvad integration, SpeechBuffer for streaming, and proper audio format validation</done>
</task>

<task type="auto">
  <name>Implement audio capture module for microphone and stream sources</name>
  <files>src/voice/audio_capture.py</files>
  <action>Create `src/voice/audio_capture.py` with AudioCapture and AudioStream classes:

**Imports:**
- `import pyaudio`
- `import asyncio`
- `import queue`
- `from typing import Optional, AsyncGenerator, Callable`
- `import numpy as np`

**Class AudioStream:**
- `__init__(self, sample_rate: int = 16000, chunk_duration_ms: int = 30)`:
  - sample_rate: target sample rate (16000 Hz recommended for Whisper)
  - chunk_duration_ms: frame size (30ms for VAD compatibility)
  - Calculate chunk_size = int(sample_rate * chunk_duration_ms / 1000)
  
- `async def read_chunk(self) -> Optional[bytes]`:
  - Returns audio chunk or None if stream ended
  - Non-blocking read with asyncio
  
- `async def iter_chunks(self) -> AsyncGenerator[bytes, None]`:
  - Yields audio chunks continuously until stopped

**Class AudioCapture:**
- `__init__(self, sample_rate: int = 16000, channels: int = 1)`:
  - Initialize PyAudio: `self.pa = pyaudio.PyAudio()`
  - Store configuration
  
- `async def start_microphone_stream(self) -> AudioStream`:
  - Open microphone stream with PyAudio
  - Format: `paInt16` (16-bit PCM)
  - Channels: 1 (mono)
  - Rate: sample_rate (16000 Hz)
  - Frames per buffer: chunk_size
  - Return AudioStream wrapper
  
- `async def start_discord_stream(self, discord_audio_source) -> AudioStream`:
  - Accept discord.py AudioSource or PCMAudio stream
  - Convert Discord's 48kHz stereo to 16kHz mono
  - Return AudioStream with standardized format
  
- `def stop(self)`:
  - Close all streams
  - Terminate PyAudio
  
- `list_devices() -> list[dict]`:
  - Return available audio input devices
  - Include device index, name, sample rates

**Audio conversion utilities:**
- `resample_audio(audio_data: bytes, src_rate: int, dst_rate: int) -> bytes`
- `stereo_to_mono(audio_data: bytes) -> bytes`
- `normalize_volume(audio_data: bytes, target_db: float = -20) -> bytes`

**Implementation notes:**
- PyAudio runs in separate thread, use asyncio.Queue for thread-safe audio passing
- Discord voice delivers 48kHz stereo Opus, convert to 16kHz mono PCM
- Handle buffer overflow/underflow gracefully
- Log device initialization at INFO level
</action>
  <verify>
1. Import check: `python -c "from src.voice.audio_capture import AudioCapture, AudioStream; print('Audio capture import OK')"`
2. Device listing: `python -c "from src.voice.audio_capture import AudioCapture; ac = AudioCapture(); print(ac.list_devices())"`
3. Mock stream test (without actual microphone):
   ```python
   import asyncio
   from src.voice.audio_capture import AudioCapture
   
   async def test():
       capture = AudioCapture()
       # Test with mock data
       print("AudioCapture initialized successfully")
       capture.stop()
   
   asyncio.run(test())
   ```
4. Audio conversion: Test resample_audio and stereo_to_mono with numpy arrays
  </verify>
  <done>AudioCapture and AudioStream created with PyAudio integration, Discord audio source support, and audio conversion utilities</done>
</task>

<task type="auto">
  <name>Create SpeechToText engine with Whisper backend</name>
  <files>src/voice/stt.py</files>
  <action>Create `src/voice/stt.py` with SpeechToText engine:

**Imports:**
- `import os`
- `import time`
- `import asyncio`
- `from typing import Optional, Callable, AsyncGenerator`
- `from dataclasses import dataclass`
- `from faster_whisper import WhisperModel`  # or openai-whisper
- `from src.voice.vad import VoiceActivityDetector, SpeechBuffer`
- `from src.voice.audio_capture import AudioCapture, AudioStream`
- `from src.core.logger import get_logger`
- `import numpy as np`

**Dataclass TranscriptionResult:**
```python
@dataclass
class TranscriptionResult:
    text: str
    confidence: float
    language: str
    duration_ms: int
    latency_ms: int  # End-to-end latency
    is_final: bool   # True when utterance complete
```

**Class SpeechToText:**
- `__init__(self, model_size: str = "base", device: str = "auto", compute_type: str = "int8")`:
  - Initialize WhisperModel: `self.model = WhisperModel(model_size, device=device, compute_type=compute_type)`
  - Initialize VAD: `self.vad = VoiceActivityDetector()`
  - Initialize AudioCapture: `self.capture = AudioCapture()`
  - Setup logger
  - Track performance metrics
  
- `async def transcribe_file(self, audio_path: str) -> TranscriptionResult`:
  - Transcribe pre-recorded audio file
  - Return complete TranscriptionResult
  
- `async def transcribe_stream(self, audio_stream: AudioStream, callback: Optional[Callable] = None) -> AsyncGenerator[TranscriptionResult, None]`:
  - Real-time streaming transcription
  - Use VAD to detect speech segments
  - Yield partial results as speech is detected
  - Call optional callback on final transcriptions
  
- `async def _process_audio_chunk(self, audio_data: bytes) -> Optional[str]`:
  - Internal method to transcribe audio chunk
  - Convert bytes to numpy array
  - Run Whisper inference
  - Return transcription text or None
  
- `async def _streaming_transcription_loop(self, audio_stream: AudioStream)`:
  - Main loop: read chunks → VAD → buffer → transcribe when speech ends
  - Maintain rolling buffer for context
  - Detect speech end (silence > 500ms)
  - Yield final transcription results
  
- `get_stats(self) -> dict`:
  - Return performance statistics:
    - total_transcriptions
    - avg_latency_ms
    - confidence_histogram
    - language_detected

**Whisper configuration:**
- Model sizes: "tiny", "base", "small" (recommended: "base" for speed/accuracy balance)
- Beam size: 5 for transcription quality
- Best of: 5 for sampling
- Temperature: 0.0 for deterministic results
- Condition on previous text: False (prevents hallucination)
- Language: "auto" for detection, or specify "en"

**Performance optimizations:**
- Use faster-whisper (CTranslate2 backend) for 4x speedup
- Quantize to int8 for CPU efficiency
- Batch small chunks for GPU efficiency
- Cache model in memory (don't reload)

**Implementation notes:**
- Transcription latency target: <3s for typical utterances
- VAD buffer adds ~300ms padding for natural speech boundaries
- Handle whisper hallucinations (repetitive text) with confidence threshold
- Log all transcriptions with latency metrics
</action>
  <verify>
1. Import check: `python -c "from src.voice.stt import SpeechToText, TranscriptionResult; print('STT import OK')"`
2. Model loading test (mock if model not downloaded):
   ```python
   # Test with tiny model first (fast download)
   from src.voice.stt import SpeechToText
   stt = SpeechToText(model_size="tiny")
   print(f"Model loaded: {stt.model}")
   ```
3. Transcription test (if model available):
   ```python
   import asyncio
   from src.voice.stt import SpeechToText
   
   async def test():
       stt = SpeechToText(model_size="tiny")
       # Create test audio (synthetic or load file)
       result = await stt.transcribe_file("tests/fixtures/test_audio.wav")
       print(f"Transcribed: {result.text}")
       print(f"Latency: {result.latency_ms}ms")
   
   asyncio.run(test())
   ```
4. Stats check: Verify get_stats() returns expected dictionary structure
  </verify>
  <done>SpeechToText engine created with Whisper backend, VAD integration, streaming transcription, and performance metrics tracking</done>
</task>

<task type="auto">
  <name>Create voice module initialization and exports</name>
  <files>src/voice/__init__.py</files>
  <action>Create `src/voice/__init__.py`:

```python
"""Voice I/O module for Demi.

Provides Speech-to-Text (STT) and Text-to-Speech (TTS) capabilities
for voice channel interactions.
"""

from src.voice.stt import SpeechToText, TranscriptionResult
from src.voice.audio_capture import AudioCapture, AudioStream
from src.voice.vad import VoiceActivityDetector

__all__ = [
    "SpeechToText",
    "TranscriptionResult", 
    "AudioCapture",
    "AudioStream",
    "VoiceActivityDetector",
]
```

**Add to requirements.txt:**
```
# Voice I/O (Phase 08)
faster-whisper>=1.0.0  # or openai-whisper
pyaudio>=0.2.11
webrtcvad>=2.0.10
numpy>=1.24.0
opus-lib>=3.0.1  # For Discord voice
```

**Implementation notes:**
- faster-whisper recommended over openai-whisper for 4x speedup
- PyAudio requires portaudio system library (install portaudio19-dev on Ubuntu)
- webrtcvad requires specific audio formats (16-bit PCM, specific frame sizes)
</action>
  <verify>
1. Import check: `python -c "from src.voice import SpeechToText, AudioCapture, VoiceActivityDetector; print('Voice module import OK')"`
2. Verify all exports available:
   ```python
   from src.voice import __all__
   print(f"Exports: {__all__}")
   assert len(__all__) == 5
   ```
3. Requirements check: Verify requirements.txt has voice dependencies
  </verify>
  <done>Voice module initialized with proper exports, all dependencies documented in requirements.txt</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **VAD Accuracy:** Test with sample audio files (speech vs silence) → verify >95% accuracy
2. **Audio Capture:** Test microphone stream (if available) or mock stream → verify 16kHz mono output
3. **STT Latency:** Transcribe 5-second audio clip → verify <5s end-to-end latency
4. **Streaming:** Run transcribe_stream with continuous audio → verify real-time results
5. **Integration:** Create end-to-end test: AudioCapture → VAD → SpeechToText → text output

**Benchmark targets:**
- Transcription latency (5s audio): <3s on CPU (base model)
- VAD accuracy: >95% (minimal false positives/negatives)
- Streaming latency: <500ms from speech end to transcription
</verification>

<success_criteria>
- VoiceActivityDetector accurately detects speech vs silence (>95% accuracy)
- AudioCapture successfully streams from microphone and Discord sources
- SpeechToText transcribes audio with <5s latency for typical utterances
- Whisper model loads and runs without errors (base or tiny model)
- VAD + STT pipeline produces clean, segmented transcriptions
- Performance metrics tracked (latency, confidence)
- All imports work, module properly exported
- Test coverage >80% for voice module components
</success_criteria>

<output>
After completion, create `.planning/phases/08-voice-io/08-01-SUMMARY.md`
</output>
