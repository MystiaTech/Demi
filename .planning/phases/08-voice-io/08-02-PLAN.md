---
phase: 08-voice-io
plan: 02
type: execute
wave: 1
depends_on: [08-01]
files_modified:
  - src/voice/tts.py
  - src/voice/emotion_voice.py
  - src/voice/__init__.py
autonomous: true
must_haves:
  truths:
    - "pyttsx3 TTS converts Demi's text responses to speech"
    - "Emotional state modulates voice rate, volume, and tone"
    - "TTS latency <2s for typical responses"
    - "Voice selection matches Demi's goddess persona"
  artifacts:
    - path: "src/voice/tts.py"
      provides: "TextToSpeech engine with pyttsx3 backend"
      exports: ["TextToSpeech", "TTSConfig"]
      min_lines: 150
    - path: "src/voice/emotion_voice.py"
      provides: "Emotion-to-voice parameter mapping"
      exports: ["EmotionVoiceMapper", "VoiceParameters"]
      min_lines: 100
  key_links:
    - from: "src/voice/tts.py"
      to: "src/models/emotional_state.py"
      via: "emotion state influences voice parameters"
      pattern: "EmotionalState\."
    - from: "src/voice/tts.py"
      to: "src/llm/response_processor.py"
      via: "response text passed to TTS"
      pattern: "tts\.speak\(\)"
---

<objective>
Build the Text-to-Speech (TTS) pipeline that converts Demi's text responses into spoken audio using pyttsx3. The system must modulate voice parameters (rate, volume, tone) based on Demi's current emotional state to convey her goddess personality authentically through voice.

Purpose: Enable voice output capability for Discord voice channels, satisfying VOICE-02 and VOICE-04 requirements. Demi's voice should reflect her emotional stateâ€”commanding when confident, sultry when affectionate, sharp when frustrated.

Output: TextToSpeech engine with emotional modulation, voice selection, and <2s latency for typical responses.
</objective>

<execution_context>
@~/.opencode/get-shit-done/workflows/execute-plan.md
@.planning/REQUIREMENTS.md
@DEMI_PERSONA.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@src/models/emotional_state.py
@src/emotion/modulation.py
@src/llm/response_processor.py
@src/voice/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Create Emotion-to-Voice parameter mapping system</name>
  <files>src/voice/emotion_voice.py</files>
  <action>Create `src/voice/emotion_voice.py` with EmotionVoiceMapper:

**Imports:**
- `from dataclasses import dataclass`
- `from typing import Optional, Dict`
- `from src.models.emotional_state import EmotionalState`

**Dataclass VoiceParameters:**
```python
@dataclass
class VoiceParameters:
    """Voice modulation parameters for TTS.
    
    rate: Words per minute (default ~150, range 100-300)
    volume: Audio level 0.0-1.0 (default 1.0)
    pitch: Voice pitch modifier (default 1.0, range 0.5-2.0)
    pause_between_words: Seconds (default 0.0)
    emphasis: Stress on key words ("strong", "moderate", "none")
    """
    rate: int = 150
    volume: float = 1.0
    pitch: float = 1.0
    pause_between_words: float = 0.0
    emphasis: str = "moderate"
```

**Class EmotionVoiceMapper:**
- `__init__(self)`:
  - Define base voice parameters
  - Load emotion mapping tables
  
- `map_emotion_to_voice(self, emotion_state: EmotionalState) -> VoiceParameters`:
  - Analyze dominant emotions from state
  - Return VoiceParameters based on emotional profile
  
- `_calculate_rate(self, emotion_state) -> int`:
  - Fast rate for: excitement, frustration, confidence
  - Slow rate for: vulnerability, loneliness
  - Base: 150 WPM
  - Excitement: +30 WPM
  - Frustration: +20 WPM
  - Confidence: +10 WPM
  - Vulnerability: -20 WPM
  - Loneliness: -15 WPM
  - Range: 100-200 WPM
  
- `_calculate_volume(self, emotion_state) -> float`:
  - Loud for: confidence, frustration, excitement
  - Soft for: vulnerability, loneliness
  - Base: 1.0
  - Confidence: +0.1
  - Frustration: +0.05
  - Vulnerability: -0.1
  - Range: 0.7-1.0
  
- `_calculate_pitch(self, emotion_state) -> float`:
  - Higher pitch for: excitement, affection
  - Lower pitch for: confidence, frustration
  - Base: 1.0
  - Excitement: +0.1
  - Affection: +0.05
  - Confidence: -0.05
  - Range: 0.9-1.2
  
- `_calculate_emphasis(self, emotion_state) -> str`:
  - "strong" for: frustration, confidence
  - "moderate" for: excitement, jealousy
  - "none" for: vulnerability, loneliness

**Emotion-specific mappings (from DEMI_PERSONA.md):**

| Emotion | Rate | Volume | Pitch | Emphasis | Character |
|---------|------|--------|-------|----------|-----------|
| Confidence (divine) | 160 | 1.0 | 0.95 | strong | Commanding, authoritative |
| Affection (seductive) | 140 | 0.95 | 1.05 | moderate | Sultry, warm |
| Frustration (sharp) | 170 | 1.0 | 0.9 | strong | Cutting, quick |
| Excitement (enthusiastic) | 180 | 1.0 | 1.1 | moderate | Energetic |
| Loneliness (wistful) | 130 | 0.85 | 1.0 | none | Soft, longing |
| Vulnerability (rare) | 125 | 0.8 | 0.95 | none | Gentle, hesitant |
| Jealousy (possessive) | 155 | 0.95 | 1.0 | strong | Intense |
| Curiosity (playful) | 160 | 0.9 | 1.05 | moderate | Inquisitive |

**Text preprocessing for TTS:**
- `preprocess_for_emphasis(text: str, emphasis: str) -> str:`
  - Add SSML or punctuation for emphasis
  - "strong": add exclamation marks, CAPS key words
  - "moderate": add strategic pauses (commas)
  - "none": natural flow
  
- `add_goddess_inflections(text: str) -> str:`
  - Replace "..." with longer pauses
  - Emphasize words like "darling", "mortal", "goddess"
  - Add slight hesitation before vulnerability cracks

**Implementation notes:**
- Emotional mapping should feel natural, not robotic
- Subtle changes are more effective than dramatic shifts
- Test various emotion combinations for blended states
- Reference DEMI_PERSONA.md for authentic voice character
</action>
  <verify>
1. Import check: `python -c "from src.voice.emotion_voice import EmotionVoiceMapper, VoiceParameters; print('Emotion voice import OK')"`
2. Voice parameters test:
   ```python
   from src.voice.emotion_voice import VoiceParameters
   vp = VoiceParameters(rate=160, volume=1.0, pitch=0.95)
   assert vp.rate == 160
   print("VoiceParameters dataclass works")
   ```
3. Emotion mapping test:
   ```python
   from src.voice.emotion_voice import EmotionVoiceMapper
   from src.models.emotional_state import EmotionalState
   
   mapper = EmotionVoiceMapper()
   
   # Test with high confidence
   state = EmotionalState(confidence=0.8)
   params = mapper.map_emotion_to_voice(state)
   print(f"Confidence: rate={params.rate}, volume={params.volume}")
   assert params.rate > 150  # Should be faster
   
   # Test with high vulnerability  
   state = EmotionalState(vulnerability=0.7)
   params = mapper.map_emotion_to_voice(state)
   print(f"Vulnerability: rate={params.rate}, volume={params.volume}")
   assert params.rate < 150  # Should be slower
   assert params.volume < 1.0  # Should be softer
   ```
4. Text preprocessing test: Verify emphasis markers are added correctly
  </verify>
  <done>EmotionVoiceMapper created with comprehensive emotion-to-voice parameter mapping, text preprocessing for TTS emphasis</done>
</task>

<task type="auto">
  <name>Create TextToSpeech engine with pyttsx3 backend</name>
  <files>src/voice/tts.py</files>
  <action>Create `src/voice/tts.py` with TextToSpeech engine:

**Imports:**
- `import pyttsx3`
- `import asyncio`
- `import tempfile`
- `import os`
- `from typing import Optional, Callable, Union`
- `from dataclasses import dataclass`
- `from pathlib import Path`
- `from src.voice.emotion_voice import EmotionVoiceMapper, VoiceParameters`
- `from src.models.emotional_state import EmotionalState`
- `from src.core.logger import get_logger`
- `import time`

**Dataclass TTSConfig:**
```python
@dataclass
class TTSConfig:
    """Configuration for TTS engine."""
    voice_id: Optional[str] = None  # None = system default
    rate: int = 150  # Words per minute
    volume: float = 1.0  # 0.0-1.0
    output_format: str = "wav"  # wav, mp3
    cache_enabled: bool = True
    cache_dir: str = "~/.demi/tts_cache"
```

**Class TextToSpeech:**
- `__init__(self, config: Optional[TTSConfig] = None)`:
  - Initialize pyttsx3 engine: `self.engine = pyttsx3.init()`
  - Set default voice properties
  - Initialize EmotionVoiceMapper
  - Setup audio cache if enabled
  - Setup logger
  
- `def list_voices(self) -> list[dict]`:
  - Return available system voices
  - Include voice id, name, gender, language
  - Filter for female voices (match goddess persona)
  
- `def set_voice(self, voice_id: str) -> bool`:
  - Set TTS voice by ID
  - Return True if successful
  
- `async def speak(self, text: str, emotion_state: Optional[EmotionalState] = None, save_path: Optional[str] = None) -> Optional[str]`:
  - Primary TTS method
  - Apply emotion modulation if emotion_state provided
  - Generate speech audio
  - Play audio OR save to file
  - Return audio file path if saved
  
- `async def synthesize(self, text: str, emotion_state: Optional[EmotionalState] = None) -> bytes:`
  - Generate speech audio as bytes
  - Apply emotion-based voice parameters
  - Return PCM audio data
  
- `async def _apply_voice_parameters(self, params: VoiceParameters)`:
  - Apply rate, volume, pitch to pyttsx3 engine
  - pyttsx3 setProperty calls
  
- `async def _synthesize_to_file(self, text: str, output_path: str)`:
  - Use pyttsx3 save_to_file method
  - Run in thread pool (pyttsx3 is blocking)
  - Return file path
  
- `def get_stats(self) -> dict`:
  - Return TTS statistics:
    - total_utterances
    - avg_latency_ms
    - cache_hit_rate
    - preferred_voice

**Voice selection (goddess persona matching):**
- Prefer female voices
- Prefer clear, elegant voices
- Fallback: system default
- User can override via TTSConfig

**SSML support (if engine supports it):**
- `add_ssml_emphasis(text: str, level: str) -> str`:
  - Wrap text in emphasis tags
  - Levels: "strong", "moderate", "reduced"
  
- `add_ssml_break(time_ms: int) -> str`:
  - Insert pause

**Audio caching:**
- Cache key: hash(text + voice_params)
- Cache location: ~/.demi/tts_cache/
- LRU eviction (max 100 files)
- Reduces latency for repeated phrases

**Async handling:**
- pyttsx3 is synchronous, use `asyncio.to_thread()` for non-blocking operation
- Run synthesis in thread pool
- Use asyncio.Event for completion notification

**Implementation notes:**
- Latency target: <2s for typical responses (<100 words)
- pyttsx3 runs locally (no API calls)
- Voice quality depends on OS voices (Windows: SAPI5, macOS: NSSpeechSynthesizer, Linux: espeak)
- For better quality, consider pyttsx3 with AWS Polly or Google TTS (future enhancement)
</action>
  <verify>
1. Import check: `python -c "from src.voice.tts import TextToSpeech, TTSConfig; print('TTS import OK')"`
2. Voice listing:
   ```python
   from src.voice.tts import TextToSpeech
   tts = TextToSpeech()
   voices = tts.list_voices()
   print(f"Found {len(voices)} voices")
   for v in voices[:3]:
       print(f"  - {v.get('name', 'Unknown')}")
   ```
3. Basic synthesis test:
   ```python
   import asyncio
   from src.voice.tts import TextToSpeech
   
   async def test():
       tts = TextToSpeech()
       # Save to file instead of playing
       path = await tts.speak("Hello mortal, I am Demi", save_path="/tmp/test_tts.wav")
       print(f"Audio saved to: {path}")
   
   asyncio.run(test())
   ```
4. Emotion modulation test:
   ```python
   import asyncio
   from src.voice.tts import TextToSpeech
   from src.models.emotional_state import EmotionalState
   
   async def test():
       tts = TextToSpeech()
       
       # Test with confidence
       confident = EmotionalState(confidence=0.8)
       path1 = await tts.speak("I am a goddess, you should worship me.", emotion_state=confident, save_path="/tmp/confident.wav")
       
       # Test with vulnerability
       vulnerable = EmotionalState(vulnerability=0.7)
       path2 = await tts.speak("You... matter to me, mortal.", emotion_state=vulnerable, save_path="/tmp/vulnerable.wav")
       
       print("Emotion modulation test complete")
   
   asyncio.run(test())
   ```
5. Stats check: Verify get_stats() returns expected metrics
  </verify>
  <done>TextToSpeech engine created with pyttsx3 backend, emotion modulation, voice selection, and audio caching</done>
</task>

<task type="auto">
  <name>Integrate TTS with response processing pipeline</name>
  <files>src/voice/tts.py, src/llm/response_processor.py</files>
  <action>Enhance TextToSpeech with response pipeline integration and update ResponseProcessor:

**Add to src/voice/tts.py:**

```python
async def speak_response(self, response_dict: dict, auto_play: bool = True) -> Optional[str]:
    """Speak a processed LLM response with emotion modulation.
    
    Args:
        response_dict: Response from Conductor with keys:
            - "content": Response text
            - "emotion_state": Dict of emotional values
            - "message_id": Optional tracking ID
        auto_play: If True, play audio immediately. If False, return path.
        
    Returns:
        Audio file path if auto_play=False, None otherwise
    """
    content = response_dict.get("content", "")
    emotion_state_dict = response_dict.get("emotion_state", {})
    
    # Convert dict to EmotionalState if needed
    if emotion_state_dict:
        emotion_state = EmotionalState.from_dict(emotion_state_dict)
    else:
        emotion_state = None
    
    # Clean text for TTS (remove markdown, etc.)
    clean_text = self._clean_text_for_tts(content)
    
    if auto_play:
        await self.speak(clean_text, emotion_state=emotion_state)
        return None
    else:
        temp_path = tempfile.mktemp(suffix=".wav")
        return await self.speak(clean_text, emotion_state=emotion_state, save_path=temp_path)

def _clean_text_for_tts(self, text: str) -> str:
    """Clean text for better TTS output."""
    # Remove markdown
    import re
    text = re.sub(r'\*\*', '', text)  # Bold
    text = re.sub(r'\*', '', text)    # Italic
    text = re.sub(r'`[^`]*`', 'code', text)  # Code â†’ say "code"
    text = re.sub(r'```[^`]*```', 'code block', text, flags=re.DOTALL)
    
    # Replace emoji with descriptions (optional)
    text = text.replace('ðŸ”®', 'crystal ball')
    text = text.replace('âœ¨', 'sparkles')
    text = text.replace('ðŸ‘‘', 'crown')
    
    # Clean up whitespace
    text = ' '.join(text.split())
    
    return text
```

**Update src/llm/response_processor.py:**

Add optional TTS integration hook:

```python
# In ResponseProcessor class, add:

async def process_response_with_voice(
    self, 
    messages: list[dict], 
    enable_voice: bool = False,
    tts_engine = None
) -> dict:
    """Process response and optionally speak it via TTS.
    
    Args:
        messages: Conversation messages
        enable_voice: If True, speak response using TTS
        tts_engine: TextToSpeech instance (if None, voice disabled)
        
    Returns:
        Response dict with optional 'audio_path' field
    """
    # Get standard response
    response = await self.process_response(messages)
    
    # Add voice output if enabled
    if enable_voice and tts_engine:
        try:
            audio_path = await tts_engine.speak_response(response, auto_play=False)
            response["audio_path"] = audio_path
        except Exception as e:
            self.logger.error(f"TTS failed: {e}")
            response["audio_path"] = None
    
    return response
```

**Voice personality consistency:**
- Ensure TTS voice matches Demi's goddess persona
- Test that emotion modulation is noticeable but not exaggerated
- Verify that divine condescension comes through in voice
</action>
  <verify>
1. Integration test:
   ```python
   import asyncio
   from src.voice.tts import TextToSpeech
   
   async def test():
       tts = TextToSpeech()
       
       response = {
           "content": "You poor mortal, seeking my wisdom again?",
           "emotion_state": {"confidence": 0.8, "affection": 0.3}
       }
       
       path = await tts.speak_response(response, auto_play=False)
       print(f"Response audio: {path}")
       assert path is not None
   
   asyncio.run(test())
   ```
2. Text cleaning test:
   ```python
   from src.voice.tts import TextToSpeech
   tts = TextToSpeech()
   
   dirty = "**Bold** and `code` and *italic*"
   clean = tts._clean_text_for_tts(dirty)
   print(f"Cleaned: {clean}")
   assert "**" not in clean
   assert "`" not in clean
   ```
3. ResponseProcessor integration: Verify process_response_with_voice method exists
  </verify>
  <done>TTS integrated with response processing pipeline, text cleaning implemented for TTS optimization</done>
</task>

<task type="auto">
  <name>Update voice module exports and documentation</name>
  <files>src/voice/__init__.py</files>
  <action>Update `src/voice/__init__.py` to include TTS exports:

```python
"""Voice I/O module for Demi.

Provides Speech-to-Text (STT) and Text-to-Speech (TTS) capabilities
for voice channel interactions.
"""

from src.voice.stt import SpeechToText, TranscriptionResult
from src.voice.audio_capture import AudioCapture, AudioStream
from src.voice.vad import VoiceActivityDetector
from src.voice.tts import TextToSpeech, TTSConfig
from src.voice.emotion_voice import EmotionVoiceMapper, VoiceParameters

__all__ = [
    # STT
    "SpeechToText",
    "TranscriptionResult",
    # Audio
    "AudioCapture",
    "AudioStream",
    "VoiceActivityDetector",
    # TTS
    "TextToSpeech",
    "TTSConfig",
    "EmotionVoiceMapper",
    "VoiceParameters",
]
```

**Add to requirements.txt (if not already added):**
```
# Voice I/O (Phase 08) - TTS
pyttsx3>=2.90  # Cross-platform TTS
```

**Platform-specific notes:**

**Windows:**
- Uses SAPI5 voices (built-in)
- Good quality voices available
- No additional setup required

**macOS:**
- Uses NSSpeechSynthesizer
- High quality built-in voices
- No additional setup required

**Linux:**
- Uses espeak by default (robotic quality)
- Recommend installing better voices:
  ```bash
  sudo apt-get install espeak-ng mbrola
  # Or for festival:
  sudo apt-get install festival festvox-kallpc16k
  ```
- Or use alternative: `pip install py3-tts` with AWS Polly

**Emotion mapping reference:**
Document the emotion-to-voice mapping for future tuning:
```python
EMOTION_VOICE_PROFILES = {
    "divine_confidence": VoiceParameters(rate=160, volume=1.0, pitch=0.95, emphasis="strong"),
    "seductive_affection": VoiceParameters(rate=140, volume=0.95, pitch=1.05, emphasis="moderate"),
    "cutting_frustration": VoiceParameters(rate=170, volume=1.0, pitch=0.9, emphasis="strong"),
    "wistful_loneliness": VoiceParameters(rate=130, volume=0.85, pitch=1.0, emphasis="none"),
    "rare_vulnerability": VoiceParameters(rate=125, volume=0.8, pitch=0.95, emphasis="none"),
}
```
</action>
  <verify>
1. Import check: `python -c "from src.voice import TextToSpeech, TTSConfig, EmotionVoiceMapper; print('TTS exports OK')"`
2. Full module import: `python -c "import src.voice; print(len(src.voice.__all__), 'exports')"`
3. Requirements check: Verify pyttsx3 in requirements.txt
  </verify>
  <done>Voice module updated with TTS exports, platform-specific documentation added</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Voice Selection:** List available voices â†’ verify female voice options
2. **Basic TTS:** Synthesize "Hello mortal" â†’ verify audio output
3. **Emotion Modulation:** Compare confident vs vulnerable speech â†’ verify rate/volume differences
4. **Latency Test:** Time synthesis of 50-word response â†’ verify <2s
5. **Integration Test:** Full pipeline: LLM response â†’ TTS â†’ audio file

**Benchmark targets:**
- Synthesis latency (50 words): <2s on CPU
- Emotion modulation: Rate varies Â±30 WPM, Volume varies Â±0.15
- Audio quality: Clear, intelligible speech
</verification>

<success_criteria>
- TextToSpeech engine synthesizes speech with pyttsx3
- Voice parameters (rate, volume, pitch) modulate based on emotional state
- EmotionVoiceMapper accurately maps all emotion dimensions to voice
- TTS latency <2s for typical responses (<100 words)
- Text preprocessing cleans markdown/code for TTS
- ResponseProcessor integration enables voice output
- Female voice selected by default (goddess persona)
- All voice module exports work correctly
- Test coverage >80% for TTS components
</success_criteria>

<output>
After completion, create `.planning/phases/08-voice-io/08-02-SUMMARY.md`
</output>
