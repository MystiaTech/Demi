---
phase: 08-voice-io
plan: 03
type: execute
wave: 3
depends_on: ["08-01", "08-02"]
files_modified: ["src/api/websocket.py", "src/voice/android_voice.py", "android/app/src/main/java/com/demi/voice/VoiceManager.kt", "android/app/src/main/java/com/demi/voice/VoiceService.kt", "tests/test_android_voice.py"]
autonomous: true

must_haves:
  truths:
    - "Android app captures voice input and sends to backend"
    - "Backend processes voice input and returns LLM response"
    - "Android app plays voice responses with emotional tone"
    - "Voice communication works over WebSocket connection"
    - "Voice features can be enabled/disabled in Android settings"
  artifacts:
    - path: "src/voice/android_voice.py"
      provides: "Backend voice processing for Android"
      min_lines: 50
      contains: "class AndroidVoiceProcessor"
    - path: "src/api/websocket.py"
      provides: "WebSocket voice message handling"
      min_lines: 30
      contains: "handle_voice_message"
    - path: "android/app/src/main/java/com/demi/voice/VoiceManager.kt"
      provides: "Android voice recording and playback"
      min_lines: 60
      contains: "class VoiceManager"
    - path: "android/app/src/main/java/com/demi/voice/VoiceService.kt"
      provides: "Background voice service for Android"
      min_lines: 40
      contains: "class VoiceService"
    - path: "tests/test_android_voice.py"
      provides: "Android voice integration tests"
      min_lines: 30
  key_links:
    - from: "android/app/src/main/java/com/demi/voice/VoiceManager.kt"
      to: "src/api/websocket.py"
      via: "WebSocket transmission of voice audio data"
      pattern: "sendVoiceData.*websocket"
    - from: "src/api/websocket.py"
      to: "src/voice/android_voice.py"
      via: "Backend voice message processing"
      pattern: "handle_voice_message.*stt"
    - from: "src/voice/android_voice.py"
      to: "src/conductor/orchestrator.py"
      via: "Voice transcription to LLM pipeline"
      pattern: "request_inference.*voice"
    - from: "src/voice/android_voice.py"
      to: "android/app/src/main/java/com/demi/voice/VoiceManager.kt"
      via: "Voice response audio transmission"
      pattern: "playVoiceResponse.*audio"
---

<objective>
Integrate voice I/O system with Android mobile app for bidirectional voice communication.

Purpose: Enable Android users to have voice conversations with Demi through the mobile app, with voice input capture, backend processing, and spoken responses with emotional modulation.

Output: Complete Android voice integration with WebSocket communication, background voice service, and emotional voice responses.
</objective>

<execution_context>
@~/.opencode/get-shit-done/workflows/execute-plan.md
@~/.opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/08-voice-io/08-01-SUMMARY.md
@.planning/phases/08-voice-io/08-02-SUMMARY.md
@.planning/PROJECT.md
@.planning/ROADMAP.md
@src/api/websocket.py
@src/voice/stt.py
@src/voice/tts.py
@android/app/src/main/java/com/demi/websocket/WebSocketManager.kt
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Android voice processing backend</name>
  <files>src/voice/android_voice.py, src/api/websocket.py</files>
  <action>
1. Create AndroidVoiceProcessor class in src/voice/android_voice.py with:
   - process_voice_audio() method for incoming voice data
   - WebSocket audio buffer management
   - Integration with WhisperSTT for transcription
   - Integration with pyttsx3TTS for response generation
   - Emotional state application to voice responses
   - Audio format conversion and optimization

2. Update WebSocket handler in src/api/websocket.py:
   - Add handle_voice_message() for voice data processing
   - Support for binary audio data transmission
   - Voice message type differentiation (text vs voice)
   - Voice response audio streaming
   - Error handling for audio processing failures

3. WebSocket protocol enhancement:
   - Define voice message format (JSON metadata + binary audio)
   - Add voice session management
   - Handle voice interruption and cancellation
   - Support for real-time voice streaming vs chunked processing
   - Voice quality adaptation based on connection speed

4. Backend configuration:
   - Voice processing timeout and retry logic
   - Audio quality settings for mobile optimization
   - Voice session persistence and recovery
   - Performance monitoring for voice processing
  </action>
  <verify>python -c "from src.voice.android_voice import AndroidVoiceProcessor; from src.api.websocket import handle_voice_message; print('Android voice backend imports successfully')"</verify>
  <done>Android voice processing backend created with WebSocket integration</done>
</task>

<task type="auto">
  <name>Task 2: Implement Android voice recording and playback</name>
  <files>android/app/src/main/java/com/demi/voice/VoiceManager.kt</files>
  <action>
1. Create VoiceManager class in VoiceManager.kt with:
   - startVoiceCapture() and stopVoiceCapture() methods
   - Real-time audio recording using AudioRecord
   - Audio buffer management and compression
   - Voice activity detection to optimize transmission
   - Integration with WebSocket for audio data sending
   - Audio format: 16kHz, 16-bit, mono for Whisper compatibility

2. Audio processing features:
   - Noise reduction and echo cancellation
   - Audio compression for network efficiency
   - Silence detection to avoid sending empty audio
   - Audio quality monitoring and adaptation
   - Error handling for microphone permissions

3. Voice response playback:
   - playVoiceResponse() method for TTS audio
   - Audio stream buffering and playback
   - Handle concurrent playback and recording
   - Audio session management and cleanup
   - Integration with Android AudioManager

4. User interface integration:
   - Voice input button and visual feedback
   - Recording indicator and volume meter
   - Voice response playback controls
   - Voice mode toggle in settings
   - Error handling and user notifications
  </action>
  <verify>cd android && ./gradlew compileDebugKotlin && echo "Android voice manager compiles successfully"</verify>
  <done>Android voice recording and playback system implemented with full UI integration</done>
</task>

<task type="auto">
  <name>Task 3: Create background voice service and integration</name>
  <files>android/app/src/main/java/com/demi/voice/VoiceService.kt, tests/test_android_voice.py</files>
  <action>
1. Create VoiceService in VoiceService.kt with:
   - Background service for always-listening mode
   - Wake word detection using simple keyword spotting
   - Service lifecycle management (start/stop)
   - Notification handling for voice service status
   - Battery optimization and resource management
   - Integration with VoiceManager for audio processing

2. Background service features:
   - Persistent voice connection maintenance
   - Automatic reconnection on network issues
   - Service notification with voice status
   - Wake word sensitivity settings
   - Privacy controls and user consent

3. Android integration points:
   - Service manifest declarations and permissions
   - Foreground service configuration
   - Battery optimization exclusion
   - Microphone permission handling
   - Integration with existing WebSocket connection

4. Create comprehensive tests in tests/test_android_voice.py:
   - Mock WebSocket for voice message testing
   - Test Android voice processor with various audio formats
   - Test emotional voice response application
   - Test WebSocket voice message handling
   - Test error handling for audio processing failures
   - Test integration with conductor pipeline

5. Integration testing:
   - End-to-end voice flow: Android -> Backend -> Android
   - Performance testing for real-time voice processing
   - Network interruption and recovery testing
   - Concurrent voice and text interaction testing
  </action>
  <verify>cd android && ./gradlew assembleDebug && echo "Android voice service builds successfully" && pytest tests/test_android_voice.py -v</verify>
  <done>Background voice service created with full integration and comprehensive test coverage</done>
</task>

</tasks>

<verification>
1. Android app can capture voice input and send to backend
2. Backend processes voice audio and returns LLM responses
3. Android app plays voice responses with emotional modulation
4. WebSocket voice communication handles interruptions gracefully
5. Background voice service works in always-listening mode
6. Wake word detection triggers voice activation on Android
7. Voice features integrate seamlessly with existing chat UI
8. All tests pass for Android voice integration
</verification>

<success_criteria>
- Android voice communication fully functional end-to-end
- Real-time voice processing with minimal latency (<5 seconds)
- Emotional voice responses match Demi's personality on mobile
- Background voice service with wake word detection
- Robust error handling and network reconnection
- Privacy controls and user consent for voice features
- Performance optimized for mobile devices
</success_criteria>

<output>
After completion, create `.planning/phases/08-voice-io/08-03-SUMMARY.md`
</output>