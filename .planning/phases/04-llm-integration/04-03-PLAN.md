---
phase: 04-llm-integration
plan: 03
type: execute
wave: 2
depends_on: ["04-01", "04-02"]
files_modified:
  - src/llm/response_processor.py
  - src/llm/inference.py
autonomous: true
must_haves:
  truths:
    - "Response text extracted from Ollama response object without corruption"
    - "Interaction logged with complete before/after emotional state"
    - "Emotional state updated based on successful LLM interaction"
    - "Response post-processing cleans up text (no truncation artifacts, proper formatting)"
  artifacts:
    - path: "src/llm/response_processor.py"
      provides: "ResponseProcessor class that cleans, logs, and persists responses"
      exports: ["ResponseProcessor", "ProcessedResponse"]
    - path: "src/llm/inference.py"
      provides: "Updated with response processing integration"
      exports: ["OllamaInference"]
  key_links:
    - from: "src/llm/response_processor.py"
      to: "src/models/emotional_state.py"
      via: "Update emotional state on successful interaction"
      pattern: "emotional_state|EmotionalState|update"
    - from: "src/llm/response_processor.py"
      to: "src/core/database.py"
      via: "Log interaction to database"
      pattern: "session|database|log_interaction"
---

<objective>
Handle response generation: extract text from Ollama, clean up artifacts, log interactions with emotional state transitions, and persist conversation history. This closes the loop between input and output.

Purpose: Raw Ollama responses need cleanup (whitespace, special tokens). Interactions must be logged for emotional decay calculations. Emotional state updates after each interaction.

Output: ResponseProcessor that cleans responses, logs interactions, and triggers emotional state updates.
</objective>

<execution_context>
@~/.opencode/get-shit-done/workflows/execute-plan.md
@.planning/phases/04-llm-integration/04-DISCOVERY.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/phases/03-emotional-system/03-04-SUMMARY.md
@src/llm/inference.py
@src/llm/prompt_builder.py
@src/llm/history_manager.py
@src/models/emotional_state.py
@src/core/database.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ResponseProcessor for text cleaning and interaction logging</name>
  <files>src/llm/response_processor.py</files>
  <action>
Create `src/llm/response_processor.py` with:

**ProcessedResponse dataclass:**
```python
@dataclass
class ProcessedResponse:
    text: str  # Cleaned response text
    tokens_generated: int  # Token count of response
    inference_time_sec: float  # Time from request to response
    interaction_log: dict  # Logged interaction data
    emotional_state_before: EmotionalState
    emotional_state_after: Optional[EmotionalState] = None
```

**Class ResponseProcessor:**
- Constructor: `__init__(logger: DemiLogger, db_session, interaction_handler: InteractionHandler)`
- Method: `process_response(response_text: str, inference_time_sec: float, emotional_state_before: EmotionalState, interaction_type: str = "llm_response") -> ProcessedResponse`
  - Input: raw response from Ollama, timing, emotional state before response, interaction type
  - Output: ProcessedResponse with cleaned text and logged interaction
  - Logic:
    1. Clean response text:
       - Strip leading/trailing whitespace
       - Remove special tokens (if any: <|end|>, <|eot_id|>, etc.)
       - Replace multiple newlines with single newline
       - Verify non-empty (if empty, use fallback: "I forgot what I was thinking... try again?")
    2. Count tokens in cleaned response: `token_counter(response_text)`
    3. Log interaction:
       - Create interaction log dict with:
         - timestamp: current UTC
         - interaction_type: "llm_response"
         - response_text: full text
         - inference_time_sec: timing
         - token_count: response tokens
    4. Update emotional state:
       - Call `interaction_handler.handle_interaction(emotional_state_before, "successful_response", context={"response_length": len(response_text)})`
       - This triggers emotional updates (excitement +1, confidence +0.5, etc.)
       - Get updated state: `emotional_state_after = result.emotional_state`
    5. Persist to database:
       - Save interaction log with InteractionPersistence (from Phase 03)
       - Include before/after emotional states
    6. Create ProcessedResponse object with all metadata
    7. Log at INFO: "Processed response ({X} tokens) in {Y:.2f}sec. Emotion delta: loneliness {before:.1f} → {after:.1f}"
    8. Return ProcessedResponse

- Method: `_clean_text(text: str) -> str` (private helper):
  - Strip whitespace
  - Remove token artifacts
  - Normalize newlines
  - Return cleaned text

**Integration with inference pipeline:**
- OllamaInference.chat() returns raw text
- ResponseProcessor.process_response() cleans and logs
- Cleaned text returned to caller (platforms/Discord/Android)

**Test cases:**
- `test_clean_response_strips_whitespace()`
- `test_clean_response_removes_tokens()`
- `test_process_response_logs_interaction()`
- `test_process_response_updates_emotional_state()`
  </action>
  <verify>
1. `from src.llm.response_processor import ResponseProcessor, ProcessedResponse` succeeds
2. Clean text function:
   - Input: "  hello world  \n\n  " → Output: "hello world"
   - Input: "hello <|end|> world" → Output: "hello world"
   - Input: "" → Output uses fallback
3. Process response:
   - Create mock emotional state, interaction handler
   - Call process_response with test data
   - Verify: ProcessedResponse contains cleaned text, inference_time, emotional_state_after
4. Emotional state updated:
   - Before: loneliness=5.0
   - After: loneliness should decrease (successful interaction)
   - Verify: emotional_state_after.loneliness < emotional_state_before.loneliness
5. Interaction logged with timestamps and state transitions
  </verify>
  <done>
ResponseProcessor cleans response text, counts tokens, logs interactions, and updates emotional state. Integration point clear for Conductor to call.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire ResponseProcessor into inference pipeline and add end-to-end tests</name>
  <files>src/llm/inference.py, src/conductor/orchestrator.py</files>
  <action>
Modify inference pipeline to use ResponseProcessor:

**Update OllamaInference (src/llm/inference.py):**
- Add to constructor: `response_processor: Optional[ResponseProcessor] = None`
- Modify `chat()` method to optionally use ResponseProcessor:
  - After successful inference, if response_processor provided:
    ```python
    response_text = response.message.content  # Raw text from Ollama
    processed = await response_processor.process_response(
        response_text, 
        inference_time_sec, 
        emotional_state_before
    )
    return processed.text  # Return cleaned text
    ```
  - If response_processor not provided, return raw text (backward compatible)
  - Measure inference_time using time.time() before/after Ollama call

**Update Conductor (src/conductor/orchestrator.py):**
- Instantiate ResponseProcessor in startup:
  ```python
  self.response_processor = ResponseProcessor(
      logger=self.logger,
      db_session=self.db_session,
      interaction_handler=self.interaction_handler  # From Phase 03
  )
  self.llm.response_processor = self.response_processor
  ```
- Update `request_inference()` to use processor:
  - Pass emotional_state_before to ResponseProcessor
  - Log processed response with timing and emotional delta

**Create integration test (tests/test_llm_e2e.py):**
- `test_inference_with_response_processing()`
  - Create mock Ollama response
  - Run through full pipeline: inference → response processing → logging
  - Verify: text cleaned, interaction logged, emotional state updated, timing recorded
- `test_inference_pipeline_error_handling()`
  - Mock Ollama timeout, verify fallback response
  - Mock empty response, verify fallback response
  - Verify errors logged but don't crash pipeline
- `test_inference_latency_tracking()`
  - Time full pipeline (inference + processing)
  - Verify < 3 seconds for typical response
  - Verify latency recorded in metrics
  </action>
  <verify>
1. Conductor starts with response_processor initialized
2. Call `conductor.request_inference([{"role": "user", "content": "test"}])`
3. Response returned, cleaned (whitespace stripped)
4. Logs show: "Processed response ({X} tokens) in {Y:.2f}sec"
5. Interaction logged to database
6. Emotional state updated: loneliness decreased (successful interaction)
7. Integration tests pass: `pytest tests/test_llm_e2e.py -v`
8. Latency < 3 seconds p90 on test system
  </verify>
  <done>
ResponseProcessor integrated into inference pipeline. Full end-to-end: message → inference → response processing → logging → emotional update. All tests passing. Ready for Plan 04 (Conductor integration and AUTO-01 self-awareness).
  </done>
</task>

</tasks>

<verification>
After all tasks:
1. ResponseProcessor created and tested
2. Response text cleaned (no artifacts)
3. Interactions logged with emotional state transitions
4. Emotional state updated after successful responses
5. Inference latency tracked and under 3 seconds
6. End-to-end tests passing: `pytest tests/test_llm_response_processor.py tests/test_llm_e2e.py -v`
7. Logs show processing details and emotional deltas
8. Ready for Plan 04 (full Conductor integration + AUTO-01)
</verification>

<success_criteria>
- ResponseProcessor cleans response text (strips whitespace, removes tokens)
- Interactions logged with complete before/after emotional state
- Emotional state updates after successful LLM responses
- Inference latency measured and tracked in metrics
- End-to-end pipeline working: message → inference → processing → logging → emotional update
- All tests passing (5-7 tests total)
- Logs show processing details, timing, emotional deltas
</success_criteria>

<output>
Create `.planning/phases/04-llm-integration/04-03-SUMMARY.md` with:
- ResponseProcessor created, response text cleaned and logged
- Interactions persisted with before/after emotional states
- Emotional state updates after responses (loneliness ↓, confidence ↑, etc.)
- Inference latency tracked, baseline <3sec
- End-to-end pipeline validated: inference → response processing → logging
- Integration with Phase 03 emotional system complete
- Ready for Plan 04 (full Conductor integration and self-awareness feature)
</output>
