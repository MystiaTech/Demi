---
phase: 04-llm-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llm/__init__.py
  - src/llm/inference.py
  - src/llm/config.py
autonomous: true
must_haves:
  truths:
    - "Ollama server (localhost:11434) responds to health check without errors"
    - "LLM generates response from user message in <3 seconds p90"
    - "Context window limit (8K tokens) is enforced before inference"
    - "Inference errors don't crash Conductor, logged gracefully"
  artifacts:
    - path: "src/llm/inference.py"
      provides: "OllamaInference class with async chat interface"
      exports: ["OllamaInference", "InferenceError", "ContextOverflowError"]
    - path: "src/llm/config.py"
      provides: "LLM configuration (model name, temperature, timeouts)"
      exports: ["LLMConfig"]
    - path: "src/llm/__init__.py"
      provides: "LLM module exports"
      exports: ["OllamaInference", "LLMConfig", "InferenceError"]
  key_links:
    - from: "src/llm/inference.py"
      to: "http://localhost:11434"
      via: "ollama.AsyncClient().chat()"
      pattern: "AsyncClient|ollama.chat"
    - from: "src/llm/inference.py"
      to: "src/core/logger.py"
      via: "logger.info/error on inference events"
      pattern: "logger\\.(info|error|warning)"
---

<objective>
Create the inference engine foundation that interfaces with Ollama, manages LLM requests, and enforces context window constraints. This establishes the core async pipeline for response generation.

Purpose: Demi needs a reliable, fast inference system that talks to Ollama without blocking Conductor. Errors must be caught and logged, not crash the system.

Output: OllamaInference class with async chat interface, context trimming, and comprehensive error handling.
</objective>

<execution_context>
@~/.opencode/get-shit-done/workflows/execute-plan.md
@.planning/phases/04-llm-integration/04-DISCOVERY.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/PROJECT.md
@.planning/phases/03-emotional-system/03-04-SUMMARY.md
@src/core/logger.py
@src/core/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLM inference engine with Ollama async client</name>
  <files>src/llm/__init__.py, src/llm/inference.py, src/llm/config.py</files>
  <action>
Create `src/llm/` package with three files:

**src/llm/config.py:**
- Dataclass `LLMConfig` with: model_name (default "llama3.2:1b"), temperature (0.7), max_tokens (256), timeout_sec (10), ollama_base_url (default "http://localhost:11434")
- Load from global config.llm_settings if available
- Validate model_name non-empty, temperature in [0.0, 1.0], max_tokens > 0

**src/llm/inference.py:**
- Class `OllamaInference`:
  - Constructor: `__init__(config: LLMConfig, logger: DemiLogger)`
  - Method: `async health_check() -> bool` - calls Ollama `/api/tags` endpoint, returns True if 200, False otherwise (no exception)
  - Method: `async chat(messages: list[dict], max_context_tokens: int = 8000) -> str` - primary interface
    - Validates messages format: each dict has "role" (system/user/assistant) and "content" (str)
    - Validates total context (sum of token counts for all messages) ≤ max_context_tokens - 256 (safety margin)
    - Calls `ollama.AsyncClient().chat()` with timeout=self.config.timeout_sec
    - Returns response.message.content (just the text)
    - Raises `InferenceError` on HTTP errors, timeouts, Ollama not running
    - Raises `ContextOverflowError` if context exceeds limit even before trimming
  - Method: `_count_tokens(text: str) -> int` - estimates tokens (fallback: len(text) // 4)

- Custom exceptions:
  - `InferenceError(Exception)` - generic LLM error
  - `ContextOverflowError(InferenceError)` - context window exceeded

**src/llm/__init__.py:**
- Export: OllamaInference, LLMConfig, InferenceError, ContextOverflowError

Implementation notes:
- Use `ollama` package (pip install ollama)
- AsyncClient from ollama, not synchronous chat()
- Log all inference calls at DEBUG level (model, message count, token estimate)
- Log errors at ERROR level with full context
- Do NOT attempt to load transformers tokenizer yet (Plan 02)
- Token counting fallback is acceptable for Plan 01 (token validation only)
  </action>
  <verify>
1. `from src.llm import OllamaInference, LLMConfig, InferenceError, ContextOverflowError` succeeds
2. `python -c "from src.llm import OllamaInference; print(OllamaInference.__doc__)"` shows docstring
3. Run unit tests (create test_inference.py):
   - `test_llm_config_validation()` - valid config loads, invalid config raises ValueError
   - `test_ollama_health_check()` - mocked Ollama returns True on 200, False on 500
   - `test_chat_message_validation()` - invalid messages format raises ValueError
   - `test_context_overflow()` - messages with >8000 tokens raise ContextOverflowError
   - `test_token_count_estimation()` - fallback token counter estimates ~1 token per 4 chars
4. All tests pass without warnings
  </verify>
  <done>
OllamaInference class created with async chat interface, context validation, and error handling. Token counting fallback implemented. Health check method operational. All exports available from src.llm package.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate inference engine with Conductor and wire health monitoring</name>
  <files>src/conductor/orchestrator.py, src/llm/inference.py</files>
  <action>
Modify Conductor to initialize and monitor LLM:

**src/conductor/orchestrator.py changes:**
- Add to `Conductor.__init__()`: `self.llm = OllamaInference(LLMConfig(), self.logger)` (import from src.llm)
- Add to startup sequence (after health_monitor.start()): 
  ```python
  llm_health = await self.llm.health_check()
  if llm_health:
      self.logger.info("LLM: Ollama online at http://localhost:11434")
  else:
      self.logger.warning("LLM: Ollama not responding, inference disabled")
      # Store in self.llm_available = False for later checks
  ```
- Add method: `async request_inference(messages: list[dict]) -> str` that:
  - Checks if LLM is available
  - Calls `self.llm.chat(messages)`
  - Catches InferenceError, logs, returns fallback: "I'm not ready to talk right now... wait a sec?"
  - Returns response text on success

- Track inference latency in health monitor or new metric:
  - Add `inference_latency_sec` to metrics (gauge or histogram)
  - Record time in `request_inference()` using context manager or manual timing

**Error handling in orchestrator:**
- Inference failures should not crash Conductor, just log and return fallback
- Track failed inference attempts per 5-minute window (future: auto-disable if >3 failures)

**No changes to other files required** - just orchestrator initialization.
  </action>
  <verify>
1. Conductor starts without errors: `python main.py` boots, logs "LLM: Ollama online" or "LLM: Ollama not responding"
2. `conductor.llm_available` reflects Ollama health status
3. `await conductor.request_inference([{"role": "user", "content": "test"}])` succeeds if Ollama running, returns fallback if not
4. Inference latency recorded in metrics (check prometheus endpoint if exposed)
5. 3 test calls: one valid, one timeout (mocked), one with huge context - verify logs show proper error messages
  </verify>
  <done>
Conductor has llm attribute initialized, health check runs at startup, request_inference() method available for platforms to call. Inference errors logged but don't crash system. Latency metrics recorded.
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement context window trimming and token counting with tests</name>
  <files>src/llm/inference.py</files>
  <action>
Add context trimming logic to OllamaInference:

**New method: `_trim_context(messages: list[dict], max_tokens: int) -> list[dict]`:**
- Input: full message list, maximum allowed tokens
- Calculate token count for each message:
  - System messages (first): always keep, count tokens
  - User/assistant messages: count tokens for each
- Reserve tokens: system + safety_margin (256) + response_space (256)
- Remove oldest user/assistant messages until context fits
- Return trimmed messages list (system always first)
- Log trimming: "Trimmed context from 5 to 3 messages to fit 8000 token window"

**Update `chat()` method:**
- Before calling Ollama, call `_trim_context(messages, max_context_tokens)`
- Use trimmed messages for actual inference
- Still raise ContextOverflowError if even after trimming, context exceeds limit (shouldn't happen with algorithm)

**Token counting implementation:**
- Modify `_count_tokens(text: str)` to attempt transformers tokenizer:
  ```python
  def _count_tokens(self, text: str) -> int:
      try:
          from transformers import AutoTokenizer
          if not hasattr(self, '_tokenizer'):
              self._tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
          tokens = self._tokenizer.encode(text, add_special_tokens=False)
          return len(tokens)
      except Exception as e:
          # Fallback: rough estimation
          self.logger.debug(f"Tokenizer failed, using fallback: {e}")
          return len(text) // 4
  ```
- Make tokenizer lazy-loaded (only fetch on first call)
- Catch all exceptions (ImportError, network errors, etc.) and silently fallback
- Log at DEBUG level only ("Tokenizer fallback used for token counting")

**Test coverage:**
- `test_trim_context_empty()` - empty messages returns empty
- `test_trim_context_keeps_system()` - system message always kept
- `test_trim_context_removes_oldest()` - removes user/assistant from start
- `test_trim_context_respects_limit()` - trimmed context ≤ max_tokens
- `test_token_counting_transformers()` - tokenizer works (with mock)
- `test_token_counting_fallback()` - fallback estimation is ~4x char length
  </action>
  <verify>
1. `_trim_context([system_msg, user1, user2, user3], max_tokens=500)` returns [system_msg, user3] or similar, removes oldest non-system
2. `_count_tokens("Hello world")` returns 2-3 (tokenizer) or 3 (fallback estimation)
3. Large message list (10+ messages) trims to fit 8K context without crash
4. All tests pass: `pytest tests/test_inference.py -v`
5. Log messages appear at DEBUG for tokenizer fallback, ERROR for real inference errors
  </verify>
  <done>
Context trimming algorithm implemented, removes oldest messages while preserving system prompt. Token counting with transformers fallback working. All tests passing. Inference pipeline ready for prompt building.
  </done>
</task>

</tasks>

<verification>
After all tasks:
1. Create `tests/test_llm_inference.py` with full unit test suite (6-8 tests)
2. All tests pass: `pytest tests/ -v --tb=short`
3. Conductor boots without LLM crashes: `python main.py` starts successfully
4. Context trimming works: manually pass 10+ messages to chat(), verify trimming logs appear
5. Token counting tries transformers first, falls back gracefully: check logs for "Tokenizer" mentions
6. Error handling works: stop Ollama, call inference, verify fallback message returned
</verification>

<success_criteria>
- OllamaInference class created with full async interface
- Health check confirms Ollama availability
- Context window enforced (<8K tokens)
- Token counting with transformers + fallback
- Conductor initialized with LLM, no crashes on errors
- All unit tests passing (6-8 tests)
- Inference latency tracked in metrics
- Ready for Plan 02 (prompt building with emotional modulation)
</success_criteria>

<output>
Create `.planning/phases/04-llm-integration/04-01-SUMMARY.md` with:
- Inference engine initialized, Ollama health checked
- Context trimming algorithm: removes oldest messages, respects 8K limit
- Token counting: transformers-based with fallback estimation
- Latency metrics collected, baseline: <3sec p90 on test system
- Conductor integration complete, errors handled gracefully
</output>
