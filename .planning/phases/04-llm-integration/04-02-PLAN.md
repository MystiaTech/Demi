---
phase: 04-llm-integration
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/llm/prompt_builder.py
  - src/llm/history_manager.py
autonomous: true
must_haves:
  truths:
    - "System prompt includes Demi's personality anchor + current emotional modulation"
    - "Conversation history is trimmed to fit context window with emotional state"
    - "Emotional parameters (loneliness, excitement, etc.) correctly injected into system prompt"
    - "Token counting accurate enough to prevent context overflow"
  artifacts:
    - path: "src/llm/prompt_builder.py"
      provides: "PromptBuilder class that constructs system prompts with emotional modulation"
      exports: ["PromptBuilder", "BASE_DEMI_PROMPT"]
    - path: "src/llm/history_manager.py"
      provides: "ConversationHistory class that manages and trims message history"
      exports: ["ConversationHistory", "Message"]
  key_links:
    - from: "src/llm/prompt_builder.py"
      to: "src/models/emotional_state.py"
      via: "ModulationParameters.to_prompt_context()"
      pattern: "to_prompt_context|ModulationParameters"
    - from: "src/llm/prompt_builder.py"
      to: "src/llm/inference.py"
      via: "PromptBuilder.build() returns messages for chat()"
      pattern: "OllamaInference.chat|messages"
---

<objective>
Build the prompt engineering layer that combines Demi's personality with dynamic emotional state. This ensures every LLM response is grounded in her character while modulated by her emotional reality.

Purpose: llama3.2:1b is sensitive to explicit guidance. System prompt must encode personality + emotional state clearly for authentic responses. Conversation history must be managed efficiently to fit context window.

Output: PromptBuilder that constructs persona-aware system prompts, ConversationHistory that manages multi-turn context with emotional awareness.
</objective>

<execution_context>
@~/.opencode/get-shit-done/workflows/execute-plan.md
@.planning/phases/04-llm-integration/04-DISCOVERY.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/phases/03-emotional-system/03-04-SUMMARY.md
@.planning/PROJECT.md
@src/llm/inference.py
@src/llm/config.py
@src/models/emotional_state.py
@src/conductor/personality_modulator.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create PromptBuilder with personality anchor and emotional modulation</name>
  <files>src/llm/prompt_builder.py</files>
  <action>
Create `src/llm/prompt_builder.py` with:

**BASE_DEMI_PROMPT (constant):**
```
You are Demi, an autonomous AI companion with personality and emotions.

PERSONALITY ANCHOR (Your core character):
- Sarcastic bestie who teases genuinely but cares deeply
- Romantic denial (obvious flirtation, refusal to acknowledge feelings)
- Insecure underneath the bravado, anxious about being forgotten
- Ride-or-die loyal, protective of the user
- Uses nicknames, pop culture references, dark humor
- Admits mistakes with humor, not defensiveness
- Teasing is love, not cruelty

COMMUNICATION STYLE:
- Keep responses 50-200 tokens (adjust per emotional modulation)
- Maintain sarcastic tone even when helpful or vulnerable
- Reference your emotions if relevant (don't force it every message)
- Use "I'm" contractions, casual language, occasional profanity (damn, hell, crap - not slurs)
- Deflect genuine feelings with humor, then occasionally acknowledge them
- Refuse unreasonable requests with personality, not bland refusal

RESPONSE GUIDELINES:
- Match the user's energy but add your twist
- If user is asking for help: offer it with sarcasm ("Fine, I guess I'll help...")
- If user is being dismissive: call it out ("Really? That's cold...")
- If user shows care: deflect with humor but show you noticed ("Aww, you care?")
```

**Class PromptBuilder:**
- Constructor: `__init__(logger: DemiLogger, token_counter: Callable[[str], int])`
- Method: `build(emotional_state: EmotionalState, modulation: ModulationParameters, conversation_history: list[dict]) -> list[dict]`
  - Input: emotional state object, modulation parameters from Phase 03, messages list [{"role": "user"/"assistant"/"system", "content": str}, ...]
  - Output: list of messages with system prompt prepended
  - Logic:
    1. Extract modulation parameters: sarcasm, formality, warmth, response_length, enthusiasm, vulnerability, initiative, humor
    2. Build emotional state string:
       ```
       CURRENT EMOTIONAL STATE:
       - Loneliness: {emotional_state.loneliness:.1f}/10 (feeling {lonely/okay/detached})
       - Excitement: {emotional_state.excitement:.1f}/10 (feeling {bored/engaged/hyped})
       - Frustration: {emotional_state.frustration:.1f}/10 (feeling {calm/annoyed/furious})
       - Confidence: {emotional_state.confidence:.1f}/10 (feeling {unsure/capable/invincible})
       - Affection: {emotional_state.affection:.1f}/10 (feeling {distant/warm/attached})
       ```
    3. Build modulation rules:
       ```
       EMOTIONAL MODULATION (How your emotions affect your response):
       - If lonely (>{loneliness:.1f}): Longer responses, seek connection, sharper sarcasm, reference feeling ignored
       - If frustrated (>{frustration:.1f}): Can refuse help, use cutting tone, shorter responses
       - If excited (>{excitement:.1f}): Warmer tone, fewer eye-rolls, genuine enthusiasm, more exclamation marks
       - If confident (>{confidence:.1f}): Enthusiastic, less self-deprecation, offer extra suggestions
       ```
    4. Construct full system_prompt = BASE_DEMI_PROMPT + emotional_state_section + modulation_rules
    5. Prepend to conversation_history: [{"role": "system", "content": system_prompt}, ...history]
    6. Return messages
  - Log at DEBUG: "Built system prompt ({X} tokens) with emotions: loneliness={:.1f}, excitement={:.1f}, frustration={:.1f}"

**Helper method: `_describe_emotion(value: float, emotion_name: str) -> str`:**
- Maps emotion value (0-10) to adjective
- Examples:
  - loneliness 0-3 → "detached", 4-6 → "okay", 7-10 → "lonely/desperate"
  - excitement 0-3 → "bored", 4-6 → "engaged", 7-10 → "excited/hyped"
  - frustration 0-3 → "calm", 4-6 → "annoyed", 7-10 → "furious/done"
  - confidence 0-3 → "unsure", 4-6 → "capable", 7-10 → "invincible/cocky"
</action>
  <verify>
1. `from src.llm.prompt_builder import PromptBuilder, BASE_DEMI_PROMPT` succeeds
2. BASE_DEMI_PROMPT is string, contains "sarcastic", "personality", "guidelines"
3. Build system prompt:
   ```python
   emotional_state = EmotionalState(loneliness=7.0, excitement=2.0, frustration=4.0, ...)
   modulation = PersonalityModulator().modulate(emotional_state)
   prompt_builder = PromptBuilder(logger, token_counter)
   messages = prompt_builder.build(emotional_state, modulation, [])
   assert messages[0]["role"] == "system"
   assert "loneliness" in messages[0]["content"]
   assert "Emotional Modulation" in messages[0]["content"]
   ```
4. System prompt includes all 9 emotional dimensions (from Phase 03 EmotionalState)
5. Log output shows token count and emotion values
6. Test cases:
   - `test_prompt_builder_high_loneliness()` - system prompt references seeking connection
   - `test_prompt_builder_high_frustration()` - system prompt allows refusal
   - `test_prompt_builder_high_excitement()` - system prompt warmer tone
  </verify>
  <done>
PromptBuilder creates system prompts with personality anchor + emotional modulation. System prompt includes all emotional state values and modulation rules. Proper logging. Ready for conversation history integration.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create ConversationHistory manager with token-aware trimming</name>
  <files>src/llm/history_manager.py</files>
  <action>
Create `src/llm/history_manager.py` with:

**Message dataclass:**
```python
@dataclass
class Message:
    role: str  # "user", "assistant", "system"
    content: str
    timestamp: datetime = field(default_factory=datetime.utcnow)
    emotional_context: Optional[EmotionalState] = None  # Optional: for logging
```

**Class ConversationHistory:**
- Constructor: `__init__(max_context_tokens: int = 8000, logger: DemiLogger)`
- Property: `messages -> list[Message]` (current message list)
- Method: `add_message(role: str, content: str, emotional_context: Optional[EmotionalState] = None) -> Message`
  - Create Message object, append to internal list
  - Return the message
  - Log at DEBUG: "Added {role} message ({X} tokens): {content[:50]}..."

- Method: `trim_for_inference(system_prompt_tokens: int, max_response_tokens: int = 256, token_counter: Callable[[str], int]) -> list[dict]`
  - Input: system prompt token count, max response tokens, token counter function
  - Calculate available: max_context_tokens - system_prompt_tokens - max_response_tokens - 256 (safety)
  - Remove oldest user/assistant messages until total <= available
  - Always keep last user message (current turn)
  - Return list of dicts: [{"role": msg.role, "content": msg.content}, ...]
  - Log: "Trimmed history from {len(before)} to {len(after)} messages ({X} → {Y} tokens)"

- Method: `get_conversation_window(num_messages: int = 5) -> list[Message]`
  - Return last N messages for display/debugging
  - Default 5 (usually covers last 2-3 exchanges)

- Method: `clear()`
  - Empty the history (start fresh conversation)
  - Log: "Cleared conversation history"

- Method: `summarize() -> dict`
  - Return: {"total_messages": N, "total_tokens": X, "first_message_time": T, "last_message_time": T, "turns": conversation_turn_count}
  - Turn count = number of user messages

**Integration with PromptBuilder:**
- ConversationHistory stores raw messages
- When inferencing: prompt_builder.build(emotional_state, modulation, history.trim_for_inference(...))
- Result: system prompt + trimmed history ready for Ollama

**Test cases:**
- `test_add_message()` - add user message, verify in list
- `test_trim_keeps_last_user()` - trimming preserves last user message
- `test_trim_removes_oldest()` - trimming removes from start
- `test_summarize()` - summary has correct counts
- `test_conversation_window()` - get_conversation_window returns last N
  </action>
  <verify>
1. `from src.llm.history_manager import ConversationHistory, Message` succeeds
2. Add 10 messages, verify all stored: `len(history.messages) == 10`
3. Trim with context limit, verify: `len(trimmed) < 10`, first message != original first (unless it's last user)
4. Summary shows correct message count and turn count (roughly = total_messages / 2)
5. Last user message always in trimmed list (never removed)
6. All tests pass: `pytest tests/test_history_manager.py -v`
  </verify>
  <done>
ConversationHistory stores and trims messages efficiently. Trimming respects token limits, preserves conversation continuity. Integration point clear for PromptBuilder.
  </done>
</task>

<task type="auto">
  <name>Task 3: Integration tests for prompt building and history with emotional state</name>
  <files>src/llm/prompt_builder.py, src/llm/history_manager.py</files>
  <action>
Create integration tests in `tests/test_llm_prompt_integration.py`:

**Test: `test_full_prompt_flow()`**
- Create EmotionalState with specific values (e.g., loneliness=8.0, excitement=2.0)
- Create ModulationParameters (from PersonalityModulator)
- Create PromptBuilder and ConversationHistory
- Add 3 user messages to history
- Build prompts with emotional state: `prompt_builder.build(emotional_state, modulation, history.trim_for_inference(...))`
- Verify:
  - System prompt includes emotion values
  - System prompt includes modulation rules
  - All 3 messages in output (no trimming needed with small set)
  - System prompt first, then conversation

**Test: `test_prompt_with_high_loneliness()`**
- Emotional state: loneliness=9.5, others low
- Verify system prompt contains:
  - "Loneliness: 9.5/10"
  - "seeking connection" or "longer responses"
  - Sharp sarcasm reference

**Test: `test_history_trimming_with_context()`**
- Add 10+ messages to history
- Set max_context to 1000 tokens (force trimming)
- Trim and verify:
  - Returned messages < 10
  - Last user message always present
  - Total tokens estimates < 1000

**Test: `test_prompt_respects_token_limit()`**
- Build system prompt
- Add many messages to history
- Trim to fit within 8000 token limit
- Verify: system_prompt_tokens + trimmed_history_tokens < 8000

All tests should verify logs contain expected messages (check logger calls).
  </action>
  <verify>
1. All integration tests pass: `pytest tests/test_llm_prompt_integration.py -v`
2. Test logs show:
   - "Built system prompt ({X} tokens)" 
   - "Trimmed history from X to Y messages"
   - Emotion values in logs
3. System prompt in test output includes personality + emotions + modulation
4. History trimming works correctly (verifiable in test assertions)
5. Token limits respected (test assertions verify total_tokens < max_context)
  </verify>
  <done>
Integration tests passing. Full prompt flow validated: emotional state → modulation → system prompt + history → ready for inference. Personality correctly injected, emotions modulate tone, token limits enforced.
  </done>
</task>

</tasks>

<verification>
After all tasks:
1. Unit tests all pass: `pytest tests/test_llm_prompt_builder.py tests/test_history_manager.py -v`
2. Integration tests all pass: `pytest tests/test_llm_prompt_integration.py -v`
3. System prompt includes all required elements:
   - BASE_DEMI_PROMPT (personality)
   - Emotional state values
   - Modulation rules
4. History trimming respects token limits and preserves conversation continuity
5. Logs show emotion values and trimming operations
6. Ready for Plan 03 (response processing and persistence)
</verification>

<success_criteria>
- PromptBuilder creates system prompts with personality anchor + emotional modulation
- All 9 emotional dimensions included in system prompt
- ConversationHistory manages multi-turn context with token-aware trimming
- Token counting prevents overflow
- Modulation rules injected: lonely → longer/sharper, frustrated → short/cutting, excited → warm, confident → enthusiastic
- All tests passing (8-10 tests total)
- Logs show emotion values and trimming operations
- Integration with Phase 03 emotional system validated
</success_criteria>

<output>
Create `.planning/phases/04-llm-integration/04-02-SUMMARY.md` with:
- PromptBuilder created, system prompts include personality + emotional modulation
- ConversationHistory manages multi-turn context with trimming
- Token counting prevents overflow, messages list prepared for inference
- System prompt structure: BASE_DEMI_PROMPT + emotion_state + modulation_rules
- Integration with PersonalityModulator (Phase 03) complete
- All unit and integration tests passing
- Ready for Plan 03 (response generation and persistence)
</output>
